{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing packages an dependencies\n",
    "#%tensorflow_version 1.x\n",
    "\n",
    "import cv2\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from google.colab.patches import cv2_imshow\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "\n",
    "from keras.applications import VGG16\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization, Activation\n",
    "from keras.layers import Convolution2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "#import imageio as io\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import *\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "            base = 0.001 max = 0.006\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.0001, max_lr=0.01, step_size=300., mode='exp_range',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import *\n",
    "\n",
    "clr_triangular = CyclicLR(mode='exp_range')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"F:\\\\EIP\\\\hvc_data\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13573, 10)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./hvc_annotations.csv\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "#display sample images\n",
    "#from google.colab.patches import cv2_imshow\n",
    "ext_img = imageio.imread(\"resized/8335.jpg\")\n",
    "print(ext_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>image_path</th>\n",
       "      <td>resized/1.jpg</td>\n",
       "      <td>resized/2.jpg</td>\n",
       "      <td>resized/3.jpg</td>\n",
       "      <td>resized/4.jpg</td>\n",
       "      <td>resized/5.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender_female</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender_male</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imagequality_Average</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imagequality_Bad</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imagequality_Good</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_15-25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_25-35</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_35-45</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_45-55</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_55+</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_normal-healthy</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_over-weight</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_slightly-overweight</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_underweight</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carryingbag_None</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>footwear_CantSee</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>footwear_Fancy</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>footwear_Normal</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion_Angry/Serious</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion_Happy</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion_Neutral</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion_Sad</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bodypose_Back</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bodypose_Front-Frontish</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bodypose_Side</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  0              1  \\\n",
       "image_path                            resized/1.jpg  resized/2.jpg   \n",
       "gender_female                                     0              1   \n",
       "gender_male                                       1              0   \n",
       "imagequality_Average                              1              1   \n",
       "imagequality_Bad                                  0              0   \n",
       "imagequality_Good                                 0              0   \n",
       "age_15-25                                         0              0   \n",
       "age_25-35                                         0              0   \n",
       "age_35-45                                         1              1   \n",
       "age_45-55                                         0              0   \n",
       "age_55+                                           0              0   \n",
       "weight_normal-healthy                             1              0   \n",
       "weight_over-weight                                0              1   \n",
       "weight_slightly-overweight                        0              0   \n",
       "weight_underweight                                0              0   \n",
       "carryingbag_Daily/Office/Work Bag                 0              0   \n",
       "carryingbag_Grocery/Home/Plastic Bag              1              0   \n",
       "carryingbag_None                                  0              1   \n",
       "footwear_CantSee                                  0              0   \n",
       "footwear_Fancy                                    0              0   \n",
       "footwear_Normal                                   1              1   \n",
       "emotion_Angry/Serious                             0              1   \n",
       "emotion_Happy                                     0              0   \n",
       "emotion_Neutral                                   1              0   \n",
       "emotion_Sad                                       0              0   \n",
       "bodypose_Back                                     0              0   \n",
       "bodypose_Front-Frontish                           1              1   \n",
       "bodypose_Side                                     0              0   \n",
       "\n",
       "                                                  2              3  \\\n",
       "image_path                            resized/3.jpg  resized/4.jpg   \n",
       "gender_female                                     0              0   \n",
       "gender_male                                       1              1   \n",
       "imagequality_Average                              0              0   \n",
       "imagequality_Bad                                  0              0   \n",
       "imagequality_Good                                 1              1   \n",
       "age_15-25                                         0              0   \n",
       "age_25-35                                         0              0   \n",
       "age_35-45                                         0              0   \n",
       "age_45-55                                         1              1   \n",
       "age_55+                                           0              0   \n",
       "weight_normal-healthy                             1              1   \n",
       "weight_over-weight                                0              0   \n",
       "weight_slightly-overweight                        0              0   \n",
       "weight_underweight                                0              0   \n",
       "carryingbag_Daily/Office/Work Bag                 0              1   \n",
       "carryingbag_Grocery/Home/Plastic Bag              1              0   \n",
       "carryingbag_None                                  0              0   \n",
       "footwear_CantSee                                  1              0   \n",
       "footwear_Fancy                                    0              0   \n",
       "footwear_Normal                                   0              1   \n",
       "emotion_Angry/Serious                             0              0   \n",
       "emotion_Happy                                     0              0   \n",
       "emotion_Neutral                                   1              1   \n",
       "emotion_Sad                                       0              0   \n",
       "bodypose_Back                                     0              0   \n",
       "bodypose_Front-Frontish                           1              1   \n",
       "bodypose_Side                                     0              0   \n",
       "\n",
       "                                                  4  \n",
       "image_path                            resized/5.jpg  \n",
       "gender_female                                     1  \n",
       "gender_male                                       0  \n",
       "imagequality_Average                              0  \n",
       "imagequality_Bad                                  0  \n",
       "imagequality_Good                                 1  \n",
       "age_15-25                                         0  \n",
       "age_25-35                                         0  \n",
       "age_35-45                                         1  \n",
       "age_45-55                                         0  \n",
       "age_55+                                           0  \n",
       "weight_normal-healthy                             0  \n",
       "weight_over-weight                                0  \n",
       "weight_slightly-overweight                        1  \n",
       "weight_underweight                                0  \n",
       "carryingbag_Daily/Office/Work Bag                 0  \n",
       "carryingbag_Grocery/Home/Plastic Bag              0  \n",
       "carryingbag_None                                  1  \n",
       "footwear_CantSee                                  1  \n",
       "footwear_Fancy                                    0  \n",
       "footwear_Normal                                   0  \n",
       "emotion_Angry/Serious                             0  \n",
       "emotion_Happy                                     0  \n",
       "emotion_Neutral                                   1  \n",
       "emotion_Sad                                       0  \n",
       "bodypose_Back                                     0  \n",
       "bodypose_Front-Frontish                           1  \n",
       "bodypose_Side                                     0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_df = pd.concat([\n",
    "                        df[['image_path']],\n",
    "                        pd.get_dummies(df['gender'], prefix = \"gender\"),\n",
    "                        pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
    "    pd.get_dummies(df.age, prefix=\"age\"),\n",
    "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
    "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
    "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
    "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
    "    pd.get_dummies(df.bodypose, prefix=\"bodypose\")\n",
    "                        ], axis = 1)\n",
    "one_hot_df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "# Label columns per attribute\n",
    "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
    "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
    "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
    "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
    "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
    "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
    "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
    "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
    "\n",
    "class PersonDataGenerator(keras.utils.Sequence):\n",
    "    \"\"\"Ground truth data generator\"\"\"\n",
    "\n",
    "    \n",
    "    def __init__(self, df, batch_size=32, shuffle=True):\n",
    "        self.df = df\n",
    "        self.batch_size=batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"fetch batched images and targets\"\"\"\n",
    "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
    "        items = self.df.iloc[batch_slice]\n",
    "        image = np.stack([imageio.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
    "        target = {\n",
    "            \"gender_output\": items[_gender_cols_].values,\n",
    "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
    "            \"age_output\": items[_age_cols_].values,\n",
    "            \"weight_output\": items[_weight_cols_].values,\n",
    "            \"bag_output\": items[_carryingbag_cols_].values,\n",
    "            \"pose_output\": items[_bodypose_cols_].values,\n",
    "            \"footwear_output\": items[_footwear_cols_].values,\n",
    "            \"emotion_output\": items[_emotion_cols_].values,\n",
    "        }\n",
    "        return image, target\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\"\"\"\n",
    "        if self.shuffle == True:\n",
    "            self.df = self.df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11537, 28), (2036, 28))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(one_hot_df, test_size=0.15,random_state = 111)\n",
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>gender_female</th>\n",
       "      <th>gender_male</th>\n",
       "      <th>imagequality_Average</th>\n",
       "      <th>imagequality_Bad</th>\n",
       "      <th>imagequality_Good</th>\n",
       "      <th>age_15-25</th>\n",
       "      <th>age_25-35</th>\n",
       "      <th>age_35-45</th>\n",
       "      <th>age_45-55</th>\n",
       "      <th>...</th>\n",
       "      <th>footwear_CantSee</th>\n",
       "      <th>footwear_Fancy</th>\n",
       "      <th>footwear_Normal</th>\n",
       "      <th>emotion_Angry/Serious</th>\n",
       "      <th>emotion_Happy</th>\n",
       "      <th>emotion_Neutral</th>\n",
       "      <th>emotion_Sad</th>\n",
       "      <th>bodypose_Back</th>\n",
       "      <th>bodypose_Front-Frontish</th>\n",
       "      <th>bodypose_Side</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>resized/2250.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5664</th>\n",
       "      <td>resized/5665.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5608</th>\n",
       "      <td>resized/5609.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4157</th>\n",
       "      <td>resized/4158.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7254</th>\n",
       "      <td>resized/7255.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            image_path  gender_female  gender_male  imagequality_Average  \\\n",
       "2249  resized/2250.jpg              1            0                     0   \n",
       "5664  resized/5665.jpg              0            1                     0   \n",
       "5608  resized/5609.jpg              1            0                     1   \n",
       "4157  resized/4158.jpg              0            1                     0   \n",
       "7254  resized/7255.jpg              0            1                     0   \n",
       "\n",
       "      imagequality_Bad  imagequality_Good  age_15-25  age_25-35  age_35-45  \\\n",
       "2249                 1                  0          1          0          0   \n",
       "5664                 0                  1          0          0          1   \n",
       "5608                 0                  0          1          0          0   \n",
       "4157                 0                  1          0          1          0   \n",
       "7254                 1                  0          0          1          0   \n",
       "\n",
       "      age_45-55  ...  footwear_CantSee  footwear_Fancy  footwear_Normal  \\\n",
       "2249          0  ...                 0               1                0   \n",
       "5664          0  ...                 1               0                0   \n",
       "5608          0  ...                 0               0                1   \n",
       "4157          0  ...                 1               0                0   \n",
       "7254          0  ...                 0               0                1   \n",
       "\n",
       "      emotion_Angry/Serious  emotion_Happy  emotion_Neutral  emotion_Sad  \\\n",
       "2249                      0              1                0            0   \n",
       "5664                      1              0                0            0   \n",
       "5608                      0              0                1            0   \n",
       "4157                      0              0                1            0   \n",
       "7254                      0              0                1            0   \n",
       "\n",
       "      bodypose_Back  bodypose_Front-Frontish  bodypose_Side  \n",
       "2249              0                        0              1  \n",
       "5664              0                        1              0  \n",
       "5608              1                        0              0  \n",
       "4157              0                        1              0  \n",
       "7254              1                        0              0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = PersonDataGenerator(train_df, batch_size=32)\n",
    "valid_gen = PersonDataGenerator(val_df, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gender': 2,\n",
       " 'image_quality': 3,\n",
       " 'age': 5,\n",
       " 'weight': 4,\n",
       " 'bag': 3,\n",
       " 'pose': 3,\n",
       " 'footwear': 3,\n",
       " 'emotion': 4}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get number of output units from data\n",
    "images, targets = next(iter(train_gen))\n",
    "num_units = {k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
    "num_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\"gender_output\" : \"binary_crossentropy\",\n",
    "         \"image_quality_output\" : \"categorical_crossentropy\",\n",
    "         \"age_output\" : \"categorical_crossentropy\",\n",
    "         \"weight_output\" : \"categorical_crossentropy\",\n",
    "         \"bag_output\" : \"categorical_crossentropy\",\n",
    "         \"footwear_output\" : \"categorical_crossentropy\",\n",
    "         \"pose_output\" : \"categorical_crossentropy\",\n",
    "         \"emotion_output\" : \"categorical_crossentropy\"}\n",
    "lossWeights = {\"gender_output\" : 1.0,\n",
    "         \"image_quality_output\" : 1.0,\n",
    "         \"age_output\" : 1.0,\n",
    "         \"weight_output\" : 1.0,\n",
    "         \"bag_output\" : 1.0,\n",
    "         \"footwear_output\" : 1.0,\n",
    "         \"pose_output\" : 1.0,\n",
    "         \"emotion_output\" : 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######inceptionV3### MOD\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "backend = keras.backend\n",
    "layers = keras.layers\n",
    "models = keras.models\n",
    "keras_utils = keras.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras_applications.imagenet_utils import _obtain_input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_bn(x,\n",
    "              filters,\n",
    "              num_row,\n",
    "              num_col,\n",
    "              padding='same',\n",
    "              strides=(1, 1),\n",
    "              name=None):\n",
    "    \"\"\"Utility function to apply conv + BN.\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        filters: filters in `Conv2D`.\n",
    "        num_row: height of the convolution kernel.\n",
    "        num_col: width of the convolution kernel.\n",
    "        padding: padding mode in `Conv2D`.\n",
    "        strides: strides in `Conv2D`.\n",
    "        name: name of the ops; will become `name + '_conv'`\n",
    "            for the convolution and `name + '_bn'` for the\n",
    "            batch norm layer.\n",
    "    # Returns\n",
    "        Output tensor after applying `Conv2D` and `BatchNormalization`.\n",
    "    \"\"\"\n",
    "    if name is not None:\n",
    "        bn_name = name + '_bn'\n",
    "        conv_name = name + '_conv'\n",
    "    else:\n",
    "        bn_name = None\n",
    "        conv_name = None\n",
    "    if backend.image_data_format() == 'channels_first':\n",
    "        bn_axis = 1\n",
    "    else:\n",
    "        bn_axis = 3\n",
    "    x = layers.Conv2D(\n",
    "        filters, (num_row, num_col),\n",
    "        strides=strides,\n",
    "        padding=padding,\n",
    "        use_bias=False,\n",
    "        name=conv_name)(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, scale=False, name=bn_name)(x)\n",
    "    x = layers.Activation('relu', name=name)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InceptionV3(include_top=True,\n",
    "                input_tensor=None,\n",
    "                input_shape=None,\n",
    "                **kwargs):\n",
    "    \"\"\"Instantiates the Inception v3 architecture.\n",
    "    Optionally loads weights pre-trained on ImageNet.\n",
    "    Note that the data format convention used by the model is\n",
    "    the one specified in your Keras config at `~/.keras/keras.json`.\n",
    "    # Arguments\n",
    "        include_top: whether to include the fully-connected\n",
    "            layer at the top of the network.\n",
    "        weights: one of `None` (random initialization),\n",
    "              'imagenet' (pre-training on ImageNet),\n",
    "              or the path to the weights file to be loaded.\n",
    "        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(299, 299, 3)` (with `channels_last` data format)\n",
    "            or `(3, 299, 299)` (with `channels_first` data format).\n",
    "            It should have exactly 3 inputs channels,\n",
    "            and width and height should be no smaller than 75.\n",
    "            E.g. `(150, 150, 3)` would be one valid value.\n",
    "        pooling: Optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model will be\n",
    "                the 4D tensor output of the\n",
    "                last convolutional block.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional block, and thus\n",
    "                the output of the model will be a 2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "    # Raises\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape.\n",
    "    \"\"\"\n",
    "    global backend, layers, models, keras_utils\n",
    "    weights = None \n",
    "    \n",
    "\n",
    "    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization), `imagenet` '\n",
    "                         '(pre-training on ImageNet), '\n",
    "                         'or the path to the weights file to be loaded.')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as `\"imagenet\"` with `include_top`'\n",
    "                         ' as true, `classes` should be 1000')\n",
    "\n",
    "    # Determine proper input shape\n",
    "    backend = keras.backend\n",
    "    layers = keras.layers\n",
    "    models = keras.models\n",
    "    keras_utils = keras.utils\n",
    "    \n",
    "    input_shape = _obtain_input_shape(\n",
    "        input_shape,\n",
    "        default_size=299,\n",
    "        min_size=75,\n",
    "        data_format=backend.image_data_format(),\n",
    "        require_flatten=include_top,\n",
    "        weights=weights)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = layers.Input(shape=input_shape)\n",
    "    else:\n",
    "        if not backend.is_keras_tensor(input_tensor):\n",
    "            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    if backend.image_data_format() == 'channels_first':\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = 3\n",
    "\n",
    "    x = conv2d_bn(img_input, 32, 3, 3, strides=(2, 2), padding='valid')\n",
    "    x = conv2d_bn(x, 32, 3, 3, padding='valid')\n",
    "    x = conv2d_bn(x, 64, 3, 3)\n",
    "    x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    x = conv2d_bn(x, 80, 1, 1, padding='valid')\n",
    "    x = conv2d_bn(x, 192, 3, 3, padding='valid')\n",
    "    x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    # mixed 0: 35 x 35 x 256\n",
    "    branch1x1 = conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "    branch5x5 = conv2d_bn(x, 48, 1, 1)\n",
    "    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "\n",
    "    branch_pool = layers.AveragePooling2D((3, 3),\n",
    "                                          strides=(1, 1),\n",
    "                                          padding='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 32, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed0')\n",
    "\n",
    "    # mixed 1: 35 x 35 x 288\n",
    "    branch1x1 = conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "    branch5x5 = conv2d_bn(x, 48, 1, 1)\n",
    "    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "\n",
    "    branch_pool = layers.AveragePooling2D((3, 3),\n",
    "                                          strides=(1, 1),\n",
    "                                          padding='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 64, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed1')\n",
    "\n",
    "    # mixed 2: 35 x 35 x 288\n",
    "    branch1x1 = conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "    branch5x5 = conv2d_bn(x, 48, 1, 1)\n",
    "    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "\n",
    "    branch_pool = layers.AveragePooling2D((3, 3),\n",
    "                                          strides=(1, 1),\n",
    "                                          padding='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 64, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed2')\n",
    "\n",
    "    # mixed 3: 17 x 17 x 768\n",
    "    branch3x3 = conv2d_bn(x, 384, 3, 3, strides=(2, 2), padding='valid')\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(\n",
    "        branch3x3dbl, 96, 3, 3, strides=(2, 2), padding='valid')\n",
    "\n",
    "    branch_pool = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "    x = layers.concatenate(\n",
    "        [branch3x3, branch3x3dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed3')\n",
    "\n",
    "    # mixed 4: 17 x 17 x 768\n",
    "    branch1x1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "    branch7x7 = conv2d_bn(x, 128, 1, 1)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 128, 1, 7)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "    branch7x7dbl = conv2d_bn(x, 128, 1, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 1, 7)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "    branch_pool = layers.AveragePooling2D((3, 3),\n",
    "                                          strides=(1, 1),\n",
    "                                          padding='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed4')\n",
    "\n",
    "    # mixed 5, 6: 17 x 17 x 768\n",
    "    for i in range(2):\n",
    "        branch1x1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "        branch7x7 = conv2d_bn(x, 160, 1, 1)\n",
    "        branch7x7 = conv2d_bn(branch7x7, 160, 1, 7)\n",
    "        branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "        branch7x7dbl = conv2d_bn(x, 160, 1, 1)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 1, 7)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "        branch_pool = layers.AveragePooling2D(\n",
    "            (3, 3), strides=(1, 1), padding='same')(x)\n",
    "        branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "        x = layers.concatenate(\n",
    "            [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "            axis=channel_axis,\n",
    "            name='mixed' + str(5 + i))\n",
    "\n",
    "    # mixed 7: 17 x 17 x 768\n",
    "    branch1x1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "    branch7x7 = conv2d_bn(x, 192, 1, 1)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 192, 1, 7)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "    branch7x7dbl = conv2d_bn(x, 192, 1, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "    branch_pool = layers.AveragePooling2D((3, 3),\n",
    "                                          strides=(1, 1),\n",
    "                                          padding='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed7')\n",
    "\n",
    "    # mixed 8: 8 x 8 x 1280\n",
    "    branch3x3 = conv2d_bn(x, 192, 1, 1)\n",
    "    branch3x3 = conv2d_bn(branch3x3, 320, 3, 3,\n",
    "                          strides=(2, 2), padding='valid')\n",
    "\n",
    "    branch7x7x3 = conv2d_bn(x, 192, 1, 1)\n",
    "    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 1, 7)\n",
    "    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 7, 1)\n",
    "    branch7x7x3 = conv2d_bn(\n",
    "        branch7x7x3, 192, 3, 3, strides=(2, 2), padding='valid')\n",
    "\n",
    "    branch_pool = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "    x = layers.concatenate(\n",
    "        [branch3x3, branch7x7x3, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed8')\n",
    "\n",
    "    # mixed 9: 8 x 8 x 2048\n",
    "    for i in range(2):\n",
    "        branch1x1 = conv2d_bn(x, 320, 1, 1)\n",
    "\n",
    "        branch3x3 = conv2d_bn(x, 384, 1, 1)\n",
    "        branch3x3_1 = conv2d_bn(branch3x3, 384, 1, 3)\n",
    "        branch3x3_2 = conv2d_bn(branch3x3, 384, 3, 1)\n",
    "        branch3x3 = layers.concatenate(\n",
    "            [branch3x3_1, branch3x3_2],\n",
    "            axis=channel_axis,\n",
    "            name='mixed9_' + str(i))\n",
    "\n",
    "        branch3x3dbl = conv2d_bn(x, 448, 1, 1)\n",
    "        branch3x3dbl = conv2d_bn(branch3x3dbl, 384, 3, 3)\n",
    "        branch3x3dbl_1 = conv2d_bn(branch3x3dbl, 384, 1, 3)\n",
    "        branch3x3dbl_2 = conv2d_bn(branch3x3dbl, 384, 3, 1)\n",
    "        branch3x3dbl = layers.concatenate(\n",
    "            [branch3x3dbl_1, branch3x3dbl_2], axis=channel_axis)\n",
    "\n",
    "        branch_pool = layers.AveragePooling2D(\n",
    "            (3, 3), strides=(1, 1), padding='same')(x)\n",
    "        branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "        x = layers.concatenate(\n",
    "            [branch1x1, branch3x3, branch3x3dbl, branch_pool],\n",
    "            axis=channel_axis,\n",
    "            name='mixed' + str(9 + i))\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = keras_utils.get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = models.Model(inputs, x, name='inception_v3')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "basemodel = InceptionV3(input_shape = (224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tower(layer,numclasses):\n",
    "    x = Dropout(0.2)(layer)\n",
    "    x = Convolution2D(filters=numclasses,kernel_size = (1,1),activation='relu',use_bias=False,kernel_initializer='he_uniform')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    return x\n",
    "  #x = Activation('softmax')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "neck = basemodel.output\n",
    "\n",
    "def build_head(name, in_layer):\n",
    "    return Activation(activation=\"softmax\", name=f\"{name}_output\")(in_layer)\n",
    "\n",
    "gender = build_head(\"gender\", build_tower(neck,2))\n",
    "image_quality = build_head(\"image_quality\", build_tower(neck,3))\n",
    "age = build_head(\"age\", build_tower(neck,5))\n",
    "weight = build_head(\"weight\", build_tower(neck,4))\n",
    "bag = build_head(\"bag\", build_tower(neck,3))\n",
    "footwear = build_head(\"footwear\", build_tower(neck,3))\n",
    "emotion = build_head(\"emotion\", build_tower(neck,4))\n",
    "pose = build_head(\"pose\", build_tower(neck,3))\n",
    "\n",
    "\n",
    "model2 = Model(\n",
    "    inputs=basemodel.input, \n",
    "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 111, 111, 32) 864         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 111, 111, 32) 96          conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 111, 111, 32) 0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 109, 109, 32) 9216        activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 109, 109, 32) 96          conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 109, 109, 32) 0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 109, 109, 64) 18432       activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 109, 109, 64) 192         conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 109, 109, 64) 0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 54, 54, 64)   0           activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 54, 54, 80)   5120        max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 54, 54, 80)   240         conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 54, 54, 80)   0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 52, 52, 192)  138240      activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 52, 52, 192)  576         conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 52, 52, 192)  0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 25, 25, 192)  0           activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 25, 25, 64)   12288       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 25, 25, 64)   192         conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 25, 25, 64)   0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 25, 25, 48)   9216        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 25, 25, 96)   55296       activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 25, 25, 48)   144         conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 25, 25, 96)   288         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 25, 25, 48)   0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 25, 25, 96)   0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_10 (AveragePo (None, 25, 25, 192)  0           max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 25, 25, 64)   12288       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 25, 25, 64)   76800       activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 25, 25, 96)   82944       activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 25, 25, 32)   6144        average_pooling2d_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 25, 25, 64)   192         conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 25, 25, 64)   192         conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 25, 25, 96)   288         conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 25, 25, 32)   96          conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 25, 25, 64)   0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 25, 25, 64)   0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 25, 25, 96)   0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 25, 25, 32)   0           batch_normalization_106[0][0]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 25, 25, 256)  0           activation_100[0][0]             \n",
      "                                                                 activation_102[0][0]             \n",
      "                                                                 activation_105[0][0]             \n",
      "                                                                 activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 25, 25, 64)   192         conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 25, 25, 64)   0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 25, 25, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 25, 25, 96)   55296       activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 25, 25, 48)   144         conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 25, 25, 96)   288         conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 25, 25, 48)   0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 25, 25, 96)   0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_11 (AveragePo (None, 25, 25, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 25, 25, 64)   76800       activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 25, 25, 96)   82944       activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, 25, 25, 64)   16384       average_pooling2d_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 25, 25, 64)   192         conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 25, 25, 64)   192         conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 25, 25, 96)   288         conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 25, 25, 64)   192         conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 25, 25, 64)   0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 25, 25, 64)   0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 25, 25, 96)   0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 25, 25, 64)   0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 25, 25, 288)  0           activation_107[0][0]             \n",
      "                                                                 activation_109[0][0]             \n",
      "                                                                 activation_112[0][0]             \n",
      "                                                                 activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 25, 25, 64)   192         conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 25, 25, 64)   0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 25, 25, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 25, 25, 96)   55296       activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 25, 25, 48)   144         conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 25, 25, 96)   288         conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 25, 25, 48)   0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 25, 25, 96)   0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_12 (AveragePo (None, 25, 25, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 25, 25, 64)   76800       activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 25, 25, 96)   82944       activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 25, 25, 64)   18432       average_pooling2d_12[0][0]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 25, 25, 64)   192         conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 25, 25, 64)   192         conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 25, 25, 96)   288         conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 25, 25, 64)   192         conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 25, 25, 64)   0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 25, 25, 64)   0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 25, 25, 96)   0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 25, 25, 64)   0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 25, 25, 288)  0           activation_114[0][0]             \n",
      "                                                                 activation_116[0][0]             \n",
      "                                                                 activation_119[0][0]             \n",
      "                                                                 activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 25, 25, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 25, 25, 64)   192         conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 25, 25, 64)   0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 25, 25, 96)   55296       activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 25, 25, 96)   288         conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 25, 25, 96)   0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 12, 12, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 12, 12, 96)   82944       activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 12, 12, 384)  1152        conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 12, 12, 96)   288         conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 12, 12, 384)  0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 12, 12, 96)   0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 12, 12, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 12, 12, 768)  0           activation_121[0][0]             \n",
      "                                                                 activation_124[0][0]             \n",
      "                                                                 max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 12, 12, 128)  384         conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 12, 12, 128)  0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 12, 12, 128)  114688      activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 12, 12, 128)  384         conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 12, 12, 128)  0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 12, 12, 128)  114688      activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 12, 12, 128)  384         conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 12, 12, 128)  384         conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 12, 12, 128)  0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 12, 12, 128)  0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 12, 12, 128)  114688      activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 12, 12, 128)  114688      activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 12, 12, 128)  384         conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_132 (BatchN (None, 12, 12, 128)  384         conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 12, 12, 128)  0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 12, 12, 128)  0           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_13 (AveragePo (None, 12, 12, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 12, 12, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 12, 12, 192)  172032      activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 12, 12, 192)  172032      activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 12, 12, 192)  576         conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 12, 12, 192)  576         conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 12, 12, 192)  576         conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 12, 12, 192)  576         conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 12, 12, 192)  0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 12, 12, 192)  0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 12, 12, 192)  0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 12, 12, 192)  0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 12, 12, 768)  0           activation_125[0][0]             \n",
      "                                                                 activation_128[0][0]             \n",
      "                                                                 activation_133[0][0]             \n",
      "                                                                 activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 12, 12, 160)  480         conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 12, 12, 160)  0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 12, 12, 160)  179200      activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 12, 12, 160)  480         conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 12, 12, 160)  0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 12, 12, 160)  179200      activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 12, 12, 160)  480         conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 12, 12, 160)  480         conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 12, 12, 160)  0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 12, 12, 160)  0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 12, 12, 160)  179200      activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 12, 12, 160)  179200      activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 12, 12, 160)  480         conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 12, 12, 160)  480         conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 12, 12, 160)  0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 12, 12, 160)  0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_14 (AveragePo (None, 12, 12, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 12, 12, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 12, 12, 192)  215040      activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 12, 12, 192)  215040      activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_14[0][0]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 12, 12, 192)  576         conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 12, 12, 192)  576         conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 12, 12, 192)  576         conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 12, 12, 192)  576         conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 12, 12, 192)  0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 12, 12, 192)  0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 12, 12, 192)  0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 12, 12, 192)  0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 12, 12, 768)  0           activation_135[0][0]             \n",
      "                                                                 activation_138[0][0]             \n",
      "                                                                 activation_143[0][0]             \n",
      "                                                                 activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 12, 12, 160)  480         conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 12, 12, 160)  0           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, 12, 12, 160)  179200      activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 12, 12, 160)  480         conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, 12, 12, 160)  0           batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 12, 12, 160)  179200      activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 12, 12, 160)  480         conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 12, 12, 160)  480         conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 12, 12, 160)  0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, 12, 12, 160)  0           batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 12, 12, 160)  179200      activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, 12, 12, 160)  179200      activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 12, 12, 160)  480         conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, 12, 12, 160)  480         conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 12, 12, 160)  0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, 12, 12, 160)  0           batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_15 (AveragePo (None, 12, 12, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 12, 12, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, 12, 12, 192)  215040      activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, 12, 12, 192)  215040      activation_152[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_15[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 12, 12, 192)  576         conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 12, 12, 192)  576         conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, 12, 12, 192)  576         conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, 12, 12, 192)  576         conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 12, 12, 192)  0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 12, 12, 192)  0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, 12, 12, 192)  0           batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation_154 (Activation)     (None, 12, 12, 192)  0           batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 12, 12, 768)  0           activation_145[0][0]             \n",
      "                                                                 activation_148[0][0]             \n",
      "                                                                 activation_153[0][0]             \n",
      "                                                                 activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 12, 12, 192)  576         conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 12, 12, 192)  0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 12, 12, 192)  258048      activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, 12, 12, 192)  576         conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, 12, 12, 192)  0           batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 12, 12, 192)  258048      activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, 12, 12, 192)  576         conv2d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, 12, 12, 192)  576         conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, 12, 12, 192)  0           batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, 12, 12, 192)  0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, 12, 12, 192)  258048      activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, 12, 12, 192)  258048      activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 12, 12, 192)  576         conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, 12, 12, 192)  576         conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 12, 12, 192)  0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, 12, 12, 192)  0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_16 (AveragePo (None, 12, 12, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, 12, 12, 192)  258048      activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, 12, 12, 192)  258048      activation_162[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_16[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, 12, 12, 192)  576         conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 12, 12, 192)  576         conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 12, 12, 192)  576         conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 12, 12, 192)  576         conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, 12, 12, 192)  0           batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 12, 12, 192)  0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, 12, 12, 192)  0           batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, 12, 12, 192)  0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 12, 12, 768)  0           activation_155[0][0]             \n",
      "                                                                 activation_158[0][0]             \n",
      "                                                                 activation_163[0][0]             \n",
      "                                                                 activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 12, 12, 192)  576         conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, 12, 12, 192)  0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, 12, 12, 192)  258048      activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_168 (BatchN (None, 12, 12, 192)  576         conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, 12, 12, 192)  0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 12, 12, 192)  258048      activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 12, 12, 192)  576         conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 12, 12, 192)  576         conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, 12, 12, 192)  0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, 12, 12, 192)  0           batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, 5, 5, 320)    552960      activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 5, 5, 192)    331776      activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 5, 5, 320)    960         conv2d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, 5, 5, 192)    576         conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, 5, 5, 320)    0           batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, 5, 5, 192)    0           batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 5, 5, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 5, 5, 1280)   0           activation_166[0][0]             \n",
      "                                                                 activation_170[0][0]             \n",
      "                                                                 max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, 5, 5, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, 5, 5, 448)    1344        conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, 5, 5, 448)    0           batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, 5, 5, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, 5, 5, 384)    1548288     activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, 5, 5, 384)    1152        conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, 5, 5, 384)    1152        conv2d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, 5, 5, 384)    0           batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, 5, 5, 384)    0           batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, 5, 5, 384)    442368      activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, 5, 5, 384)    442368      activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, 5, 5, 384)    442368      activation_176[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_186 (Conv2D)             (None, 5, 5, 384)    442368      activation_176[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_17 (AveragePo (None, 5, 5, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 5, 5, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, 5, 5, 384)    1152        conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, 5, 5, 384)    1152        conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, 5, 5, 384)    1152        conv2d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, 5, 5, 384)    1152        conv2d_186[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_187 (Conv2D)             (None, 5, 5, 192)    245760      average_pooling2d_17[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, 5, 5, 320)    960         conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, 5, 5, 384)    0           batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, 5, 5, 384)    0           batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_177 (Activation)     (None, 5, 5, 384)    0           batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation_178 (Activation)     (None, 5, 5, 384)    0           batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, 5, 5, 192)    576         conv2d_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, 5, 5, 320)    0           batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 5, 5, 768)    0           activation_173[0][0]             \n",
      "                                                                 activation_174[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 5, 5, 768)    0           activation_177[0][0]             \n",
      "                                                                 activation_178[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_179 (Activation)     (None, 5, 5, 192)    0           batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 5, 5, 2048)   0           activation_171[0][0]             \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_3[0][0]              \n",
      "                                                                 activation_179[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_192 (Conv2D)             (None, 5, 5, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_184 (BatchN (None, 5, 5, 448)    1344        conv2d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_184 (Activation)     (None, 5, 5, 448)    0           batch_normalization_184[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_189 (Conv2D)             (None, 5, 5, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_193 (Conv2D)             (None, 5, 5, 384)    1548288     activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, 5, 5, 384)    1152        conv2d_189[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_185 (BatchN (None, 5, 5, 384)    1152        conv2d_193[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, 5, 5, 384)    0           batch_normalization_181[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_185 (Activation)     (None, 5, 5, 384)    0           batch_normalization_185[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_190 (Conv2D)             (None, 5, 5, 384)    442368      activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_191 (Conv2D)             (None, 5, 5, 384)    442368      activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_194 (Conv2D)             (None, 5, 5, 384)    442368      activation_185[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_195 (Conv2D)             (None, 5, 5, 384)    442368      activation_185[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_18 (AveragePo (None, 5, 5, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_188 (Conv2D)             (None, 5, 5, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, 5, 5, 384)    1152        conv2d_190[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_183 (BatchN (None, 5, 5, 384)    1152        conv2d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_186 (BatchN (None, 5, 5, 384)    1152        conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_187 (BatchN (None, 5, 5, 384)    1152        conv2d_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_196 (Conv2D)             (None, 5, 5, 192)    393216      average_pooling2d_18[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, 5, 5, 320)    960         conv2d_188[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, 5, 5, 384)    0           batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_183 (Activation)     (None, 5, 5, 384)    0           batch_normalization_183[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_186 (Activation)     (None, 5, 5, 384)    0           batch_normalization_186[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_187 (Activation)     (None, 5, 5, 384)    0           batch_normalization_187[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_188 (BatchN (None, 5, 5, 192)    576         conv2d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, 5, 5, 320)    0           batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 5, 5, 768)    0           activation_182[0][0]             \n",
      "                                                                 activation_183[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 5, 5, 768)    0           activation_186[0][0]             \n",
      "                                                                 activation_187[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_188 (Activation)     (None, 5, 5, 192)    0           batch_normalization_188[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 5, 5, 2048)   0           activation_180[0][0]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_4[0][0]              \n",
      "                                                                 activation_188[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 5, 5, 2048)   0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 5, 5, 2048)   0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 5, 5, 2048)   0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 5, 5, 2048)   0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 5, 5, 2048)   0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 5, 5, 2048)   0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 5, 5, 2048)   0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 5, 5, 2048)   0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_205 (Conv2D)             (None, 5, 5, 2)      4096        dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_206 (Conv2D)             (None, 5, 5, 3)      6144        dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_207 (Conv2D)             (None, 5, 5, 5)      10240       dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_208 (Conv2D)             (None, 5, 5, 4)      8192        dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_209 (Conv2D)             (None, 5, 5, 3)      6144        dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_210 (Conv2D)             (None, 5, 5, 3)      6144        dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_212 (Conv2D)             (None, 5, 5, 3)      6144        dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_211 (Conv2D)             (None, 5, 5, 4)      8192        dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_17 (Gl (None, 2)            0           conv2d_205[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_18 (Gl (None, 3)            0           conv2d_206[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_19 (Gl (None, 5)            0           conv2d_207[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_20 (Gl (None, 4)            0           conv2d_208[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_21 (Gl (None, 3)            0           conv2d_209[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_22 (Gl (None, 3)            0           conv2d_210[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_24 (Gl (None, 3)            0           conv2d_212[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_23 (Gl (None, 4)            0           conv2d_211[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "gender_output (Activation)      (None, 2)            0           global_average_pooling2d_17[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "image_quality_output (Activatio (None, 3)            0           global_average_pooling2d_18[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "age_output (Activation)         (None, 5)            0           global_average_pooling2d_19[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "weight_output (Activation)      (None, 4)            0           global_average_pooling2d_20[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "bag_output (Activation)         (None, 3)            0           global_average_pooling2d_21[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "footwear_output (Activation)    (None, 3)            0           global_average_pooling2d_22[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "pose_output (Activation)        (None, 3)            0           global_average_pooling2d_24[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "emotion_output (Activation)     (None, 4)            0           global_average_pooling2d_23[0][0]\n",
      "==================================================================================================\n",
      "Total params: 21,858,080\n",
      "Trainable params: 21,823,648\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss=losses,\n",
    "              optimizer=Adam(lr=0.01),loss_weights=lossWeights,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import *\n",
    "\n",
    "clr_triangular = CyclicLR(mode='triangular')\n",
    "filepath = \"model.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=2, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint,clr_triangular]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 86s - loss: 7.9121 - gender_output_loss: 0.6904 - image_quality_output_loss: 0.9882 - age_output_loss: 1.4503 - weight_output_loss: 0.9936 - bag_output_loss: 0.9306 - footwear_output_loss: 0.9903 - pose_output_loss: 0.9408 - emotion_output_loss: 0.9280 - gender_output_acc: 0.5266 - image_quality_output_acc: 0.5503 - age_output_acc: 0.3911 - weight_output_acc: 0.6345 - bag_output_acc: 0.5523 - footwear_output_acc: 0.5242 - pose_output_acc: 0.6161 - emotion_output_acc: 0.7117 - val_loss: 7.8591 - val_gender_output_loss: 0.6845 - val_image_quality_output_loss: 0.9886 - val_age_output_loss: 1.4303 - val_weight_output_loss: 1.0041 - val_bag_output_loss: 0.9272 - val_footwear_output_loss: 0.9868 - val_pose_output_loss: 0.9211 - val_emotion_output_loss: 0.9165 - val_gender_output_acc: 0.5531 - val_image_quality_output_acc: 0.5432 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6379 - val_bag_output_acc: 0.5590 - val_footwear_output_acc: 0.5218 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7103\n",
      "\n",
      "Epoch 00001: loss improved from inf to 7.91210, saving model to model.h5\n",
      "Epoch 2/200\n",
      " - 84s - loss: 7.8009 - gender_output_loss: 0.6774 - image_quality_output_loss: 0.9800 - age_output_loss: 1.4387 - weight_output_loss: 0.9800 - bag_output_loss: 0.9152 - footwear_output_loss: 0.9612 - pose_output_loss: 0.9334 - emotion_output_loss: 0.9151 - gender_output_acc: 0.5653 - image_quality_output_acc: 0.5543 - age_output_acc: 0.3978 - weight_output_acc: 0.6346 - bag_output_acc: 0.5622 - footwear_output_acc: 0.5519 - pose_output_acc: 0.6164 - emotion_output_acc: 0.7119 - val_loss: 8.4751 - val_gender_output_loss: 0.6828 - val_image_quality_output_loss: 1.0029 - val_age_output_loss: 1.5019 - val_weight_output_loss: 1.1170 - val_bag_output_loss: 0.9890 - val_footwear_output_loss: 1.1887 - val_pose_output_loss: 0.9607 - val_emotion_output_loss: 1.0322 - val_gender_output_acc: 0.5551 - val_image_quality_output_acc: 0.5432 - val_age_output_acc: 0.3810 - val_weight_output_acc: 0.6285 - val_bag_output_acc: 0.5536 - val_footwear_output_acc: 0.4544 - val_pose_output_acc: 0.6136 - val_emotion_output_acc: 0.6959\n",
      "\n",
      "Epoch 00002: loss improved from 7.91210 to 7.80095, saving model to model.h5\n",
      "Epoch 3/200\n",
      " - 84s - loss: 7.7160 - gender_output_loss: 0.6477 - image_quality_output_loss: 0.9808 - age_output_loss: 1.4273 - weight_output_loss: 0.9760 - bag_output_loss: 0.9042 - footwear_output_loss: 0.9353 - pose_output_loss: 0.9331 - emotion_output_loss: 0.9117 - gender_output_acc: 0.5851 - image_quality_output_acc: 0.5545 - age_output_acc: 0.3968 - weight_output_acc: 0.6352 - bag_output_acc: 0.5648 - footwear_output_acc: 0.5687 - pose_output_acc: 0.6162 - emotion_output_acc: 0.7117 - val_loss: 7.8940 - val_gender_output_loss: 0.6876 - val_image_quality_output_loss: 0.9891 - val_age_output_loss: 1.4240 - val_weight_output_loss: 0.9983 - val_bag_output_loss: 0.9319 - val_footwear_output_loss: 1.0310 - val_pose_output_loss: 0.9209 - val_emotion_output_loss: 0.9112 - val_gender_output_acc: 0.5357 - val_image_quality_output_acc: 0.5432 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6374 - val_bag_output_acc: 0.5392 - val_footwear_output_acc: 0.4702 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7103\n",
      "\n",
      "Epoch 00003: loss improved from 7.80095 to 7.71604, saving model to model.h5\n",
      "Epoch 4/200\n",
      " - 84s - loss: 7.6820 - gender_output_loss: 0.6379 - image_quality_output_loss: 0.9819 - age_output_loss: 1.4223 - weight_output_loss: 0.9767 - bag_output_loss: 0.8976 - footwear_output_loss: 0.9267 - pose_output_loss: 0.9315 - emotion_output_loss: 0.9075 - gender_output_acc: 0.5930 - image_quality_output_acc: 0.5542 - age_output_acc: 0.3966 - weight_output_acc: 0.6349 - bag_output_acc: 0.5641 - footwear_output_acc: 0.5705 - pose_output_acc: 0.6164 - emotion_output_acc: 0.7121 - val_loss: 8.0036 - val_gender_output_loss: 0.6800 - val_image_quality_output_loss: 0.9923 - val_age_output_loss: 1.4559 - val_weight_output_loss: 1.0586 - val_bag_output_loss: 0.9572 - val_footwear_output_loss: 0.9899 - val_pose_output_loss: 0.9320 - val_emotion_output_loss: 0.9375 - val_gender_output_acc: 0.5548 - val_image_quality_output_acc: 0.5427 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6374 - val_bag_output_acc: 0.5585 - val_footwear_output_acc: 0.5312 - val_pose_output_acc: 0.6240 - val_emotion_output_acc: 0.7093\n",
      "\n",
      "Epoch 00004: loss improved from 7.71604 to 7.68204, saving model to model.h5\n",
      "Epoch 5/200\n",
      " - 84s - loss: 7.5619 - gender_output_loss: 0.6084 - image_quality_output_loss: 0.9771 - age_output_loss: 1.4097 - weight_output_loss: 0.9736 - bag_output_loss: 0.8766 - footwear_output_loss: 0.8968 - pose_output_loss: 0.9213 - emotion_output_loss: 0.8985 - gender_output_acc: 0.6149 - image_quality_output_acc: 0.5540 - age_output_acc: 0.3964 - weight_output_acc: 0.6333 - bag_output_acc: 0.5831 - footwear_output_acc: 0.5908 - pose_output_acc: 0.6165 - emotion_output_acc: 0.7118 - val_loss: 7.8524 - val_gender_output_loss: 0.6615 - val_image_quality_output_loss: 0.9838 - val_age_output_loss: 1.4691 - val_weight_output_loss: 1.0229 - val_bag_output_loss: 0.9247 - val_footwear_output_loss: 0.9371 - val_pose_output_loss: 0.9435 - val_emotion_output_loss: 0.9098 - val_gender_output_acc: 0.5821 - val_image_quality_output_acc: 0.5432 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6374 - val_bag_output_acc: 0.5303 - val_footwear_output_acc: 0.5655 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7103\n",
      "\n",
      "Epoch 00005: loss improved from 7.68204 to 7.56194, saving model to model.h5\n",
      "Epoch 6/200\n",
      " - 84s - loss: 7.5828 - gender_output_loss: 0.6121 - image_quality_output_loss: 0.9783 - age_output_loss: 1.4120 - weight_output_loss: 0.9750 - bag_output_loss: 0.8847 - footwear_output_loss: 0.9021 - pose_output_loss: 0.9158 - emotion_output_loss: 0.9028 - gender_output_acc: 0.6132 - image_quality_output_acc: 0.5543 - age_output_acc: 0.3970 - weight_output_acc: 0.6345 - bag_output_acc: 0.5762 - footwear_output_acc: 0.5881 - pose_output_acc: 0.6163 - emotion_output_acc: 0.7116 - val_loss: 8.1770 - val_gender_output_loss: 0.6584 - val_image_quality_output_loss: 0.9964 - val_age_output_loss: 1.4798 - val_weight_output_loss: 1.0587 - val_bag_output_loss: 1.0685 - val_footwear_output_loss: 0.9311 - val_pose_output_loss: 1.0684 - val_emotion_output_loss: 0.9156 - val_gender_output_acc: 0.5878 - val_image_quality_output_acc: 0.5432 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6379 - val_bag_output_acc: 0.4201 - val_footwear_output_acc: 0.5699 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7103\n",
      "\n",
      "Epoch 00006: loss did not improve from 7.56194\n",
      "Epoch 7/200\n",
      " - 84s - loss: 7.4309 - gender_output_loss: 0.5845 - image_quality_output_loss: 0.9742 - age_output_loss: 1.4027 - weight_output_loss: 0.9712 - bag_output_loss: 0.8666 - footwear_output_loss: 0.8676 - pose_output_loss: 0.8695 - emotion_output_loss: 0.8947 - gender_output_acc: 0.6297 - image_quality_output_acc: 0.5536 - age_output_acc: 0.3959 - weight_output_acc: 0.6338 - bag_output_acc: 0.6010 - footwear_output_acc: 0.6123 - pose_output_acc: 0.6161 - emotion_output_acc: 0.7119 - val_loss: 8.3060 - val_gender_output_loss: 0.7302 - val_image_quality_output_loss: 0.9997 - val_age_output_loss: 1.4359 - val_weight_output_loss: 1.0537 - val_bag_output_loss: 0.9490 - val_footwear_output_loss: 1.0469 - val_pose_output_loss: 1.1652 - val_emotion_output_loss: 0.9255 - val_gender_output_acc: 0.6029 - val_image_quality_output_acc: 0.5392 - val_age_output_acc: 0.3542 - val_weight_output_acc: 0.6369 - val_bag_output_acc: 0.5585 - val_footwear_output_acc: 0.5079 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7103\n",
      "\n",
      "Epoch 00007: loss improved from 7.56194 to 7.43092, saving model to model.h5\n",
      "Epoch 8/200\n",
      " - 84s - loss: 7.3564 - gender_output_loss: 0.5823 - image_quality_output_loss: 0.9744 - age_output_loss: 1.4000 - weight_output_loss: 0.9709 - bag_output_loss: 0.8638 - footwear_output_loss: 0.8587 - pose_output_loss: 0.8142 - emotion_output_loss: 0.8921 - gender_output_acc: 0.6307 - image_quality_output_acc: 0.5536 - age_output_acc: 0.3953 - weight_output_acc: 0.6342 - bag_output_acc: 0.5970 - footwear_output_acc: 0.6219 - pose_output_acc: 0.6277 - emotion_output_acc: 0.7115 - val_loss: 7.2196 - val_gender_output_loss: 0.5683 - val_image_quality_output_loss: 0.9737 - val_age_output_loss: 1.3707 - val_weight_output_loss: 0.9872 - val_bag_output_loss: 0.8675 - val_footwear_output_loss: 0.8202 - val_pose_output_loss: 0.7433 - val_emotion_output_loss: 0.8887 - val_gender_output_acc: 0.6364 - val_image_quality_output_acc: 0.5451 - val_age_output_acc: 0.4167 - val_weight_output_acc: 0.6369 - val_bag_output_acc: 0.5982 - val_footwear_output_acc: 0.6493 - val_pose_output_acc: 0.6622 - val_emotion_output_acc: 0.7103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00008: loss improved from 7.43092 to 7.35635, saving model to model.h5\n",
      "Epoch 9/200\n",
      " - 84s - loss: 7.2633 - gender_output_loss: 0.5775 - image_quality_output_loss: 0.9743 - age_output_loss: 1.4004 - weight_output_loss: 0.9754 - bag_output_loss: 0.8636 - footwear_output_loss: 0.8315 - pose_output_loss: 0.7537 - emotion_output_loss: 0.8869 - gender_output_acc: 0.6345 - image_quality_output_acc: 0.5549 - age_output_acc: 0.3964 - weight_output_acc: 0.6337 - bag_output_acc: 0.6020 - footwear_output_acc: 0.6291 - pose_output_acc: 0.6428 - emotion_output_acc: 0.7115 - val_loss: 7.3911 - val_gender_output_loss: 0.5850 - val_image_quality_output_loss: 0.9794 - val_age_output_loss: 1.3957 - val_weight_output_loss: 0.9928 - val_bag_output_loss: 0.9035 - val_footwear_output_loss: 0.8512 - val_pose_output_loss: 0.7794 - val_emotion_output_loss: 0.9042 - val_gender_output_acc: 0.6186 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.4023 - val_weight_output_acc: 0.6374 - val_bag_output_acc: 0.5724 - val_footwear_output_acc: 0.6305 - val_pose_output_acc: 0.6359 - val_emotion_output_acc: 0.7073\n",
      "\n",
      "Epoch 00009: loss improved from 7.35635 to 7.26325, saving model to model.h5\n",
      "Epoch 10/200\n",
      " - 84s - loss: 7.0834 - gender_output_loss: 0.5521 - image_quality_output_loss: 0.9705 - age_output_loss: 1.3920 - weight_output_loss: 0.9705 - bag_output_loss: 0.8504 - footwear_output_loss: 0.7908 - pose_output_loss: 0.6826 - emotion_output_loss: 0.8745 - gender_output_acc: 0.6520 - image_quality_output_acc: 0.5576 - age_output_acc: 0.3935 - weight_output_acc: 0.6354 - bag_output_acc: 0.6129 - footwear_output_acc: 0.6488 - pose_output_acc: 0.6742 - emotion_output_acc: 0.7118 - val_loss: 7.2826 - val_gender_output_loss: 0.5803 - val_image_quality_output_loss: 0.9802 - val_age_output_loss: 1.3819 - val_weight_output_loss: 0.9829 - val_bag_output_loss: 0.8714 - val_footwear_output_loss: 0.7854 - val_pose_output_loss: 0.7948 - val_emotion_output_loss: 0.9055 - val_gender_output_acc: 0.6431 - val_image_quality_output_acc: 0.5422 - val_age_output_acc: 0.4142 - val_weight_output_acc: 0.6374 - val_bag_output_acc: 0.5997 - val_footwear_output_acc: 0.6528 - val_pose_output_acc: 0.5987 - val_emotion_output_acc: 0.7103\n",
      "\n",
      "Epoch 00010: loss improved from 7.26325 to 7.08342, saving model to model.h5\n",
      "Epoch 11/200\n",
      " - 84s - loss: 7.2038 - gender_output_loss: 0.5728 - image_quality_output_loss: 0.9735 - age_output_loss: 1.4016 - weight_output_loss: 0.9778 - bag_output_loss: 0.8587 - footwear_output_loss: 0.8226 - pose_output_loss: 0.7129 - emotion_output_loss: 0.8838 - gender_output_acc: 0.6436 - image_quality_output_acc: 0.5547 - age_output_acc: 0.3905 - weight_output_acc: 0.6346 - bag_output_acc: 0.6052 - footwear_output_acc: 0.6293 - pose_output_acc: 0.6631 - emotion_output_acc: 0.7111 - val_loss: 7.5628 - val_gender_output_loss: 0.5722 - val_image_quality_output_loss: 0.9797 - val_age_output_loss: 1.3921 - val_weight_output_loss: 1.0083 - val_bag_output_loss: 0.8970 - val_footwear_output_loss: 0.8110 - val_pose_output_loss: 0.9966 - val_emotion_output_loss: 0.9059 - val_gender_output_acc: 0.6508 - val_image_quality_output_acc: 0.5407 - val_age_output_acc: 0.3934 - val_weight_output_acc: 0.6374 - val_bag_output_acc: 0.5739 - val_footwear_output_acc: 0.6513 - val_pose_output_acc: 0.6369 - val_emotion_output_acc: 0.7088\n",
      "\n",
      "Epoch 00011: loss did not improve from 7.08342\n",
      "Epoch 12/200\n",
      " - 84s - loss: 7.0468 - gender_output_loss: 0.5454 - image_quality_output_loss: 0.9685 - age_output_loss: 1.3914 - weight_output_loss: 0.9713 - bag_output_loss: 0.8443 - footwear_output_loss: 0.7847 - pose_output_loss: 0.6688 - emotion_output_loss: 0.8724 - gender_output_acc: 0.6576 - image_quality_output_acc: 0.5550 - age_output_acc: 0.3976 - weight_output_acc: 0.6346 - bag_output_acc: 0.6190 - footwear_output_acc: 0.6549 - pose_output_acc: 0.6778 - emotion_output_acc: 0.7119 - val_loss: 9.4597 - val_gender_output_loss: 0.6925 - val_image_quality_output_loss: 1.0968 - val_age_output_loss: 1.6089 - val_weight_output_loss: 1.3852 - val_bag_output_loss: 1.0987 - val_footwear_output_loss: 1.0969 - val_pose_output_loss: 1.0953 - val_emotion_output_loss: 1.3853 - val_gender_output_acc: 0.5020 - val_image_quality_output_acc: 0.5432 - val_age_output_acc: 0.1949 - val_weight_output_acc: 0.6374 - val_bag_output_acc: 0.3383 - val_footwear_output_acc: 0.3750 - val_pose_output_acc: 0.1771 - val_emotion_output_acc: 0.1091\n",
      "\n",
      "Epoch 00012: loss improved from 7.08342 to 7.04680, saving model to model.h5\n",
      "Epoch 13/200\n",
      " - 84s - loss: 7.0659 - gender_output_loss: 0.5541 - image_quality_output_loss: 0.9642 - age_output_loss: 1.3954 - weight_output_loss: 0.9714 - bag_output_loss: 0.8442 - footwear_output_loss: 0.7889 - pose_output_loss: 0.6754 - emotion_output_loss: 0.8722 - gender_output_acc: 0.6523 - image_quality_output_acc: 0.5569 - age_output_acc: 0.3989 - weight_output_acc: 0.6350 - bag_output_acc: 0.6156 - footwear_output_acc: 0.6526 - pose_output_acc: 0.6801 - emotion_output_acc: 0.7115 - val_loss: 6.9873 - val_gender_output_loss: 0.5518 - val_image_quality_output_loss: 0.9585 - val_age_output_loss: 1.3757 - val_weight_output_loss: 0.9847 - val_bag_output_loss: 0.8517 - val_footwear_output_loss: 0.7464 - val_pose_output_loss: 0.6337 - val_emotion_output_loss: 0.8847 - val_gender_output_acc: 0.6545 - val_image_quality_output_acc: 0.5471 - val_age_output_acc: 0.4142 - val_weight_output_acc: 0.6369 - val_bag_output_acc: 0.6081 - val_footwear_output_acc: 0.6845 - val_pose_output_acc: 0.6999 - val_emotion_output_acc: 0.7103\n",
      "\n",
      "Epoch 00013: loss did not improve from 7.04680\n",
      "Epoch 14/200\n",
      " - 84s - loss: 7.0138 - gender_output_loss: 0.5416 - image_quality_output_loss: 0.9554 - age_output_loss: 1.3934 - weight_output_loss: 0.9700 - bag_output_loss: 0.8382 - footwear_output_loss: 0.7832 - pose_output_loss: 0.6587 - emotion_output_loss: 0.8733 - gender_output_acc: 0.6616 - image_quality_output_acc: 0.5556 - age_output_acc: 0.3958 - weight_output_acc: 0.6344 - bag_output_acc: 0.6220 - footwear_output_acc: 0.6543 - pose_output_acc: 0.6883 - emotion_output_acc: 0.7118 - val_loss: 7.5706 - val_gender_output_loss: 0.6732 - val_image_quality_output_loss: 0.9873 - val_age_output_loss: 1.4277 - val_weight_output_loss: 1.0012 - val_bag_output_loss: 0.9119 - val_footwear_output_loss: 0.8936 - val_pose_output_loss: 0.7534 - val_emotion_output_loss: 0.9224 - val_gender_output_acc: 0.5885 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3978 - val_weight_output_acc: 0.6374 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5987 - val_pose_output_acc: 0.6448 - val_emotion_output_acc: 0.7103\n",
      "\n",
      "Epoch 00014: loss improved from 7.04680 to 7.01384, saving model to model.h5\n",
      "Epoch 15/200\n",
      " - 84s - loss: 6.9204 - gender_output_loss: 0.5288 - image_quality_output_loss: 0.9380 - age_output_loss: 1.3849 - weight_output_loss: 0.9677 - bag_output_loss: 0.8281 - footwear_output_loss: 0.7680 - pose_output_loss: 0.6372 - emotion_output_loss: 0.8677 - gender_output_acc: 0.6650 - image_quality_output_acc: 0.5600 - age_output_acc: 0.4010 - weight_output_acc: 0.6352 - bag_output_acc: 0.6352 - footwear_output_acc: 0.6632 - pose_output_acc: 0.6957 - emotion_output_acc: 0.7119 - val_loss: 7.3344 - val_gender_output_loss: 0.6430 - val_image_quality_output_loss: 0.9907 - val_age_output_loss: 1.3932 - val_weight_output_loss: 0.9921 - val_bag_output_loss: 0.9482 - val_footwear_output_loss: 0.8063 - val_pose_output_loss: 0.6571 - val_emotion_output_loss: 0.9037 - val_gender_output_acc: 0.6099 - val_image_quality_output_acc: 0.4683 - val_age_output_acc: 0.4082 - val_weight_output_acc: 0.6374 - val_bag_output_acc: 0.5258 - val_footwear_output_acc: 0.6384 - val_pose_output_acc: 0.6811 - val_emotion_output_acc: 0.7098\n",
      "\n",
      "Epoch 00015: loss improved from 7.01384 to 6.92039, saving model to model.h5\n",
      "Epoch 16/200\n",
      " - 84s - loss: 6.9877 - gender_output_loss: 0.5425 - image_quality_output_loss: 0.9381 - age_output_loss: 1.3899 - weight_output_loss: 0.9710 - bag_output_loss: 0.8403 - footwear_output_loss: 0.7809 - pose_output_loss: 0.6518 - emotion_output_loss: 0.8731 - gender_output_acc: 0.6598 - image_quality_output_acc: 0.5561 - age_output_acc: 0.3951 - weight_output_acc: 0.6341 - bag_output_acc: 0.6215 - footwear_output_acc: 0.6577 - pose_output_acc: 0.6905 - emotion_output_acc: 0.7115 - val_loss: 7.0671 - val_gender_output_loss: 0.5576 - val_image_quality_output_loss: 0.9590 - val_age_output_loss: 1.3868 - val_weight_output_loss: 0.9873 - val_bag_output_loss: 0.8735 - val_footwear_output_loss: 0.7830 - val_pose_output_loss: 0.6339 - val_emotion_output_loss: 0.8860 - val_gender_output_acc: 0.6649 - val_image_quality_output_acc: 0.5516 - val_age_output_acc: 0.4167 - val_weight_output_acc: 0.6374 - val_bag_output_acc: 0.6091 - val_footwear_output_acc: 0.6687 - val_pose_output_acc: 0.7063 - val_emotion_output_acc: 0.7103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00016: loss did not improve from 6.92039\n",
      "Epoch 17/200\n",
      " - 84s - loss: 6.8015 - gender_output_loss: 0.5113 - image_quality_output_loss: 0.9136 - age_output_loss: 1.3813 - weight_output_loss: 0.9637 - bag_output_loss: 0.8163 - footwear_output_loss: 0.7522 - pose_output_loss: 0.6034 - emotion_output_loss: 0.8597 - gender_output_acc: 0.6756 - image_quality_output_acc: 0.5719 - age_output_acc: 0.4002 - weight_output_acc: 0.6352 - bag_output_acc: 0.6430 - footwear_output_acc: 0.6694 - pose_output_acc: 0.7196 - emotion_output_acc: 0.7120 - val_loss: 7.2682 - val_gender_output_loss: 0.5753 - val_image_quality_output_loss: 0.9611 - val_age_output_loss: 1.3819 - val_weight_output_loss: 0.9804 - val_bag_output_loss: 0.8732 - val_footwear_output_loss: 0.8550 - val_pose_output_loss: 0.7323 - val_emotion_output_loss: 0.9090 - val_gender_output_acc: 0.6471 - val_image_quality_output_acc: 0.5184 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6379 - val_bag_output_acc: 0.6146 - val_footwear_output_acc: 0.6270 - val_pose_output_acc: 0.6349 - val_emotion_output_acc: 0.7103\n",
      "\n",
      "Epoch 00017: loss improved from 6.92039 to 6.80150, saving model to model.h5\n",
      "Epoch 18/200\n",
      " - 84s - loss: 6.8522 - gender_output_loss: 0.5234 - image_quality_output_loss: 0.9116 - age_output_loss: 1.3803 - weight_output_loss: 0.9635 - bag_output_loss: 0.8277 - footwear_output_loss: 0.7632 - pose_output_loss: 0.6184 - emotion_output_loss: 0.8640 - gender_output_acc: 0.6668 - image_quality_output_acc: 0.5681 - age_output_acc: 0.3995 - weight_output_acc: 0.6343 - bag_output_acc: 0.6358 - footwear_output_acc: 0.6624 - pose_output_acc: 0.7095 - emotion_output_acc: 0.7118 - val_loss: 6.8349 - val_gender_output_loss: 0.5373 - val_image_quality_output_loss: 0.9058 - val_age_output_loss: 1.3716 - val_weight_output_loss: 0.9769 - val_bag_output_loss: 0.8293 - val_footwear_output_loss: 0.7525 - val_pose_output_loss: 0.5845 - val_emotion_output_loss: 0.8770 - val_gender_output_acc: 0.6667 - val_image_quality_output_acc: 0.5640 - val_age_output_acc: 0.4187 - val_weight_output_acc: 0.6384 - val_bag_output_acc: 0.6285 - val_footwear_output_acc: 0.6736 - val_pose_output_acc: 0.7336 - val_emotion_output_acc: 0.7103\n",
      "\n",
      "Epoch 00018: loss did not improve from 6.80150\n",
      "Epoch 19/200\n",
      " - 84s - loss: 6.8174 - gender_output_loss: 0.5133 - image_quality_output_loss: 0.9136 - age_output_loss: 1.3789 - weight_output_loss: 0.9643 - bag_output_loss: 0.8255 - footwear_output_loss: 0.7567 - pose_output_loss: 0.6029 - emotion_output_loss: 0.8621 - gender_output_acc: 0.6733 - image_quality_output_acc: 0.5661 - age_output_acc: 0.4039 - weight_output_acc: 0.6347 - bag_output_acc: 0.6372 - footwear_output_acc: 0.6664 - pose_output_acc: 0.7267 - emotion_output_acc: 0.7123 - val_loss: 7.3555 - val_gender_output_loss: 0.5987 - val_image_quality_output_loss: 0.9559 - val_age_output_loss: 1.4276 - val_weight_output_loss: 1.0243 - val_bag_output_loss: 0.9369 - val_footwear_output_loss: 0.8277 - val_pose_output_loss: 0.6450 - val_emotion_output_loss: 0.9395 - val_gender_output_acc: 0.6362 - val_image_quality_output_acc: 0.5521 - val_age_output_acc: 0.4162 - val_weight_output_acc: 0.6374 - val_bag_output_acc: 0.5357 - val_footwear_output_acc: 0.6453 - val_pose_output_acc: 0.7113 - val_emotion_output_acc: 0.7103\n",
      "\n",
      "Epoch 00019: loss did not improve from 6.80150\n",
      "Epoch 20/200\n",
      " - 84s - loss: 6.5934 - gender_output_loss: 0.3949 - image_quality_output_loss: 0.8997 - age_output_loss: 1.3708 - weight_output_loss: 0.9559 - bag_output_loss: 0.8132 - footwear_output_loss: 0.7337 - pose_output_loss: 0.5693 - emotion_output_loss: 0.8558 - gender_output_acc: 0.8271 - image_quality_output_acc: 0.5729 - age_output_acc: 0.4051 - weight_output_acc: 0.6346 - bag_output_acc: 0.6460 - footwear_output_acc: 0.6796 - pose_output_acc: 0.7423 - emotion_output_acc: 0.7117 - val_loss: 6.8712 - val_gender_output_loss: 0.4435 - val_image_quality_output_loss: 0.9764 - val_age_output_loss: 1.3780 - val_weight_output_loss: 0.9846 - val_bag_output_loss: 0.8340 - val_footwear_output_loss: 0.7654 - val_pose_output_loss: 0.6022 - val_emotion_output_loss: 0.8871 - val_gender_output_acc: 0.7927 - val_image_quality_output_acc: 0.5456 - val_age_output_acc: 0.4107 - val_weight_output_acc: 0.6374 - val_bag_output_acc: 0.6310 - val_footwear_output_acc: 0.6756 - val_pose_output_acc: 0.7207 - val_emotion_output_acc: 0.7103\n",
      "\n",
      "Epoch 00020: loss improved from 6.80150 to 6.59335, saving model to model.h5\n",
      "Epoch 21/200\n",
      " - 84s - loss: 6.7211 - gender_output_loss: 0.4035 - image_quality_output_loss: 0.9104 - age_output_loss: 1.3809 - weight_output_loss: 0.9657 - bag_output_loss: 0.8297 - footwear_output_loss: 0.7618 - pose_output_loss: 0.6034 - emotion_output_loss: 0.8656 - gender_output_acc: 0.8152 - image_quality_output_acc: 0.5639 - age_output_acc: 0.3995 - weight_output_acc: 0.6350 - bag_output_acc: 0.6337 - footwear_output_acc: 0.6676 - pose_output_acc: 0.7286 - emotion_output_acc: 0.7119 - val_loss: 6.8149 - val_gender_output_loss: 0.4376 - val_image_quality_output_loss: 0.9040 - val_age_output_loss: 1.3726 - val_weight_output_loss: 0.9770 - val_bag_output_loss: 0.8657 - val_footwear_output_loss: 0.7510 - val_pose_output_loss: 0.6132 - val_emotion_output_loss: 0.8938 - val_gender_output_acc: 0.8165 - val_image_quality_output_acc: 0.5694 - val_age_output_acc: 0.4196 - val_weight_output_acc: 0.6384 - val_bag_output_acc: 0.6071 - val_footwear_output_acc: 0.6771 - val_pose_output_acc: 0.7743 - val_emotion_output_acc: 0.7103\n",
      "\n",
      "Epoch 00021: loss did not improve from 6.59335\n",
      "Epoch 22/200\n",
      " - 84s - loss: 6.4737 - gender_output_loss: 0.3218 - image_quality_output_loss: 0.8930 - age_output_loss: 1.3680 - weight_output_loss: 0.9562 - bag_output_loss: 0.8063 - footwear_output_loss: 0.7249 - pose_output_loss: 0.5493 - emotion_output_loss: 0.8541 - gender_output_acc: 0.8638 - image_quality_output_acc: 0.5750 - age_output_acc: 0.4034 - weight_output_acc: 0.6332 - bag_output_acc: 0.6531 - footwear_output_acc: 0.6844 - pose_output_acc: 0.7686 - emotion_output_acc: 0.7120 - val_loss: 7.5656 - val_gender_output_loss: 0.5153 - val_image_quality_output_loss: 0.9242 - val_age_output_loss: 1.3916 - val_weight_output_loss: 1.0008 - val_bag_output_loss: 0.8561 - val_footwear_output_loss: 0.8295 - val_pose_output_loss: 1.1355 - val_emotion_output_loss: 0.9127 - val_gender_output_acc: 0.8001 - val_image_quality_output_acc: 0.5605 - val_age_output_acc: 0.4023 - val_weight_output_acc: 0.6260 - val_bag_output_acc: 0.6245 - val_footwear_output_acc: 0.6582 - val_pose_output_acc: 0.6399 - val_emotion_output_acc: 0.7103\n",
      "\n",
      "Epoch 00022: loss improved from 6.59335 to 6.47367, saving model to model.h5\n",
      "Epoch 23/200\n",
      " - 84s - loss: 6.5185 - gender_output_loss: 0.3269 - image_quality_output_loss: 0.8965 - age_output_loss: 1.3689 - weight_output_loss: 0.9591 - bag_output_loss: 0.8143 - footwear_output_loss: 0.7348 - pose_output_loss: 0.5610 - emotion_output_loss: 0.8569 - gender_output_acc: 0.8574 - image_quality_output_acc: 0.5755 - age_output_acc: 0.4041 - weight_output_acc: 0.6350 - bag_output_acc: 0.6472 - footwear_output_acc: 0.6776 - pose_output_acc: 0.7539 - emotion_output_acc: 0.7119 - val_loss: 6.5928 - val_gender_output_loss: 0.3522 - val_image_quality_output_loss: 0.8971 - val_age_output_loss: 1.3594 - val_weight_output_loss: 0.9745 - val_bag_output_loss: 0.8286 - val_footwear_output_loss: 0.7504 - val_pose_output_loss: 0.5553 - val_emotion_output_loss: 0.8753 - val_gender_output_acc: 0.8457 - val_image_quality_output_acc: 0.5680 - val_age_output_acc: 0.4167 - val_weight_output_acc: 0.6434 - val_bag_output_acc: 0.6334 - val_footwear_output_acc: 0.6915 - val_pose_output_acc: 0.7922 - val_emotion_output_acc: 0.7103\n",
      "\n",
      "Epoch 00023: loss did not improve from 6.47367\n",
      "Epoch 24/200\n",
      " - 84s - loss: 6.4834 - gender_output_loss: 0.3234 - image_quality_output_loss: 0.8951 - age_output_loss: 1.3658 - weight_output_loss: 0.9532 - bag_output_loss: 0.8060 - footwear_output_loss: 0.7364 - pose_output_loss: 0.5490 - emotion_output_loss: 0.8545 - gender_output_acc: 0.8611 - image_quality_output_acc: 0.5760 - age_output_acc: 0.4006 - weight_output_acc: 0.6344 - bag_output_acc: 0.6480 - footwear_output_acc: 0.6800 - pose_output_acc: 0.7627 - emotion_output_acc: 0.7119 - val_loss: 7.0088 - val_gender_output_loss: 0.4725 - val_image_quality_output_loss: 0.9150 - val_age_output_loss: 1.4113 - val_weight_output_loss: 0.9974 - val_bag_output_loss: 0.8647 - val_footwear_output_loss: 0.8150 - val_pose_output_loss: 0.6429 - val_emotion_output_loss: 0.8899 - val_gender_output_acc: 0.8006 - val_image_quality_output_acc: 0.5625 - val_age_output_acc: 0.4008 - val_weight_output_acc: 0.6379 - val_bag_output_acc: 0.6081 - val_footwear_output_acc: 0.6448 - val_pose_output_acc: 0.7649 - val_emotion_output_acc: 0.7103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00024: loss did not improve from 6.47367\n",
      "Epoch 25/200\n",
      " - 84s - loss: 6.2781 - gender_output_loss: 0.2703 - image_quality_output_loss: 0.8768 - age_output_loss: 1.3554 - weight_output_loss: 0.9449 - bag_output_loss: 0.7865 - footwear_output_loss: 0.7091 - pose_output_loss: 0.4856 - emotion_output_loss: 0.8495 - gender_output_acc: 0.8874 - image_quality_output_acc: 0.5808 - age_output_acc: 0.4068 - weight_output_acc: 0.6359 - bag_output_acc: 0.6638 - footwear_output_acc: 0.6951 - pose_output_acc: 0.8105 - emotion_output_acc: 0.7118 - val_loss: 6.7375 - val_gender_output_loss: 0.3856 - val_image_quality_output_loss: 0.9278 - val_age_output_loss: 1.3907 - val_weight_output_loss: 1.0049 - val_bag_output_loss: 0.8407 - val_footwear_output_loss: 0.7879 - val_pose_output_loss: 0.5201 - val_emotion_output_loss: 0.8798 - val_gender_output_acc: 0.8333 - val_image_quality_output_acc: 0.5556 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6384 - val_bag_output_acc: 0.6310 - val_footwear_output_acc: 0.6597 - val_pose_output_acc: 0.7986 - val_emotion_output_acc: 0.7103\n",
      "\n",
      "Epoch 00025: loss improved from 6.47367 to 6.27805, saving model to model.h5\n",
      "Epoch 26/200\n",
      " - 84s - loss: 6.3958 - gender_output_loss: 0.3208 - image_quality_output_loss: 0.8969 - age_output_loss: 1.3705 - weight_output_loss: 0.9528 - bag_output_loss: 0.8038 - footwear_output_loss: 0.7288 - pose_output_loss: 0.4658 - emotion_output_loss: 0.8564 - gender_output_acc: 0.8606 - image_quality_output_acc: 0.5743 - age_output_acc: 0.4068 - weight_output_acc: 0.6346 - bag_output_acc: 0.6502 - footwear_output_acc: 0.6813 - pose_output_acc: 0.8179 - emotion_output_acc: 0.7118 - val_loss: 6.8390 - val_gender_output_loss: 0.5509 - val_image_quality_output_loss: 0.9015 - val_age_output_loss: 1.3767 - val_weight_output_loss: 0.9730 - val_bag_output_loss: 0.8675 - val_footwear_output_loss: 0.7641 - val_pose_output_loss: 0.5202 - val_emotion_output_loss: 0.8851 - val_gender_output_acc: 0.7594 - val_image_quality_output_acc: 0.5625 - val_age_output_acc: 0.4048 - val_weight_output_acc: 0.6429 - val_bag_output_acc: 0.5938 - val_footwear_output_acc: 0.6691 - val_pose_output_acc: 0.7922 - val_emotion_output_acc: 0.7103\n",
      "\n",
      "Epoch 00026: loss did not improve from 6.27805\n",
      "Epoch 27/200\n",
      " - 84s - loss: 6.1468 - gender_output_loss: 0.2598 - image_quality_output_loss: 0.8765 - age_output_loss: 1.3528 - weight_output_loss: 0.9385 - bag_output_loss: 0.7768 - footwear_output_loss: 0.6995 - pose_output_loss: 0.3937 - emotion_output_loss: 0.8492 - gender_output_acc: 0.8920 - image_quality_output_acc: 0.5838 - age_output_acc: 0.4122 - weight_output_acc: 0.6387 - bag_output_acc: 0.6714 - footwear_output_acc: 0.6946 - pose_output_acc: 0.8522 - emotion_output_acc: 0.7115 - val_loss: 8.4039 - val_gender_output_loss: 1.2144 - val_image_quality_output_loss: 1.0152 - val_age_output_loss: 1.4533 - val_weight_output_loss: 1.0104 - val_bag_output_loss: 1.2597 - val_footwear_output_loss: 0.9121 - val_pose_output_loss: 0.5822 - val_emotion_output_loss: 0.9565 - val_gender_output_acc: 0.6915 - val_image_quality_output_acc: 0.5501 - val_age_output_acc: 0.3621 - val_weight_output_acc: 0.6255 - val_bag_output_acc: 0.5699 - val_footwear_output_acc: 0.6543 - val_pose_output_acc: 0.7768 - val_emotion_output_acc: 0.7103\n",
      "\n",
      "Epoch 00027: loss improved from 6.27805 to 6.14681, saving model to model.h5\n",
      "Epoch 28/200\n",
      " - 84s - loss: 6.1719 - gender_output_loss: 0.2604 - image_quality_output_loss: 0.8788 - age_output_loss: 1.3577 - weight_output_loss: 0.9432 - bag_output_loss: 0.7780 - footwear_output_loss: 0.7069 - pose_output_loss: 0.3993 - emotion_output_loss: 0.8475 - gender_output_acc: 0.8899 - image_quality_output_acc: 0.5789 - age_output_acc: 0.4077 - weight_output_acc: 0.6370 - bag_output_acc: 0.6715 - footwear_output_acc: 0.6953 - pose_output_acc: 0.8494 - emotion_output_acc: 0.7120 - val_loss: 6.4723 - val_gender_output_loss: 0.3649 - val_image_quality_output_loss: 0.8866 - val_age_output_loss: 1.3596 - val_weight_output_loss: 0.9704 - val_bag_output_loss: 0.8196 - val_footwear_output_loss: 0.7592 - val_pose_output_loss: 0.4394 - val_emotion_output_loss: 0.8727 - val_gender_output_acc: 0.8487 - val_image_quality_output_acc: 0.5660 - val_age_output_acc: 0.4117 - val_weight_output_acc: 0.6404 - val_bag_output_acc: 0.6409 - val_footwear_output_acc: 0.6835 - val_pose_output_acc: 0.8358 - val_emotion_output_acc: 0.7103\n",
      "\n",
      "Epoch 00028: loss did not improve from 6.14681\n",
      "Epoch 29/200\n",
      " - 84s - loss: 6.1116 - gender_output_loss: 0.2549 - image_quality_output_loss: 0.8748 - age_output_loss: 1.3517 - weight_output_loss: 0.9394 - bag_output_loss: 0.7628 - footwear_output_loss: 0.6992 - pose_output_loss: 0.3790 - emotion_output_loss: 0.8499 - gender_output_acc: 0.8958 - image_quality_output_acc: 0.5826 - age_output_acc: 0.4097 - weight_output_acc: 0.6385 - bag_output_acc: 0.6830 - footwear_output_acc: 0.6951 - pose_output_acc: 0.8601 - emotion_output_acc: 0.7116 - val_loss: 7.2384 - val_gender_output_loss: 0.6406 - val_image_quality_output_loss: 0.9856 - val_age_output_loss: 1.3852 - val_weight_output_loss: 0.9878 - val_bag_output_loss: 0.9472 - val_footwear_output_loss: 0.8260 - val_pose_output_loss: 0.5843 - val_emotion_output_loss: 0.8817 - val_gender_output_acc: 0.7406 - val_image_quality_output_acc: 0.4921 - val_age_output_acc: 0.4172 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5382 - val_footwear_output_acc: 0.6141 - val_pose_output_acc: 0.7991 - val_emotion_output_acc: 0.7103\n",
      "\n",
      "Epoch 00029: loss improved from 6.14681 to 6.11165, saving model to model.h5\n",
      "Epoch 30/200\n",
      " - 84s - loss: 5.9323 - gender_output_loss: 0.2030 - image_quality_output_loss: 0.8625 - age_output_loss: 1.3332 - weight_output_loss: 0.9285 - bag_output_loss: 0.7479 - footwear_output_loss: 0.6738 - pose_output_loss: 0.3422 - emotion_output_loss: 0.8411 - gender_output_acc: 0.9206 - image_quality_output_acc: 0.5942 - age_output_acc: 0.4154 - weight_output_acc: 0.6402 - bag_output_acc: 0.6899 - footwear_output_acc: 0.7053 - pose_output_acc: 0.8778 - emotion_output_acc: 0.7118 - val_loss: 6.6524 - val_gender_output_loss: 0.3985 - val_image_quality_output_loss: 0.9475 - val_age_output_loss: 1.3692 - val_weight_output_loss: 0.9760 - val_bag_output_loss: 0.8345 - val_footwear_output_loss: 0.7839 - val_pose_output_loss: 0.4665 - val_emotion_output_loss: 0.8764 - val_gender_output_acc: 0.8398 - val_image_quality_output_acc: 0.5451 - val_age_output_acc: 0.4067 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.6270 - val_footwear_output_acc: 0.6766 - val_pose_output_acc: 0.8294 - val_emotion_output_acc: 0.7103\n",
      "\n",
      "Epoch 00030: loss improved from 6.11165 to 5.93228, saving model to model.h5\n",
      "Epoch 31/200\n",
      " - 84s - loss: 6.0496 - gender_output_loss: 0.2275 - image_quality_output_loss: 0.8739 - age_output_loss: 1.3445 - weight_output_loss: 0.9398 - bag_output_loss: 0.7599 - footwear_output_loss: 0.6942 - pose_output_loss: 0.3618 - emotion_output_loss: 0.8480 - gender_output_acc: 0.9079 - image_quality_output_acc: 0.5826 - age_output_acc: 0.4124 - weight_output_acc: 0.6393 - bag_output_acc: 0.6825 - footwear_output_acc: 0.6996 - pose_output_acc: 0.8678 - emotion_output_acc: 0.7112 - val_loss: 6.5275 - val_gender_output_loss: 0.3421 - val_image_quality_output_loss: 0.8929 - val_age_output_loss: 1.3684 - val_weight_output_loss: 0.9768 - val_bag_output_loss: 0.8328 - val_footwear_output_loss: 0.7650 - val_pose_output_loss: 0.4757 - val_emotion_output_loss: 0.8738 - val_gender_output_acc: 0.8527 - val_image_quality_output_acc: 0.5739 - val_age_output_acc: 0.3934 - val_weight_output_acc: 0.6394 - val_bag_output_acc: 0.6349 - val_footwear_output_acc: 0.6786 - val_pose_output_acc: 0.8209 - val_emotion_output_acc: 0.7103\n",
      "\n",
      "Epoch 00031: loss did not improve from 5.93228\n",
      "Epoch 32/200\n",
      " - 84s - loss: 5.7368 - gender_output_loss: 0.1628 - image_quality_output_loss: 0.8515 - age_output_loss: 1.3155 - weight_output_loss: 0.9146 - bag_output_loss: 0.7259 - footwear_output_loss: 0.6494 - pose_output_loss: 0.2805 - emotion_output_loss: 0.8368 - gender_output_acc: 0.9404 - image_quality_output_acc: 0.5966 - age_output_acc: 0.4256 - weight_output_acc: 0.6435 - bag_output_acc: 0.7023 - footwear_output_acc: 0.7217 - pose_output_acc: 0.9022 - emotion_output_acc: 0.7114 - val_loss: 7.2331 - val_gender_output_loss: 0.4598 - val_image_quality_output_loss: 0.9397 - val_age_output_loss: 1.4565 - val_weight_output_loss: 1.0512 - val_bag_output_loss: 0.9391 - val_footwear_output_loss: 0.8690 - val_pose_output_loss: 0.6276 - val_emotion_output_loss: 0.8901 - val_gender_output_acc: 0.8070 - val_image_quality_output_acc: 0.5575 - val_age_output_acc: 0.3323 - val_weight_output_acc: 0.5556 - val_bag_output_acc: 0.5853 - val_footwear_output_acc: 0.6657 - val_pose_output_acc: 0.7505 - val_emotion_output_acc: 0.7103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00032: loss improved from 5.93228 to 5.73676, saving model to model.h5\n",
      "Epoch 33/200\n",
      " - 84s - loss: 5.8426 - gender_output_loss: 0.1931 - image_quality_output_loss: 0.8633 - age_output_loss: 1.3215 - weight_output_loss: 0.9152 - bag_output_loss: 0.7328 - footwear_output_loss: 0.6651 - pose_output_loss: 0.3116 - emotion_output_loss: 0.8400 - gender_output_acc: 0.9259 - image_quality_output_acc: 0.5905 - age_output_acc: 0.4205 - weight_output_acc: 0.6450 - bag_output_acc: 0.6979 - footwear_output_acc: 0.7147 - pose_output_acc: 0.8888 - emotion_output_acc: 0.7116 - val_loss: 6.5212 - val_gender_output_loss: 0.3607 - val_image_quality_output_loss: 0.8960 - val_age_output_loss: 1.3615 - val_weight_output_loss: 0.9736 - val_bag_output_loss: 0.8362 - val_footwear_output_loss: 0.7737 - val_pose_output_loss: 0.4455 - val_emotion_output_loss: 0.8741 - val_gender_output_acc: 0.8636 - val_image_quality_output_acc: 0.5501 - val_age_output_acc: 0.4053 - val_weight_output_acc: 0.6443 - val_bag_output_acc: 0.6438 - val_footwear_output_acc: 0.6811 - val_pose_output_acc: 0.8428 - val_emotion_output_acc: 0.7103\n",
      "\n",
      "Epoch 00033: loss did not improve from 5.73676\n",
      "Epoch 34/200\n",
      " - 84s - loss: 5.7355 - gender_output_loss: 0.1751 - image_quality_output_loss: 0.8532 - age_output_loss: 1.3114 - weight_output_loss: 0.9081 - bag_output_loss: 0.7236 - footwear_output_loss: 0.6478 - pose_output_loss: 0.2791 - emotion_output_loss: 0.8372 - gender_output_acc: 0.9322 - image_quality_output_acc: 0.5914 - age_output_acc: 0.4290 - weight_output_acc: 0.6461 - bag_output_acc: 0.7042 - footwear_output_acc: 0.7210 - pose_output_acc: 0.9036 - emotion_output_acc: 0.7115 - val_loss: 7.0058 - val_gender_output_loss: 0.4395 - val_image_quality_output_loss: 0.9242 - val_age_output_loss: 1.4236 - val_weight_output_loss: 1.0610 - val_bag_output_loss: 0.8606 - val_footwear_output_loss: 0.8789 - val_pose_output_loss: 0.5224 - val_emotion_output_loss: 0.8956 - val_gender_output_acc: 0.8512 - val_image_quality_output_acc: 0.5714 - val_age_output_acc: 0.3854 - val_weight_output_acc: 0.5352 - val_bag_output_acc: 0.6295 - val_footwear_output_acc: 0.6518 - val_pose_output_acc: 0.8180 - val_emotion_output_acc: 0.7103\n",
      "\n",
      "Epoch 00034: loss improved from 5.73676 to 5.73546, saving model to model.h5\n",
      "Epoch 35/200\n",
      " - 84s - loss: 5.4512 - gender_output_loss: 0.1221 - image_quality_output_loss: 0.8308 - age_output_loss: 1.2826 - weight_output_loss: 0.8838 - bag_output_loss: 0.6784 - footwear_output_loss: 0.6027 - pose_output_loss: 0.2256 - emotion_output_loss: 0.8252 - gender_output_acc: 0.9561 - image_quality_output_acc: 0.6069 - age_output_acc: 0.4405 - weight_output_acc: 0.6582 - bag_output_acc: 0.7281 - footwear_output_acc: 0.7405 - pose_output_acc: 0.9258 - emotion_output_acc: 0.7122 - val_loss: 7.0547 - val_gender_output_loss: 0.6019 - val_image_quality_output_loss: 0.9137 - val_age_output_loss: 1.4419 - val_weight_output_loss: 1.0257 - val_bag_output_loss: 0.8528 - val_footwear_output_loss: 0.8114 - val_pose_output_loss: 0.5141 - val_emotion_output_loss: 0.8931 - val_gender_output_acc: 0.8080 - val_image_quality_output_acc: 0.5536 - val_age_output_acc: 0.4048 - val_weight_output_acc: 0.6419 - val_bag_output_acc: 0.6334 - val_footwear_output_acc: 0.6746 - val_pose_output_acc: 0.8388 - val_emotion_output_acc: 0.7044\n",
      "\n",
      "Epoch 00035: loss improved from 5.73546 to 5.45119, saving model to model.h5\n",
      "Epoch 36/200\n",
      " - 84s - loss: 5.7414 - gender_output_loss: 0.1787 - image_quality_output_loss: 0.8632 - age_output_loss: 1.3052 - weight_output_loss: 0.9070 - bag_output_loss: 0.7127 - footwear_output_loss: 0.6457 - pose_output_loss: 0.2922 - emotion_output_loss: 0.8366 - gender_output_acc: 0.9287 - image_quality_output_acc: 0.5926 - age_output_acc: 0.4276 - weight_output_acc: 0.6498 - bag_output_acc: 0.7117 - footwear_output_acc: 0.7248 - pose_output_acc: 0.8942 - emotion_output_acc: 0.7111 - val_loss: 6.6615 - val_gender_output_loss: 0.3759 - val_image_quality_output_loss: 0.9103 - val_age_output_loss: 1.3917 - val_weight_output_loss: 0.9878 - val_bag_output_loss: 0.8281 - val_footwear_output_loss: 0.7877 - val_pose_output_loss: 0.4936 - val_emotion_output_loss: 0.8864 - val_gender_output_acc: 0.8552 - val_image_quality_output_acc: 0.5362 - val_age_output_acc: 0.3943 - val_weight_output_acc: 0.6404 - val_bag_output_acc: 0.6434 - val_footwear_output_acc: 0.6756 - val_pose_output_acc: 0.8279 - val_emotion_output_acc: 0.7103\n",
      "\n",
      "Epoch 00036: loss did not improve from 5.45119\n",
      "Epoch 37/200\n",
      " - 84s - loss: 5.3329 - gender_output_loss: 0.1117 - image_quality_output_loss: 0.8277 - age_output_loss: 1.2611 - weight_output_loss: 0.8645 - bag_output_loss: 0.6590 - footwear_output_loss: 0.5805 - pose_output_loss: 0.2053 - emotion_output_loss: 0.8229 - gender_output_acc: 0.9602 - image_quality_output_acc: 0.6098 - age_output_acc: 0.4477 - weight_output_acc: 0.6645 - bag_output_acc: 0.7387 - footwear_output_acc: 0.7550 - pose_output_acc: 0.9318 - emotion_output_acc: 0.7130 - val_loss: 7.6137 - val_gender_output_loss: 0.6653 - val_image_quality_output_loss: 1.0299 - val_age_output_loss: 1.4883 - val_weight_output_loss: 1.0974 - val_bag_output_loss: 0.8893 - val_footwear_output_loss: 0.8662 - val_pose_output_loss: 0.6879 - val_emotion_output_loss: 0.8895 - val_gender_output_acc: 0.8021 - val_image_quality_output_acc: 0.5293 - val_age_output_acc: 0.4077 - val_weight_output_acc: 0.6394 - val_bag_output_acc: 0.6101 - val_footwear_output_acc: 0.6404 - val_pose_output_acc: 0.7946 - val_emotion_output_acc: 0.7078\n",
      "\n",
      "Epoch 00037: loss improved from 5.45119 to 5.33291, saving model to model.h5\n",
      "Epoch 38/200\n",
      " - 84s - loss: 5.4285 - gender_output_loss: 0.1357 - image_quality_output_loss: 0.8343 - age_output_loss: 1.2646 - weight_output_loss: 0.8734 - bag_output_loss: 0.6679 - footwear_output_loss: 0.6046 - pose_output_loss: 0.2237 - emotion_output_loss: 0.8242 - gender_output_acc: 0.9506 - image_quality_output_acc: 0.6094 - age_output_acc: 0.4479 - weight_output_acc: 0.6609 - bag_output_acc: 0.7345 - footwear_output_acc: 0.7440 - pose_output_acc: 0.9234 - emotion_output_acc: 0.7115 - val_loss: 6.9037 - val_gender_output_loss: 0.4769 - val_image_quality_output_loss: 0.8976 - val_age_output_loss: 1.3857 - val_weight_output_loss: 0.9924 - val_bag_output_loss: 0.8825 - val_footwear_output_loss: 0.8398 - val_pose_output_loss: 0.5427 - val_emotion_output_loss: 0.8861 - val_gender_output_acc: 0.8457 - val_image_quality_output_acc: 0.5580 - val_age_output_acc: 0.3963 - val_weight_output_acc: 0.6161 - val_bag_output_acc: 0.6364 - val_footwear_output_acc: 0.6642 - val_pose_output_acc: 0.8165 - val_emotion_output_acc: 0.7103\n",
      "\n",
      "Epoch 00038: loss did not improve from 5.33291\n",
      "Epoch 39/200\n",
      " - 84s - loss: 5.2636 - gender_output_loss: 0.1220 - image_quality_output_loss: 0.8172 - age_output_loss: 1.2367 - weight_output_loss: 0.8520 - bag_output_loss: 0.6405 - footwear_output_loss: 0.5858 - pose_output_loss: 0.1911 - emotion_output_loss: 0.8185 - gender_output_acc: 0.9549 - image_quality_output_acc: 0.6191 - age_output_acc: 0.4630 - weight_output_acc: 0.6694 - bag_output_acc: 0.7470 - footwear_output_acc: 0.7457 - pose_output_acc: 0.9367 - emotion_output_acc: 0.7110 - val_loss: 7.1095 - val_gender_output_loss: 0.4567 - val_image_quality_output_loss: 0.9208 - val_age_output_loss: 1.4092 - val_weight_output_loss: 1.0189 - val_bag_output_loss: 1.0098 - val_footwear_output_loss: 0.8299 - val_pose_output_loss: 0.5695 - val_emotion_output_loss: 0.8947 - val_gender_output_acc: 0.8452 - val_image_quality_output_acc: 0.5407 - val_age_output_acc: 0.4087 - val_weight_output_acc: 0.6409 - val_bag_output_acc: 0.5878 - val_footwear_output_acc: 0.6572 - val_pose_output_acc: 0.8209 - val_emotion_output_acc: 0.7078\n",
      "\n",
      "Epoch 00039: loss improved from 5.33291 to 5.26364, saving model to model.h5\n",
      "Epoch 40/200\n",
      " - 84s - loss: 4.9311 - gender_output_loss: 0.0744 - image_quality_output_loss: 0.7892 - age_output_loss: 1.1928 - weight_output_loss: 0.8128 - bag_output_loss: 0.5848 - footwear_output_loss: 0.5312 - pose_output_loss: 0.1386 - emotion_output_loss: 0.8072 - gender_output_acc: 0.9738 - image_quality_output_acc: 0.6311 - age_output_acc: 0.4805 - weight_output_acc: 0.6854 - bag_output_acc: 0.7755 - footwear_output_acc: 0.7757 - pose_output_acc: 0.9561 - emotion_output_acc: 0.7148 - val_loss: 7.2722 - val_gender_output_loss: 0.5140 - val_image_quality_output_loss: 1.0101 - val_age_output_loss: 1.4118 - val_weight_output_loss: 1.0165 - val_bag_output_loss: 0.9333 - val_footwear_output_loss: 0.9049 - val_pose_output_loss: 0.5941 - val_emotion_output_loss: 0.8875 - val_gender_output_acc: 0.8358 - val_image_quality_output_acc: 0.5392 - val_age_output_acc: 0.3819 - val_weight_output_acc: 0.6076 - val_bag_output_acc: 0.6344 - val_footwear_output_acc: 0.6567 - val_pose_output_acc: 0.8209 - val_emotion_output_acc: 0.7059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00040: loss improved from 5.26364 to 4.93114, saving model to model.h5\n",
      "Epoch 41/200\n",
      " - 84s - loss: 5.1482 - gender_output_loss: 0.1110 - image_quality_output_loss: 0.8078 - age_output_loss: 1.2185 - weight_output_loss: 0.8344 - bag_output_loss: 0.6130 - footwear_output_loss: 0.5667 - pose_output_loss: 0.1801 - emotion_output_loss: 0.8167 - gender_output_acc: 0.9596 - image_quality_output_acc: 0.6208 - age_output_acc: 0.4738 - weight_output_acc: 0.6760 - bag_output_acc: 0.7636 - footwear_output_acc: 0.7535 - pose_output_acc: 0.9379 - emotion_output_acc: 0.7129 - val_loss: 7.1731 - val_gender_output_loss: 0.5634 - val_image_quality_output_loss: 0.9256 - val_age_output_loss: 1.4011 - val_weight_output_loss: 1.0072 - val_bag_output_loss: 0.9211 - val_footwear_output_loss: 0.8653 - val_pose_output_loss: 0.5837 - val_emotion_output_loss: 0.9058 - val_gender_output_acc: 0.8338 - val_image_quality_output_acc: 0.5397 - val_age_output_acc: 0.3963 - val_weight_output_acc: 0.6141 - val_bag_output_acc: 0.6379 - val_footwear_output_acc: 0.6597 - val_pose_output_acc: 0.8140 - val_emotion_output_acc: 0.7093\n",
      "\n",
      "Epoch 00041: loss did not improve from 4.93114\n",
      "Epoch 42/200\n",
      " - 84s - loss: 4.6469 - gender_output_loss: 0.0632 - image_quality_output_loss: 0.7465 - age_output_loss: 1.1388 - weight_output_loss: 0.7766 - bag_output_loss: 0.5258 - footwear_output_loss: 0.4959 - pose_output_loss: 0.1060 - emotion_output_loss: 0.7939 - gender_output_acc: 0.9773 - image_quality_output_acc: 0.6582 - age_output_acc: 0.5123 - weight_output_acc: 0.7035 - bag_output_acc: 0.8027 - footwear_output_acc: 0.7856 - pose_output_acc: 0.9661 - emotion_output_acc: 0.7147 - val_loss: 7.8570 - val_gender_output_loss: 0.5729 - val_image_quality_output_loss: 1.1513 - val_age_output_loss: 1.4968 - val_weight_output_loss: 1.0721 - val_bag_output_loss: 0.9983 - val_footwear_output_loss: 0.9687 - val_pose_output_loss: 0.6941 - val_emotion_output_loss: 0.9027 - val_gender_output_acc: 0.8338 - val_image_quality_output_acc: 0.4405 - val_age_output_acc: 0.3829 - val_weight_output_acc: 0.5923 - val_bag_output_acc: 0.6245 - val_footwear_output_acc: 0.6399 - val_pose_output_acc: 0.8249 - val_emotion_output_acc: 0.6999\n",
      "\n",
      "Epoch 00042: loss improved from 4.93114 to 4.64686, saving model to model.h5\n",
      "Epoch 43/200\n",
      " - 84s - loss: 4.7919 - gender_output_loss: 0.0822 - image_quality_output_loss: 0.7618 - age_output_loss: 1.1537 - weight_output_loss: 0.7923 - bag_output_loss: 0.5495 - footwear_output_loss: 0.5145 - pose_output_loss: 0.1362 - emotion_output_loss: 0.8017 - gender_output_acc: 0.9691 - image_quality_output_acc: 0.6493 - age_output_acc: 0.5051 - weight_output_acc: 0.6977 - bag_output_acc: 0.7970 - footwear_output_acc: 0.7755 - pose_output_acc: 0.9547 - emotion_output_acc: 0.7118 - val_loss: 7.3760 - val_gender_output_loss: 0.5107 - val_image_quality_output_loss: 0.9612 - val_age_output_loss: 1.4474 - val_weight_output_loss: 1.0468 - val_bag_output_loss: 0.9424 - val_footwear_output_loss: 0.9153 - val_pose_output_loss: 0.6410 - val_emotion_output_loss: 0.9112 - val_gender_output_acc: 0.8492 - val_image_quality_output_acc: 0.5377 - val_age_output_acc: 0.3924 - val_weight_output_acc: 0.6007 - val_bag_output_acc: 0.6250 - val_footwear_output_acc: 0.6533 - val_pose_output_acc: 0.8249 - val_emotion_output_acc: 0.6979\n",
      "\n",
      "Epoch 00043: loss did not improve from 4.64686\n",
      "Epoch 44/200\n",
      " - 84s - loss: 4.6377 - gender_output_loss: 0.0778 - image_quality_output_loss: 0.7312 - age_output_loss: 1.1328 - weight_output_loss: 0.7748 - bag_output_loss: 0.5135 - footwear_output_loss: 0.4938 - pose_output_loss: 0.1246 - emotion_output_loss: 0.7892 - gender_output_acc: 0.9718 - image_quality_output_acc: 0.6673 - age_output_acc: 0.5160 - weight_output_acc: 0.7015 - bag_output_acc: 0.8084 - footwear_output_acc: 0.7861 - pose_output_acc: 0.9580 - emotion_output_acc: 0.7174 - val_loss: 8.0273 - val_gender_output_loss: 0.6164 - val_image_quality_output_loss: 1.1387 - val_age_output_loss: 1.4352 - val_weight_output_loss: 1.0222 - val_bag_output_loss: 1.0897 - val_footwear_output_loss: 1.0724 - val_pose_output_loss: 0.7547 - val_emotion_output_loss: 0.8980 - val_gender_output_acc: 0.8229 - val_image_quality_output_acc: 0.4960 - val_age_output_acc: 0.3889 - val_weight_output_acc: 0.6096 - val_bag_output_acc: 0.6111 - val_footwear_output_acc: 0.6255 - val_pose_output_acc: 0.7852 - val_emotion_output_acc: 0.7059\n",
      "\n",
      "Epoch 00044: loss improved from 4.64686 to 4.63771, saving model to model.h5\n",
      "Epoch 45/200\n",
      " - 84s - loss: 4.2694 - gender_output_loss: 0.0504 - image_quality_output_loss: 0.6636 - age_output_loss: 1.0633 - weight_output_loss: 0.7353 - bag_output_loss: 0.4467 - footwear_output_loss: 0.4450 - pose_output_loss: 0.0887 - emotion_output_loss: 0.7764 - gender_output_acc: 0.9839 - image_quality_output_acc: 0.7038 - age_output_acc: 0.5514 - weight_output_acc: 0.7133 - bag_output_acc: 0.8362 - footwear_output_acc: 0.8067 - pose_output_acc: 0.9716 - emotion_output_acc: 0.7196 - val_loss: 8.3051 - val_gender_output_loss: 0.6152 - val_image_quality_output_loss: 1.3187 - val_age_output_loss: 1.5349 - val_weight_output_loss: 1.0813 - val_bag_output_loss: 1.0456 - val_footwear_output_loss: 1.0299 - val_pose_output_loss: 0.7697 - val_emotion_output_loss: 0.9098 - val_gender_output_acc: 0.8442 - val_image_quality_output_acc: 0.4856 - val_age_output_acc: 0.3780 - val_weight_output_acc: 0.5357 - val_bag_output_acc: 0.6329 - val_footwear_output_acc: 0.6473 - val_pose_output_acc: 0.8065 - val_emotion_output_acc: 0.6835\n",
      "\n",
      "Epoch 00045: loss improved from 4.63771 to 4.26937, saving model to model.h5\n",
      "Epoch 46/200\n",
      " - 84s - loss: 4.5800 - gender_output_loss: 0.0844 - image_quality_output_loss: 0.7144 - age_output_loss: 1.1148 - weight_output_loss: 0.7725 - bag_output_loss: 0.4938 - footwear_output_loss: 0.4867 - pose_output_loss: 0.1286 - emotion_output_loss: 0.7847 - gender_output_acc: 0.9692 - image_quality_output_acc: 0.6724 - age_output_acc: 0.5210 - weight_output_acc: 0.7058 - bag_output_acc: 0.8176 - footwear_output_acc: 0.7890 - pose_output_acc: 0.9559 - emotion_output_acc: 0.7162 - val_loss: 7.7600 - val_gender_output_loss: 0.5260 - val_image_quality_output_loss: 1.0231 - val_age_output_loss: 1.4966 - val_weight_output_loss: 1.0751 - val_bag_output_loss: 1.0266 - val_footwear_output_loss: 0.9799 - val_pose_output_loss: 0.7124 - val_emotion_output_loss: 0.9203 - val_gender_output_acc: 0.8438 - val_image_quality_output_acc: 0.5268 - val_age_output_acc: 0.3934 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.6146 - val_footwear_output_acc: 0.6493 - val_pose_output_acc: 0.8254 - val_emotion_output_acc: 0.6786\n",
      "\n",
      "Epoch 00046: loss did not improve from 4.26937\n",
      "Epoch 47/200\n",
      " - 84s - loss: 4.0026 - gender_output_loss: 0.0468 - image_quality_output_loss: 0.5953 - age_output_loss: 1.0140 - weight_output_loss: 0.7093 - bag_output_loss: 0.3992 - footwear_output_loss: 0.4135 - pose_output_loss: 0.0689 - emotion_output_loss: 0.7556 - gender_output_acc: 0.9844 - image_quality_output_acc: 0.7390 - age_output_acc: 0.5776 - weight_output_acc: 0.7334 - bag_output_acc: 0.8514 - footwear_output_acc: 0.8178 - pose_output_acc: 0.9791 - emotion_output_acc: 0.7268 - val_loss: 9.0342 - val_gender_output_loss: 0.6780 - val_image_quality_output_loss: 1.3097 - val_age_output_loss: 1.6729 - val_weight_output_loss: 1.2474 - val_bag_output_loss: 1.2583 - val_footwear_output_loss: 1.1016 - val_pose_output_loss: 0.8153 - val_emotion_output_loss: 0.9510 - val_gender_output_acc: 0.8204 - val_image_quality_output_acc: 0.4668 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6181 - val_bag_output_acc: 0.5784 - val_footwear_output_acc: 0.6032 - val_pose_output_acc: 0.8026 - val_emotion_output_acc: 0.6964\n",
      "\n",
      "Epoch 00047: loss improved from 4.26937 to 4.00264, saving model to model.h5\n",
      "Epoch 48/200\n",
      " - 84s - loss: 4.2071 - gender_output_loss: 0.0640 - image_quality_output_loss: 0.6184 - age_output_loss: 1.0575 - weight_output_loss: 0.7326 - bag_output_loss: 0.4352 - footwear_output_loss: 0.4394 - pose_output_loss: 0.0937 - emotion_output_loss: 0.7664 - gender_output_acc: 0.9756 - image_quality_output_acc: 0.7290 - age_output_acc: 0.5490 - weight_output_acc: 0.7211 - bag_output_acc: 0.8363 - footwear_output_acc: 0.8043 - pose_output_acc: 0.9683 - emotion_output_acc: 0.7243 - val_loss: 8.1570 - val_gender_output_loss: 0.5640 - val_image_quality_output_loss: 1.1724 - val_age_output_loss: 1.5224 - val_weight_output_loss: 1.0879 - val_bag_output_loss: 1.0780 - val_footwear_output_loss: 1.0315 - val_pose_output_loss: 0.7626 - val_emotion_output_loss: 0.9382 - val_gender_output_acc: 0.8413 - val_image_quality_output_acc: 0.5169 - val_age_output_acc: 0.3805 - val_weight_output_acc: 0.5843 - val_bag_output_acc: 0.6270 - val_footwear_output_acc: 0.6399 - val_pose_output_acc: 0.8279 - val_emotion_output_acc: 0.6825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00048: loss did not improve from 4.00264\n",
      "Epoch 49/200\n",
      " - 84s - loss: 4.0043 - gender_output_loss: 0.0568 - image_quality_output_loss: 0.5604 - age_output_loss: 1.0189 - weight_output_loss: 0.7158 - bag_output_loss: 0.3988 - footwear_output_loss: 0.4103 - pose_output_loss: 0.0896 - emotion_output_loss: 0.7539 - gender_output_acc: 0.9797 - image_quality_output_acc: 0.7569 - age_output_acc: 0.5793 - weight_output_acc: 0.7249 - bag_output_acc: 0.8503 - footwear_output_acc: 0.8201 - pose_output_acc: 0.9709 - emotion_output_acc: 0.7258 - val_loss: 8.4063 - val_gender_output_loss: 0.6131 - val_image_quality_output_loss: 1.2329 - val_age_output_loss: 1.5002 - val_weight_output_loss: 1.1010 - val_bag_output_loss: 1.1253 - val_footwear_output_loss: 1.0510 - val_pose_output_loss: 0.8271 - val_emotion_output_loss: 0.9556 - val_gender_output_acc: 0.8333 - val_image_quality_output_acc: 0.4638 - val_age_output_acc: 0.3700 - val_weight_output_acc: 0.5689 - val_bag_output_acc: 0.6022 - val_footwear_output_acc: 0.6250 - val_pose_output_acc: 0.8006 - val_emotion_output_acc: 0.6716\n",
      "\n",
      "Epoch 00049: loss did not improve from 4.00264\n",
      "Epoch 50/200\n",
      " - 84s - loss: 3.6261 - gender_output_loss: 0.0313 - image_quality_output_loss: 0.4572 - age_output_loss: 0.9544 - weight_output_loss: 0.6791 - bag_output_loss: 0.3489 - footwear_output_loss: 0.3727 - pose_output_loss: 0.0572 - emotion_output_loss: 0.7252 - gender_output_acc: 0.9889 - image_quality_output_acc: 0.8135 - age_output_acc: 0.6109 - weight_output_acc: 0.7396 - bag_output_acc: 0.8686 - footwear_output_acc: 0.8362 - pose_output_acc: 0.9825 - emotion_output_acc: 0.7372 - val_loss: 9.5572 - val_gender_output_loss: 0.7536 - val_image_quality_output_loss: 1.6167 - val_age_output_loss: 1.7177 - val_weight_output_loss: 1.1959 - val_bag_output_loss: 1.2219 - val_footwear_output_loss: 1.2018 - val_pose_output_loss: 0.8666 - val_emotion_output_loss: 0.9830 - val_gender_output_acc: 0.8338 - val_image_quality_output_acc: 0.4782 - val_age_output_acc: 0.4087 - val_weight_output_acc: 0.6111 - val_bag_output_acc: 0.6329 - val_footwear_output_acc: 0.6265 - val_pose_output_acc: 0.8259 - val_emotion_output_acc: 0.6612\n",
      "\n",
      "Epoch 00050: loss improved from 4.00264 to 3.62613, saving model to model.h5\n",
      "Epoch 51/200\n",
      " - 84s - loss: 3.9400 - gender_output_loss: 0.0598 - image_quality_output_loss: 0.5261 - age_output_loss: 1.0041 - weight_output_loss: 0.7078 - bag_output_loss: 0.3953 - footwear_output_loss: 0.4122 - pose_output_loss: 0.0901 - emotion_output_loss: 0.7446 - gender_output_acc: 0.9772 - image_quality_output_acc: 0.7734 - age_output_acc: 0.5892 - weight_output_acc: 0.7288 - bag_output_acc: 0.8509 - footwear_output_acc: 0.8168 - pose_output_acc: 0.9684 - emotion_output_acc: 0.7293 - val_loss: 8.6648 - val_gender_output_loss: 0.6350 - val_image_quality_output_loss: 1.2742 - val_age_output_loss: 1.5640 - val_weight_output_loss: 1.1231 - val_bag_output_loss: 1.1846 - val_footwear_output_loss: 1.1155 - val_pose_output_loss: 0.8186 - val_emotion_output_loss: 0.9497 - val_gender_output_acc: 0.8403 - val_image_quality_output_acc: 0.5055 - val_age_output_acc: 0.3805 - val_weight_output_acc: 0.5888 - val_bag_output_acc: 0.6250 - val_footwear_output_acc: 0.6374 - val_pose_output_acc: 0.8160 - val_emotion_output_acc: 0.6582\n",
      "\n",
      "Epoch 00051: loss did not improve from 3.62613\n",
      "Epoch 52/200\n",
      " - 84s - loss: 3.3544 - gender_output_loss: 0.0370 - image_quality_output_loss: 0.3533 - age_output_loss: 0.9075 - weight_output_loss: 0.6504 - bag_output_loss: 0.3249 - footwear_output_loss: 0.3326 - pose_output_loss: 0.0499 - emotion_output_loss: 0.6988 - gender_output_acc: 0.9866 - image_quality_output_acc: 0.8647 - age_output_acc: 0.6366 - weight_output_acc: 0.7536 - bag_output_acc: 0.8773 - footwear_output_acc: 0.8531 - pose_output_acc: 0.9837 - emotion_output_acc: 0.7439 - val_loss: 9.8190 - val_gender_output_loss: 0.6499 - val_image_quality_output_loss: 1.5299 - val_age_output_loss: 1.7768 - val_weight_output_loss: 1.2493 - val_bag_output_loss: 1.2625 - val_footwear_output_loss: 1.3214 - val_pose_output_loss: 0.9131 - val_emotion_output_loss: 1.1160 - val_gender_output_acc: 0.8294 - val_image_quality_output_acc: 0.5020 - val_age_output_acc: 0.3760 - val_weight_output_acc: 0.5823 - val_bag_output_acc: 0.5992 - val_footwear_output_acc: 0.6359 - val_pose_output_acc: 0.7981 - val_emotion_output_acc: 0.7059\n",
      "\n",
      "Epoch 00052: loss improved from 3.62613 to 3.35439, saving model to model.h5\n",
      "Epoch 53/200\n",
      " - 84s - loss: 3.5703 - gender_output_loss: 0.0511 - image_quality_output_loss: 0.4066 - age_output_loss: 0.9394 - weight_output_loss: 0.6720 - bag_output_loss: 0.3646 - footwear_output_loss: 0.3565 - pose_output_loss: 0.0742 - emotion_output_loss: 0.7059 - gender_output_acc: 0.9823 - image_quality_output_acc: 0.8347 - age_output_acc: 0.6215 - weight_output_acc: 0.7435 - bag_output_acc: 0.8622 - footwear_output_acc: 0.8498 - pose_output_acc: 0.9747 - emotion_output_acc: 0.7416 - val_loss: 9.1069 - val_gender_output_loss: 0.6534 - val_image_quality_output_loss: 1.4658 - val_age_output_loss: 1.6195 - val_weight_output_loss: 1.1367 - val_bag_output_loss: 1.1930 - val_footwear_output_loss: 1.1925 - val_pose_output_loss: 0.8494 - val_emotion_output_loss: 0.9965 - val_gender_output_acc: 0.8408 - val_image_quality_output_acc: 0.5069 - val_age_output_acc: 0.3810 - val_weight_output_acc: 0.6042 - val_bag_output_acc: 0.6319 - val_footwear_output_acc: 0.6250 - val_pose_output_acc: 0.8150 - val_emotion_output_acc: 0.6453\n",
      "\n",
      "Epoch 00053: loss did not improve from 3.35439\n",
      "Epoch 54/200\n",
      " - 84s - loss: 3.4933 - gender_output_loss: 0.0511 - image_quality_output_loss: 0.3853 - age_output_loss: 0.9196 - weight_output_loss: 0.6702 - bag_output_loss: 0.3454 - footwear_output_loss: 0.3414 - pose_output_loss: 0.0845 - emotion_output_loss: 0.6958 - gender_output_acc: 0.9819 - image_quality_output_acc: 0.8420 - age_output_acc: 0.6368 - weight_output_acc: 0.7437 - bag_output_acc: 0.8661 - footwear_output_acc: 0.8531 - pose_output_acc: 0.9734 - emotion_output_acc: 0.7437 - val_loss: 9.4909 - val_gender_output_loss: 0.6111 - val_image_quality_output_loss: 2.2026 - val_age_output_loss: 1.5462 - val_weight_output_loss: 1.1239 - val_bag_output_loss: 1.1503 - val_footwear_output_loss: 1.1919 - val_pose_output_loss: 0.7095 - val_emotion_output_loss: 0.9553 - val_gender_output_acc: 0.8135 - val_image_quality_output_acc: 0.4241 - val_age_output_acc: 0.3616 - val_weight_output_acc: 0.5516 - val_bag_output_acc: 0.6161 - val_footwear_output_acc: 0.6389 - val_pose_output_acc: 0.7941 - val_emotion_output_acc: 0.6905\n",
      "\n",
      "Epoch 00054: loss did not improve from 3.35439\n",
      "Epoch 55/200\n",
      " - 84s - loss: 3.2848 - gender_output_loss: 0.0360 - image_quality_output_loss: 0.3485 - age_output_loss: 0.8901 - weight_output_loss: 0.6443 - bag_output_loss: 0.3302 - footwear_output_loss: 0.3051 - pose_output_loss: 0.0595 - emotion_output_loss: 0.6711 - gender_output_acc: 0.9878 - image_quality_output_acc: 0.8602 - age_output_acc: 0.6435 - weight_output_acc: 0.7520 - bag_output_acc: 0.8754 - footwear_output_acc: 0.8711 - pose_output_acc: 0.9794 - emotion_output_acc: 0.7536 - val_loss: 9.9807 - val_gender_output_loss: 0.9373 - val_image_quality_output_loss: 1.7035 - val_age_output_loss: 1.6083 - val_weight_output_loss: 1.1873 - val_bag_output_loss: 1.3032 - val_footwear_output_loss: 1.2927 - val_pose_output_loss: 0.8834 - val_emotion_output_loss: 1.0651 - val_gender_output_acc: 0.8234 - val_image_quality_output_acc: 0.5154 - val_age_output_acc: 0.3770 - val_weight_output_acc: 0.6121 - val_bag_output_acc: 0.6255 - val_footwear_output_acc: 0.6096 - val_pose_output_acc: 0.8130 - val_emotion_output_acc: 0.6166\n",
      "\n",
      "Epoch 00055: loss improved from 3.35439 to 3.28479, saving model to model.h5\n",
      "Epoch 56/200\n",
      " - 84s - loss: 3.4322 - gender_output_loss: 0.0499 - image_quality_output_loss: 0.3760 - age_output_loss: 0.9126 - weight_output_loss: 0.6700 - bag_output_loss: 0.3510 - footwear_output_loss: 0.3207 - pose_output_loss: 0.0720 - emotion_output_loss: 0.6801 - gender_output_acc: 0.9812 - image_quality_output_acc: 0.8459 - age_output_acc: 0.6332 - weight_output_acc: 0.7420 - bag_output_acc: 0.8639 - footwear_output_acc: 0.8657 - pose_output_acc: 0.9745 - emotion_output_acc: 0.7511 - val_loss: 9.7354 - val_gender_output_loss: 0.7288 - val_image_quality_output_loss: 1.6044 - val_age_output_loss: 1.6998 - val_weight_output_loss: 1.1467 - val_bag_output_loss: 1.3370 - val_footwear_output_loss: 1.2571 - val_pose_output_loss: 0.9021 - val_emotion_output_loss: 1.0595 - val_gender_output_acc: 0.8284 - val_image_quality_output_acc: 0.5377 - val_age_output_acc: 0.3869 - val_weight_output_acc: 0.6057 - val_bag_output_acc: 0.6047 - val_footwear_output_acc: 0.6220 - val_pose_output_acc: 0.8080 - val_emotion_output_acc: 0.6349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00056: loss did not improve from 3.28479\n",
      "Epoch 57/200\n",
      " - 84s - loss: 2.8135 - gender_output_loss: 0.0285 - image_quality_output_loss: 0.2219 - age_output_loss: 0.7983 - weight_output_loss: 0.5924 - bag_output_loss: 0.2909 - footwear_output_loss: 0.2352 - pose_output_loss: 0.0473 - emotion_output_loss: 0.5990 - gender_output_acc: 0.9904 - image_quality_output_acc: 0.9155 - age_output_acc: 0.6885 - weight_output_acc: 0.7732 - bag_output_acc: 0.8856 - footwear_output_acc: 0.9054 - pose_output_acc: 0.9835 - emotion_output_acc: 0.7776 - val_loss: 12.4494 - val_gender_output_loss: 0.7733 - val_image_quality_output_loss: 2.2978 - val_age_output_loss: 2.4062 - val_weight_output_loss: 1.6164 - val_bag_output_loss: 1.5907 - val_footwear_output_loss: 1.5608 - val_pose_output_loss: 1.0083 - val_emotion_output_loss: 1.1959 - val_gender_output_acc: 0.8229 - val_image_quality_output_acc: 0.4782 - val_age_output_acc: 0.3824 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5179 - val_footwear_output_acc: 0.6166 - val_pose_output_acc: 0.7951 - val_emotion_output_acc: 0.6915\n",
      "\n",
      "Epoch 00057: loss improved from 3.28479 to 2.81354, saving model to model.h5\n",
      "Epoch 58/200\n",
      " - 84s - loss: 3.1215 - gender_output_loss: 0.0393 - image_quality_output_loss: 0.3013 - age_output_loss: 0.8492 - weight_output_loss: 0.6284 - bag_output_loss: 0.3337 - footwear_output_loss: 0.2804 - pose_output_loss: 0.0658 - emotion_output_loss: 0.6235 - gender_output_acc: 0.9842 - image_quality_output_acc: 0.8798 - age_output_acc: 0.6627 - weight_output_acc: 0.7580 - bag_output_acc: 0.8722 - footwear_output_acc: 0.8840 - pose_output_acc: 0.9783 - emotion_output_acc: 0.7697 - val_loss: 10.3069 - val_gender_output_loss: 0.7931 - val_image_quality_output_loss: 1.7597 - val_age_output_loss: 1.7728 - val_weight_output_loss: 1.2330 - val_bag_output_loss: 1.2470 - val_footwear_output_loss: 1.4101 - val_pose_output_loss: 0.9595 - val_emotion_output_loss: 1.1316 - val_gender_output_acc: 0.8348 - val_image_quality_output_acc: 0.5184 - val_age_output_acc: 0.3864 - val_weight_output_acc: 0.6151 - val_bag_output_acc: 0.6305 - val_footwear_output_acc: 0.6200 - val_pose_output_acc: 0.8249 - val_emotion_output_acc: 0.6369\n",
      "\n",
      "Epoch 00058: loss did not improve from 2.81354\n",
      "Epoch 59/200\n",
      " - 84s - loss: 2.9253 - gender_output_loss: 0.0387 - image_quality_output_loss: 0.2595 - age_output_loss: 0.8115 - weight_output_loss: 0.6052 - bag_output_loss: 0.3102 - footwear_output_loss: 0.2536 - pose_output_loss: 0.0558 - emotion_output_loss: 0.5908 - gender_output_acc: 0.9852 - image_quality_output_acc: 0.8973 - age_output_acc: 0.6873 - weight_output_acc: 0.7682 - bag_output_acc: 0.8768 - footwear_output_acc: 0.8971 - pose_output_acc: 0.9819 - emotion_output_acc: 0.7756 - val_loss: 11.2084 - val_gender_output_loss: 0.9195 - val_image_quality_output_loss: 2.1851 - val_age_output_loss: 1.8186 - val_weight_output_loss: 1.2736 - val_bag_output_loss: 1.3227 - val_footwear_output_loss: 1.4941 - val_pose_output_loss: 1.0052 - val_emotion_output_loss: 1.1897 - val_gender_output_acc: 0.8294 - val_image_quality_output_acc: 0.5000 - val_age_output_acc: 0.3814 - val_weight_output_acc: 0.6012 - val_bag_output_acc: 0.6245 - val_footwear_output_acc: 0.6314 - val_pose_output_acc: 0.7937 - val_emotion_output_acc: 0.6835\n",
      "\n",
      "Epoch 00059: loss did not improve from 2.81354\n",
      "Epoch 60/200\n",
      " - 84s - loss: 2.5792 - gender_output_loss: 0.0256 - image_quality_output_loss: 0.1942 - age_output_loss: 0.7418 - weight_output_loss: 0.5628 - bag_output_loss: 0.2781 - footwear_output_loss: 0.1968 - pose_output_loss: 0.0347 - emotion_output_loss: 0.5452 - gender_output_acc: 0.9917 - image_quality_output_acc: 0.9261 - age_output_acc: 0.7164 - weight_output_acc: 0.7870 - bag_output_acc: 0.8898 - footwear_output_acc: 0.9237 - pose_output_acc: 0.9891 - emotion_output_acc: 0.7947 - val_loss: 11.2560 - val_gender_output_loss: 0.8309 - val_image_quality_output_loss: 2.0526 - val_age_output_loss: 1.8093 - val_weight_output_loss: 1.4525 - val_bag_output_loss: 1.2618 - val_footwear_output_loss: 1.6089 - val_pose_output_loss: 0.9992 - val_emotion_output_loss: 1.2407 - val_gender_output_acc: 0.8393 - val_image_quality_output_acc: 0.4975 - val_age_output_acc: 0.3904 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.6136 - val_footwear_output_acc: 0.6066 - val_pose_output_acc: 0.8120 - val_emotion_output_acc: 0.6091\n",
      "\n",
      "Epoch 00060: loss improved from 2.81354 to 2.57918, saving model to model.h5\n",
      "Epoch 61/200\n",
      " - 84s - loss: 2.8607 - gender_output_loss: 0.0415 - image_quality_output_loss: 0.2569 - age_output_loss: 0.7930 - weight_output_loss: 0.6003 - bag_output_loss: 0.3092 - footwear_output_loss: 0.2259 - pose_output_loss: 0.0569 - emotion_output_loss: 0.5770 - gender_output_acc: 0.9856 - image_quality_output_acc: 0.9001 - age_output_acc: 0.6911 - weight_output_acc: 0.7703 - bag_output_acc: 0.8764 - footwear_output_acc: 0.9063 - pose_output_acc: 0.9804 - emotion_output_acc: 0.7846 - val_loss: 10.7537 - val_gender_output_loss: 0.8179 - val_image_quality_output_loss: 1.9367 - val_age_output_loss: 1.8109 - val_weight_output_loss: 1.2330 - val_bag_output_loss: 1.3632 - val_footwear_output_loss: 1.4034 - val_pose_output_loss: 0.9972 - val_emotion_output_loss: 1.1913 - val_gender_output_acc: 0.8413 - val_image_quality_output_acc: 0.5154 - val_age_output_acc: 0.3601 - val_weight_output_acc: 0.5947 - val_bag_output_acc: 0.6181 - val_footwear_output_acc: 0.6141 - val_pose_output_acc: 0.8110 - val_emotion_output_acc: 0.5655\n",
      "\n",
      "Epoch 00061: loss did not improve from 2.57918\n",
      "Epoch 62/200\n",
      " - 84s - loss: 2.4652 - gender_output_loss: 0.0285 - image_quality_output_loss: 0.1927 - age_output_loss: 0.7127 - weight_output_loss: 0.5323 - bag_output_loss: 0.2849 - footwear_output_loss: 0.1605 - pose_output_loss: 0.0502 - emotion_output_loss: 0.5034 - gender_output_acc: 0.9899 - image_quality_output_acc: 0.9219 - age_output_acc: 0.7304 - weight_output_acc: 0.7970 - bag_output_acc: 0.8883 - footwear_output_acc: 0.9398 - pose_output_acc: 0.9834 - emotion_output_acc: 0.8091 - val_loss: 10.9890 - val_gender_output_loss: 0.7812 - val_image_quality_output_loss: 2.0488 - val_age_output_loss: 1.7515 - val_weight_output_loss: 1.2726 - val_bag_output_loss: 1.4093 - val_footwear_output_loss: 1.6148 - val_pose_output_loss: 0.9204 - val_emotion_output_loss: 1.1904 - val_gender_output_acc: 0.8323 - val_image_quality_output_acc: 0.5347 - val_age_output_acc: 0.3760 - val_weight_output_acc: 0.5888 - val_bag_output_acc: 0.6310 - val_footwear_output_acc: 0.5938 - val_pose_output_acc: 0.8145 - val_emotion_output_acc: 0.5749\n",
      "\n",
      "Epoch 00062: loss improved from 2.57918 to 2.46522, saving model to model.h5\n",
      "Epoch 63/200\n",
      " - 84s - loss: 2.4436 - gender_output_loss: 0.0306 - image_quality_output_loss: 0.1921 - age_output_loss: 0.7064 - weight_output_loss: 0.5232 - bag_output_loss: 0.2766 - footwear_output_loss: 0.1688 - pose_output_loss: 0.0385 - emotion_output_loss: 0.5075 - gender_output_acc: 0.9890 - image_quality_output_acc: 0.9270 - age_output_acc: 0.7315 - weight_output_acc: 0.8003 - bag_output_acc: 0.8850 - footwear_output_acc: 0.9345 - pose_output_acc: 0.9872 - emotion_output_acc: 0.8098 - val_loss: 11.7711 - val_gender_output_loss: 0.8009 - val_image_quality_output_loss: 2.2205 - val_age_output_loss: 1.9057 - val_weight_output_loss: 1.4096 - val_bag_output_loss: 1.3601 - val_footwear_output_loss: 1.7422 - val_pose_output_loss: 1.0216 - val_emotion_output_loss: 1.3104 - val_gender_output_acc: 0.8338 - val_image_quality_output_acc: 0.5223 - val_age_output_acc: 0.3651 - val_weight_output_acc: 0.6062 - val_bag_output_acc: 0.6225 - val_footwear_output_acc: 0.6081 - val_pose_output_acc: 0.8269 - val_emotion_output_acc: 0.6176\n",
      "\n",
      "Epoch 00063: loss improved from 2.46522 to 2.44359, saving model to model.h5\n",
      "Epoch 64/200\n",
      " - 84s - loss: 2.3577 - gender_output_loss: 0.0362 - image_quality_output_loss: 0.1887 - age_output_loss: 0.6793 - weight_output_loss: 0.5004 - bag_output_loss: 0.2758 - footwear_output_loss: 0.1583 - pose_output_loss: 0.0404 - emotion_output_loss: 0.4785 - gender_output_acc: 0.9862 - image_quality_output_acc: 0.9292 - age_output_acc: 0.7443 - weight_output_acc: 0.8089 - bag_output_acc: 0.8848 - footwear_output_acc: 0.9391 - pose_output_acc: 0.9864 - emotion_output_acc: 0.8174 - val_loss: 12.1893 - val_gender_output_loss: 1.2908 - val_image_quality_output_loss: 2.1495 - val_age_output_loss: 1.8645 - val_weight_output_loss: 1.4710 - val_bag_output_loss: 1.5241 - val_footwear_output_loss: 1.5543 - val_pose_output_loss: 1.0166 - val_emotion_output_loss: 1.3183 - val_gender_output_acc: 0.7773 - val_image_quality_output_acc: 0.5119 - val_age_output_acc: 0.3671 - val_weight_output_acc: 0.5263 - val_bag_output_acc: 0.6225 - val_footwear_output_acc: 0.6255 - val_pose_output_acc: 0.8036 - val_emotion_output_acc: 0.6071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00064: loss improved from 2.44359 to 2.35770, saving model to model.h5\n",
      "Epoch 65/200\n",
      " - 84s - loss: 2.2565 - gender_output_loss: 0.0217 - image_quality_output_loss: 0.1600 - age_output_loss: 0.6571 - weight_output_loss: 0.4912 - bag_output_loss: 0.2743 - footwear_output_loss: 0.1512 - pose_output_loss: 0.0393 - emotion_output_loss: 0.4617 - gender_output_acc: 0.9924 - image_quality_output_acc: 0.9393 - age_output_acc: 0.7562 - weight_output_acc: 0.8153 - bag_output_acc: 0.8877 - footwear_output_acc: 0.9436 - pose_output_acc: 0.9872 - emotion_output_acc: 0.8283 - val_loss: 12.4944 - val_gender_output_loss: 1.0006 - val_image_quality_output_loss: 2.2515 - val_age_output_loss: 1.9437 - val_weight_output_loss: 1.4070 - val_bag_output_loss: 1.6050 - val_footwear_output_loss: 1.8554 - val_pose_output_loss: 1.0568 - val_emotion_output_loss: 1.3742 - val_gender_output_acc: 0.8269 - val_image_quality_output_acc: 0.5253 - val_age_output_acc: 0.3492 - val_weight_output_acc: 0.5967 - val_bag_output_acc: 0.6166 - val_footwear_output_acc: 0.6364 - val_pose_output_acc: 0.8155 - val_emotion_output_acc: 0.6240\n",
      "\n",
      "Epoch 00065: loss improved from 2.35770 to 2.25650, saving model to model.h5\n",
      "Epoch 66/200\n",
      " - 84s - loss: 2.4092 - gender_output_loss: 0.0361 - image_quality_output_loss: 0.1976 - age_output_loss: 0.6911 - weight_output_loss: 0.5007 - bag_output_loss: 0.2853 - footwear_output_loss: 0.1656 - pose_output_loss: 0.0467 - emotion_output_loss: 0.4859 - gender_output_acc: 0.9864 - image_quality_output_acc: 0.9216 - age_output_acc: 0.7378 - weight_output_acc: 0.8073 - bag_output_acc: 0.8819 - footwear_output_acc: 0.9359 - pose_output_acc: 0.9845 - emotion_output_acc: 0.8181 - val_loss: 12.4306 - val_gender_output_loss: 0.7727 - val_image_quality_output_loss: 2.4150 - val_age_output_loss: 1.9958 - val_weight_output_loss: 1.4988 - val_bag_output_loss: 1.5350 - val_footwear_output_loss: 1.8452 - val_pose_output_loss: 1.0207 - val_emotion_output_loss: 1.3474 - val_gender_output_acc: 0.8363 - val_image_quality_output_acc: 0.5159 - val_age_output_acc: 0.3606 - val_weight_output_acc: 0.6017 - val_bag_output_acc: 0.6250 - val_footwear_output_acc: 0.6215 - val_pose_output_acc: 0.8105 - val_emotion_output_acc: 0.5873\n",
      "\n",
      "Epoch 00066: loss did not improve from 2.25650\n",
      "Epoch 67/200\n",
      " - 84s - loss: 1.8655 - gender_output_loss: 0.0208 - image_quality_output_loss: 0.1109 - age_output_loss: 0.5468 - weight_output_loss: 0.4112 - bag_output_loss: 0.2563 - footwear_output_loss: 0.0976 - pose_output_loss: 0.0247 - emotion_output_loss: 0.3971 - gender_output_acc: 0.9936 - image_quality_output_acc: 0.9612 - age_output_acc: 0.8003 - weight_output_acc: 0.8438 - bag_output_acc: 0.8910 - footwear_output_acc: 0.9635 - pose_output_acc: 0.9919 - emotion_output_acc: 0.8476 - val_loss: 18.7332 - val_gender_output_loss: 1.0398 - val_image_quality_output_loss: 5.5128 - val_age_output_loss: 2.3400 - val_weight_output_loss: 1.8778 - val_bag_output_loss: 2.0189 - val_footwear_output_loss: 2.3914 - val_pose_output_loss: 1.0889 - val_emotion_output_loss: 2.4636 - val_gender_output_acc: 0.8120 - val_image_quality_output_acc: 0.3318 - val_age_output_acc: 0.3423 - val_weight_output_acc: 0.6116 - val_bag_output_acc: 0.5293 - val_footwear_output_acc: 0.6295 - val_pose_output_acc: 0.8095 - val_emotion_output_acc: 0.7019\n",
      "\n",
      "Epoch 00067: loss improved from 2.25650 to 1.86548, saving model to model.h5\n",
      "Epoch 68/200\n",
      " - 84s - loss: 2.2044 - gender_output_loss: 0.0304 - image_quality_output_loss: 0.1907 - age_output_loss: 0.6196 - weight_output_loss: 0.4622 - bag_output_loss: 0.2765 - footwear_output_loss: 0.1492 - pose_output_loss: 0.0443 - emotion_output_loss: 0.4315 - gender_output_acc: 0.9885 - image_quality_output_acc: 0.9295 - age_output_acc: 0.7680 - weight_output_acc: 0.8231 - bag_output_acc: 0.8849 - footwear_output_acc: 0.9448 - pose_output_acc: 0.9845 - emotion_output_acc: 0.8365 - val_loss: 12.5525 - val_gender_output_loss: 0.8355 - val_image_quality_output_loss: 2.2619 - val_age_output_loss: 2.0137 - val_weight_output_loss: 1.5353 - val_bag_output_loss: 1.4532 - val_footwear_output_loss: 1.9089 - val_pose_output_loss: 1.0908 - val_emotion_output_loss: 1.4532 - val_gender_output_acc: 0.8279 - val_image_quality_output_acc: 0.5203 - val_age_output_acc: 0.3770 - val_weight_output_acc: 0.6062 - val_bag_output_acc: 0.6225 - val_footwear_output_acc: 0.6151 - val_pose_output_acc: 0.8160 - val_emotion_output_acc: 0.6047\n",
      "\n",
      "Epoch 00068: loss did not improve from 1.86548\n",
      "Epoch 69/200\n",
      " - 84s - loss: 2.0139 - gender_output_loss: 0.0280 - image_quality_output_loss: 0.1502 - age_output_loss: 0.5680 - weight_output_loss: 0.4310 - bag_output_loss: 0.2630 - footwear_output_loss: 0.1215 - pose_output_loss: 0.0412 - emotion_output_loss: 0.4110 - gender_output_acc: 0.9899 - image_quality_output_acc: 0.9444 - age_output_acc: 0.7884 - weight_output_acc: 0.8342 - bag_output_acc: 0.8878 - footwear_output_acc: 0.9521 - pose_output_acc: 0.9859 - emotion_output_acc: 0.8442 - val_loss: 14.5947 - val_gender_output_loss: 1.0521 - val_image_quality_output_loss: 2.6237 - val_age_output_loss: 2.1130 - val_weight_output_loss: 1.6367 - val_bag_output_loss: 1.5972 - val_footwear_output_loss: 2.7043 - val_pose_output_loss: 1.1601 - val_emotion_output_loss: 1.7076 - val_gender_output_acc: 0.8061 - val_image_quality_output_acc: 0.5020 - val_age_output_acc: 0.3398 - val_weight_output_acc: 0.5263 - val_bag_output_acc: 0.6205 - val_footwear_output_acc: 0.6429 - val_pose_output_acc: 0.8031 - val_emotion_output_acc: 0.6830\n",
      "\n",
      "Epoch 00069: loss did not improve from 1.86548\n",
      "Epoch 70/200\n",
      " - 84s - loss: 1.7901 - gender_output_loss: 0.0193 - image_quality_output_loss: 0.1297 - age_output_loss: 0.5082 - weight_output_loss: 0.3897 - bag_output_loss: 0.2451 - footwear_output_loss: 0.0939 - pose_output_loss: 0.0286 - emotion_output_loss: 0.3756 - gender_output_acc: 0.9932 - image_quality_output_acc: 0.9523 - age_output_acc: 0.8123 - weight_output_acc: 0.8511 - bag_output_acc: 0.8938 - footwear_output_acc: 0.9657 - pose_output_acc: 0.9901 - emotion_output_acc: 0.8603 - val_loss: 13.0944 - val_gender_output_loss: 0.9015 - val_image_quality_output_loss: 2.3707 - val_age_output_loss: 2.2668 - val_weight_output_loss: 1.5575 - val_bag_output_loss: 1.4335 - val_footwear_output_loss: 2.0063 - val_pose_output_loss: 1.0392 - val_emotion_output_loss: 1.5190 - val_gender_output_acc: 0.8264 - val_image_quality_output_acc: 0.5278 - val_age_output_acc: 0.3681 - val_weight_output_acc: 0.5908 - val_bag_output_acc: 0.6240 - val_footwear_output_acc: 0.6190 - val_pose_output_acc: 0.8130 - val_emotion_output_acc: 0.5719\n",
      "\n",
      "Epoch 00070: loss improved from 1.86548 to 1.79011, saving model to model.h5\n",
      "Epoch 71/200\n",
      " - 84s - loss: 2.0747 - gender_output_loss: 0.0304 - image_quality_output_loss: 0.1779 - age_output_loss: 0.5794 - weight_output_loss: 0.4270 - bag_output_loss: 0.2645 - footwear_output_loss: 0.1350 - pose_output_loss: 0.0432 - emotion_output_loss: 0.4173 - gender_output_acc: 0.9885 - image_quality_output_acc: 0.9326 - age_output_acc: 0.7819 - weight_output_acc: 0.8375 - bag_output_acc: 0.8845 - footwear_output_acc: 0.9483 - pose_output_acc: 0.9847 - emotion_output_acc: 0.8462 - val_loss: 13.0106 - val_gender_output_loss: 0.8529 - val_image_quality_output_loss: 2.5057 - val_age_output_loss: 2.0219 - val_weight_output_loss: 1.6556 - val_bag_output_loss: 1.5067 - val_footwear_output_loss: 1.9632 - val_pose_output_loss: 1.0073 - val_emotion_output_loss: 1.4973 - val_gender_output_acc: 0.8185 - val_image_quality_output_acc: 0.5293 - val_age_output_acc: 0.3775 - val_weight_output_acc: 0.5828 - val_bag_output_acc: 0.6255 - val_footwear_output_acc: 0.6136 - val_pose_output_acc: 0.8175 - val_emotion_output_acc: 0.6260\n",
      "\n",
      "Epoch 00071: loss did not improve from 1.79011\n",
      "Epoch 72/200\n",
      " - 84s - loss: 1.6254 - gender_output_loss: 0.0200 - image_quality_output_loss: 0.1112 - age_output_loss: 0.4590 - weight_output_loss: 0.3543 - bag_output_loss: 0.2325 - footwear_output_loss: 0.0804 - pose_output_loss: 0.0256 - emotion_output_loss: 0.3424 - gender_output_acc: 0.9937 - image_quality_output_acc: 0.9576 - age_output_acc: 0.8331 - weight_output_acc: 0.8628 - bag_output_acc: 0.8956 - footwear_output_acc: 0.9696 - pose_output_acc: 0.9915 - emotion_output_acc: 0.8706 - val_loss: 13.9060 - val_gender_output_loss: 1.3091 - val_image_quality_output_loss: 2.6149 - val_age_output_loss: 2.1654 - val_weight_output_loss: 1.4110 - val_bag_output_loss: 1.7281 - val_footwear_output_loss: 2.0234 - val_pose_output_loss: 1.2698 - val_emotion_output_loss: 1.3843 - val_gender_output_acc: 0.7639 - val_image_quality_output_acc: 0.5104 - val_age_output_acc: 0.3313 - val_weight_output_acc: 0.5714 - val_bag_output_acc: 0.5456 - val_footwear_output_acc: 0.6215 - val_pose_output_acc: 0.8105 - val_emotion_output_acc: 0.5511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00072: loss improved from 1.79011 to 1.62542, saving model to model.h5\n",
      "Epoch 73/200\n",
      " - 84s - loss: 2.1351 - gender_output_loss: 0.0385 - image_quality_output_loss: 0.2164 - age_output_loss: 0.5720 - weight_output_loss: 0.4258 - bag_output_loss: 0.2726 - footwear_output_loss: 0.1490 - pose_output_loss: 0.0499 - emotion_output_loss: 0.4109 - gender_output_acc: 0.9865 - image_quality_output_acc: 0.9128 - age_output_acc: 0.7900 - weight_output_acc: 0.8370 - bag_output_acc: 0.8831 - footwear_output_acc: 0.9456 - pose_output_acc: 0.9822 - emotion_output_acc: 0.8443 - val_loss: 12.7351 - val_gender_output_loss: 0.8543 - val_image_quality_output_loss: 2.2035 - val_age_output_loss: 2.0795 - val_weight_output_loss: 1.6540 - val_bag_output_loss: 1.4839 - val_footwear_output_loss: 1.8797 - val_pose_output_loss: 1.0037 - val_emotion_output_loss: 1.5765 - val_gender_output_acc: 0.8304 - val_image_quality_output_acc: 0.5268 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6071 - val_bag_output_acc: 0.6136 - val_footwear_output_acc: 0.6220 - val_pose_output_acc: 0.8234 - val_emotion_output_acc: 0.6156\n",
      "\n",
      "Epoch 00073: loss did not improve from 1.62542\n",
      "Epoch 74/200\n",
      " - 84s - loss: 1.8884 - gender_output_loss: 0.0315 - image_quality_output_loss: 0.1731 - age_output_loss: 0.5062 - weight_output_loss: 0.3862 - bag_output_loss: 0.2524 - footwear_output_loss: 0.1193 - pose_output_loss: 0.0392 - emotion_output_loss: 0.3806 - gender_output_acc: 0.9892 - image_quality_output_acc: 0.9331 - age_output_acc: 0.8125 - weight_output_acc: 0.8536 - bag_output_acc: 0.8865 - footwear_output_acc: 0.9555 - pose_output_acc: 0.9862 - emotion_output_acc: 0.8564 - val_loss: 13.1682 - val_gender_output_loss: 0.7335 - val_image_quality_output_loss: 2.2126 - val_age_output_loss: 2.5243 - val_weight_output_loss: 1.8953 - val_bag_output_loss: 1.3693 - val_footwear_output_loss: 1.8600 - val_pose_output_loss: 1.0414 - val_emotion_output_loss: 1.5317 - val_gender_output_acc: 0.8254 - val_image_quality_output_acc: 0.5412 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6096 - val_bag_output_acc: 0.6205 - val_footwear_output_acc: 0.6305 - val_pose_output_acc: 0.8021 - val_emotion_output_acc: 0.6250\n",
      "\n",
      "Epoch 00074: loss did not improve from 1.62542\n",
      "Epoch 75/200\n",
      " - 84s - loss: 1.6381 - gender_output_loss: 0.0218 - image_quality_output_loss: 0.1309 - age_output_loss: 0.4475 - weight_output_loss: 0.3502 - bag_output_loss: 0.2350 - footwear_output_loss: 0.0841 - pose_output_loss: 0.0275 - emotion_output_loss: 0.3410 - gender_output_acc: 0.9927 - image_quality_output_acc: 0.9508 - age_output_acc: 0.8343 - weight_output_acc: 0.8635 - bag_output_acc: 0.8926 - footwear_output_acc: 0.9694 - pose_output_acc: 0.9903 - emotion_output_acc: 0.8724 - val_loss: 13.7771 - val_gender_output_loss: 0.8977 - val_image_quality_output_loss: 2.4122 - val_age_output_loss: 2.2372 - val_weight_output_loss: 1.8235 - val_bag_output_loss: 1.4973 - val_footwear_output_loss: 2.1363 - val_pose_output_loss: 1.0188 - val_emotion_output_loss: 1.7541 - val_gender_output_acc: 0.8244 - val_image_quality_output_acc: 0.5258 - val_age_output_acc: 0.3537 - val_weight_output_acc: 0.6171 - val_bag_output_acc: 0.6086 - val_footwear_output_acc: 0.6334 - val_pose_output_acc: 0.8304 - val_emotion_output_acc: 0.6458\n",
      "\n",
      "Epoch 00075: loss did not improve from 1.62542\n",
      "Epoch 76/200\n",
      " - 84s - loss: 1.8850 - gender_output_loss: 0.0346 - image_quality_output_loss: 0.1791 - age_output_loss: 0.4823 - weight_output_loss: 0.3921 - bag_output_loss: 0.2628 - footwear_output_loss: 0.1176 - pose_output_loss: 0.0426 - emotion_output_loss: 0.3740 - gender_output_acc: 0.9865 - image_quality_output_acc: 0.9335 - age_output_acc: 0.8179 - weight_output_acc: 0.8504 - bag_output_acc: 0.8846 - footwear_output_acc: 0.9568 - pose_output_acc: 0.9842 - emotion_output_acc: 0.8615 - val_loss: 12.8075 - val_gender_output_loss: 0.7995 - val_image_quality_output_loss: 2.3394 - val_age_output_loss: 2.1747 - val_weight_output_loss: 1.5636 - val_bag_output_loss: 1.5090 - val_footwear_output_loss: 1.9306 - val_pose_output_loss: 0.9946 - val_emotion_output_loss: 1.4961 - val_gender_output_acc: 0.8309 - val_image_quality_output_acc: 0.5218 - val_age_output_acc: 0.3557 - val_weight_output_acc: 0.5719 - val_bag_output_acc: 0.6295 - val_footwear_output_acc: 0.6215 - val_pose_output_acc: 0.8160 - val_emotion_output_acc: 0.5521\n",
      "\n",
      "Epoch 00076: loss did not improve from 1.62542\n",
      "Epoch 77/200\n",
      " - 84s - loss: 1.4056 - gender_output_loss: 0.0201 - image_quality_output_loss: 0.0910 - age_output_loss: 0.3813 - weight_output_loss: 0.3040 - bag_output_loss: 0.2190 - footwear_output_loss: 0.0704 - pose_output_loss: 0.0202 - emotion_output_loss: 0.2996 - gender_output_acc: 0.9938 - image_quality_output_acc: 0.9671 - age_output_acc: 0.8586 - weight_output_acc: 0.8783 - bag_output_acc: 0.8967 - footwear_output_acc: 0.9750 - pose_output_acc: 0.9929 - emotion_output_acc: 0.8868 - val_loss: 15.8251 - val_gender_output_loss: 0.9525 - val_image_quality_output_loss: 4.2261 - val_age_output_loss: 2.2421 - val_weight_output_loss: 1.9213 - val_bag_output_loss: 1.3603 - val_footwear_output_loss: 2.0467 - val_pose_output_loss: 1.0709 - val_emotion_output_loss: 2.0051 - val_gender_output_acc: 0.8264 - val_image_quality_output_acc: 0.4241 - val_age_output_acc: 0.3328 - val_weight_output_acc: 0.6062 - val_bag_output_acc: 0.6136 - val_footwear_output_acc: 0.6116 - val_pose_output_acc: 0.8110 - val_emotion_output_acc: 0.6558\n",
      "\n",
      "Epoch 00077: loss improved from 1.62542 to 1.40559, saving model to model.h5\n",
      "Epoch 78/200\n",
      " - 84s - loss: 1.7165 - gender_output_loss: 0.0320 - image_quality_output_loss: 0.1481 - age_output_loss: 0.4547 - weight_output_loss: 0.3580 - bag_output_loss: 0.2444 - footwear_output_loss: 0.1017 - pose_output_loss: 0.0331 - emotion_output_loss: 0.3445 - gender_output_acc: 0.9887 - image_quality_output_acc: 0.9445 - age_output_acc: 0.8295 - weight_output_acc: 0.8622 - bag_output_acc: 0.8885 - footwear_output_acc: 0.9643 - pose_output_acc: 0.9884 - emotion_output_acc: 0.8731 - val_loss: 13.9343 - val_gender_output_loss: 0.8819 - val_image_quality_output_loss: 2.3266 - val_age_output_loss: 2.3555 - val_weight_output_loss: 1.9140 - val_bag_output_loss: 1.4970 - val_footwear_output_loss: 2.0885 - val_pose_output_loss: 1.1367 - val_emotion_output_loss: 1.7340 - val_gender_output_acc: 0.8338 - val_image_quality_output_acc: 0.5298 - val_age_output_acc: 0.3834 - val_weight_output_acc: 0.6200 - val_bag_output_acc: 0.6131 - val_footwear_output_acc: 0.6319 - val_pose_output_acc: 0.8289 - val_emotion_output_acc: 0.6310\n",
      "\n",
      "Epoch 00078: loss did not improve from 1.40559\n",
      "Epoch 79/200\n",
      " - 84s - loss: 1.6016 - gender_output_loss: 0.0251 - image_quality_output_loss: 0.1267 - age_output_loss: 0.4181 - weight_output_loss: 0.3335 - bag_output_loss: 0.2369 - footwear_output_loss: 0.0981 - pose_output_loss: 0.0365 - emotion_output_loss: 0.3267 - gender_output_acc: 0.9918 - image_quality_output_acc: 0.9541 - age_output_acc: 0.8404 - weight_output_acc: 0.8681 - bag_output_acc: 0.8905 - footwear_output_acc: 0.9666 - pose_output_acc: 0.9868 - emotion_output_acc: 0.8770 - val_loss: 13.7860 - val_gender_output_loss: 1.0071 - val_image_quality_output_loss: 2.3793 - val_age_output_loss: 2.2779 - val_weight_output_loss: 1.6641 - val_bag_output_loss: 1.7890 - val_footwear_output_loss: 1.9124 - val_pose_output_loss: 1.0900 - val_emotion_output_loss: 1.6661 - val_gender_output_acc: 0.8269 - val_image_quality_output_acc: 0.5258 - val_age_output_acc: 0.3423 - val_weight_output_acc: 0.5590 - val_bag_output_acc: 0.6210 - val_footwear_output_acc: 0.6300 - val_pose_output_acc: 0.8125 - val_emotion_output_acc: 0.6057\n",
      "\n",
      "Epoch 00079: loss did not improve from 1.40559\n",
      "Epoch 80/200\n",
      " - 84s - loss: 1.3958 - gender_output_loss: 0.0193 - image_quality_output_loss: 0.1001 - age_output_loss: 0.3617 - weight_output_loss: 0.2983 - bag_output_loss: 0.2254 - footwear_output_loss: 0.0713 - pose_output_loss: 0.0210 - emotion_output_loss: 0.2988 - gender_output_acc: 0.9937 - image_quality_output_acc: 0.9645 - age_output_acc: 0.8637 - weight_output_acc: 0.8795 - bag_output_acc: 0.8938 - footwear_output_acc: 0.9754 - pose_output_acc: 0.9933 - emotion_output_acc: 0.8907 - val_loss: 16.1119 - val_gender_output_loss: 0.8486 - val_image_quality_output_loss: 3.9202 - val_age_output_loss: 2.7377 - val_weight_output_loss: 2.0615 - val_bag_output_loss: 1.5185 - val_footwear_output_loss: 2.1180 - val_pose_output_loss: 1.1631 - val_emotion_output_loss: 1.7444 - val_gender_output_acc: 0.8333 - val_image_quality_output_acc: 0.4539 - val_age_output_acc: 0.3924 - val_weight_output_acc: 0.6151 - val_bag_output_acc: 0.6285 - val_footwear_output_acc: 0.6329 - val_pose_output_acc: 0.7922 - val_emotion_output_acc: 0.6270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00080: loss improved from 1.40559 to 1.39579, saving model to model.h5\n",
      "Epoch 81/200\n",
      " - 84s - loss: 1.7189 - gender_output_loss: 0.0342 - image_quality_output_loss: 0.1644 - age_output_loss: 0.4409 - weight_output_loss: 0.3517 - bag_output_loss: 0.2434 - footwear_output_loss: 0.1106 - pose_output_loss: 0.0370 - emotion_output_loss: 0.3366 - gender_output_acc: 0.9883 - image_quality_output_acc: 0.9391 - age_output_acc: 0.8341 - weight_output_acc: 0.8650 - bag_output_acc: 0.8877 - footwear_output_acc: 0.9612 - pose_output_acc: 0.9866 - emotion_output_acc: 0.8736 - val_loss: 13.4221 - val_gender_output_loss: 0.8296 - val_image_quality_output_loss: 2.4526 - val_age_output_loss: 2.2562 - val_weight_output_loss: 1.5991 - val_bag_output_loss: 1.5607 - val_footwear_output_loss: 1.9506 - val_pose_output_loss: 1.0873 - val_emotion_output_loss: 1.6859 - val_gender_output_acc: 0.8279 - val_image_quality_output_acc: 0.4921 - val_age_output_acc: 0.3581 - val_weight_output_acc: 0.5893 - val_bag_output_acc: 0.6176 - val_footwear_output_acc: 0.6076 - val_pose_output_acc: 0.8185 - val_emotion_output_acc: 0.6290\n",
      "\n",
      "Epoch 00081: loss did not improve from 1.39579\n",
      "Epoch 82/200\n",
      " - 84s - loss: 1.2529 - gender_output_loss: 0.0131 - image_quality_output_loss: 0.0980 - age_output_loss: 0.3234 - weight_output_loss: 0.2634 - bag_output_loss: 0.2176 - footwear_output_loss: 0.0572 - pose_output_loss: 0.0244 - emotion_output_loss: 0.2558 - gender_output_acc: 0.9960 - image_quality_output_acc: 0.9659 - age_output_acc: 0.8767 - weight_output_acc: 0.8923 - bag_output_acc: 0.8937 - footwear_output_acc: 0.9807 - pose_output_acc: 0.9917 - emotion_output_acc: 0.9030 - val_loss: 16.8964 - val_gender_output_loss: 0.7935 - val_image_quality_output_loss: 4.5308 - val_age_output_loss: 2.6234 - val_weight_output_loss: 1.8416 - val_bag_output_loss: 1.4500 - val_footwear_output_loss: 2.5694 - val_pose_output_loss: 1.1241 - val_emotion_output_loss: 1.9635 - val_gender_output_acc: 0.8363 - val_image_quality_output_acc: 0.4147 - val_age_output_acc: 0.3914 - val_weight_output_acc: 0.5933 - val_bag_output_acc: 0.6141 - val_footwear_output_acc: 0.6334 - val_pose_output_acc: 0.8061 - val_emotion_output_acc: 0.6771\n",
      "\n",
      "Epoch 00082: loss improved from 1.39579 to 1.25294, saving model to model.h5\n",
      "Epoch 83/200\n",
      " - 84s - loss: 1.5813 - gender_output_loss: 0.0263 - image_quality_output_loss: 0.1416 - age_output_loss: 0.4054 - weight_output_loss: 0.3182 - bag_output_loss: 0.2361 - footwear_output_loss: 0.1052 - pose_output_loss: 0.0352 - emotion_output_loss: 0.3132 - gender_output_acc: 0.9907 - image_quality_output_acc: 0.9471 - age_output_acc: 0.8460 - weight_output_acc: 0.8717 - bag_output_acc: 0.8905 - footwear_output_acc: 0.9626 - pose_output_acc: 0.9882 - emotion_output_acc: 0.8840 - val_loss: 13.8973 - val_gender_output_loss: 0.8227 - val_image_quality_output_loss: 2.4546 - val_age_output_loss: 2.3434 - val_weight_output_loss: 1.9190 - val_bag_output_loss: 1.5414 - val_footwear_output_loss: 2.0403 - val_pose_output_loss: 1.0527 - val_emotion_output_loss: 1.7233 - val_gender_output_acc: 0.8482 - val_image_quality_output_acc: 0.5337 - val_age_output_acc: 0.3700 - val_weight_output_acc: 0.6121 - val_bag_output_acc: 0.6255 - val_footwear_output_acc: 0.6156 - val_pose_output_acc: 0.8249 - val_emotion_output_acc: 0.6359\n",
      "\n",
      "Epoch 00083: loss did not improve from 1.25294\n",
      "Epoch 84/200\n",
      " - 84s - loss: 1.4472 - gender_output_loss: 0.0217 - image_quality_output_loss: 0.1272 - age_output_loss: 0.3644 - weight_output_loss: 0.3103 - bag_output_loss: 0.2291 - footwear_output_loss: 0.0769 - pose_output_loss: 0.0288 - emotion_output_loss: 0.2889 - gender_output_acc: 0.9914 - image_quality_output_acc: 0.9533 - age_output_acc: 0.8622 - weight_output_acc: 0.8757 - bag_output_acc: 0.8929 - footwear_output_acc: 0.9723 - pose_output_acc: 0.9899 - emotion_output_acc: 0.8868 - val_loss: 15.3471 - val_gender_output_loss: 0.9502 - val_image_quality_output_loss: 2.8743 - val_age_output_loss: 3.0577 - val_weight_output_loss: 2.0365 - val_bag_output_loss: 1.6283 - val_footwear_output_loss: 1.9657 - val_pose_output_loss: 1.1393 - val_emotion_output_loss: 1.6951 - val_gender_output_acc: 0.8234 - val_image_quality_output_acc: 0.4747 - val_age_output_acc: 0.3854 - val_weight_output_acc: 0.5625 - val_bag_output_acc: 0.5982 - val_footwear_output_acc: 0.6037 - val_pose_output_acc: 0.8056 - val_emotion_output_acc: 0.5179\n",
      "\n",
      "Epoch 00084: loss did not improve from 1.25294\n",
      "Epoch 85/200\n",
      " - 84s - loss: 1.2916 - gender_output_loss: 0.0178 - image_quality_output_loss: 0.1049 - age_output_loss: 0.3304 - weight_output_loss: 0.2753 - bag_output_loss: 0.2147 - footwear_output_loss: 0.0624 - pose_output_loss: 0.0221 - emotion_output_loss: 0.2639 - gender_output_acc: 0.9941 - image_quality_output_acc: 0.9615 - age_output_acc: 0.8756 - weight_output_acc: 0.8866 - bag_output_acc: 0.8964 - footwear_output_acc: 0.9774 - pose_output_acc: 0.9934 - emotion_output_acc: 0.8986 - val_loss: 14.3017 - val_gender_output_loss: 0.8980 - val_image_quality_output_loss: 2.4840 - val_age_output_loss: 2.4257 - val_weight_output_loss: 1.9200 - val_bag_output_loss: 1.5105 - val_footwear_output_loss: 2.1502 - val_pose_output_loss: 1.1241 - val_emotion_output_loss: 1.7891 - val_gender_output_acc: 0.8323 - val_image_quality_output_acc: 0.5184 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6007 - val_bag_output_acc: 0.6220 - val_footwear_output_acc: 0.6305 - val_pose_output_acc: 0.8120 - val_emotion_output_acc: 0.6265\n",
      "\n",
      "Epoch 00085: loss did not improve from 1.25294\n",
      "Epoch 86/200\n",
      " - 84s - loss: 1.4941 - gender_output_loss: 0.0281 - image_quality_output_loss: 0.1420 - age_output_loss: 0.3730 - weight_output_loss: 0.3011 - bag_output_loss: 0.2219 - footwear_output_loss: 0.0887 - pose_output_loss: 0.0378 - emotion_output_loss: 0.3015 - gender_output_acc: 0.9902 - image_quality_output_acc: 0.9471 - age_output_acc: 0.8586 - weight_output_acc: 0.8781 - bag_output_acc: 0.8913 - footwear_output_acc: 0.9661 - pose_output_acc: 0.9866 - emotion_output_acc: 0.8853 - val_loss: 14.3186 - val_gender_output_loss: 0.7733 - val_image_quality_output_loss: 2.4219 - val_age_output_loss: 2.4756 - val_weight_output_loss: 1.9357 - val_bag_output_loss: 1.7037 - val_footwear_output_loss: 2.0871 - val_pose_output_loss: 1.1438 - val_emotion_output_loss: 1.7774 - val_gender_output_acc: 0.8323 - val_image_quality_output_acc: 0.5263 - val_age_output_acc: 0.3666 - val_weight_output_acc: 0.5759 - val_bag_output_acc: 0.6176 - val_footwear_output_acc: 0.6205 - val_pose_output_acc: 0.8075 - val_emotion_output_acc: 0.6126\n",
      "\n",
      "Epoch 00086: loss did not improve from 1.25294\n",
      "Epoch 87/200\n",
      " - 84s - loss: 1.1288 - gender_output_loss: 0.0168 - image_quality_output_loss: 0.0863 - age_output_loss: 0.2772 - weight_output_loss: 0.2467 - bag_output_loss: 0.1966 - footwear_output_loss: 0.0522 - pose_output_loss: 0.0178 - emotion_output_loss: 0.2352 - gender_output_acc: 0.9944 - image_quality_output_acc: 0.9689 - age_output_acc: 0.8928 - weight_output_acc: 0.8957 - bag_output_acc: 0.8970 - footwear_output_acc: 0.9816 - pose_output_acc: 0.9936 - emotion_output_acc: 0.9099 - val_loss: 17.0614 - val_gender_output_loss: 1.3919 - val_image_quality_output_loss: 2.6567 - val_age_output_loss: 2.5986 - val_weight_output_loss: 2.4165 - val_bag_output_loss: 2.1731 - val_footwear_output_loss: 2.6891 - val_pose_output_loss: 1.1068 - val_emotion_output_loss: 2.0286 - val_gender_output_acc: 0.7917 - val_image_quality_output_acc: 0.5288 - val_age_output_acc: 0.3631 - val_weight_output_acc: 0.4866 - val_bag_output_acc: 0.6270 - val_footwear_output_acc: 0.6528 - val_pose_output_acc: 0.8105 - val_emotion_output_acc: 0.6101\n",
      "\n",
      "Epoch 00087: loss improved from 1.25294 to 1.12875, saving model to model.h5\n",
      "Epoch 88/200\n",
      " - 84s - loss: 1.4870 - gender_output_loss: 0.0306 - image_quality_output_loss: 0.1434 - age_output_loss: 0.3685 - weight_output_loss: 0.3003 - bag_output_loss: 0.2357 - footwear_output_loss: 0.0884 - pose_output_loss: 0.0328 - emotion_output_loss: 0.2873 - gender_output_acc: 0.9892 - image_quality_output_acc: 0.9473 - age_output_acc: 0.8597 - weight_output_acc: 0.8786 - bag_output_acc: 0.8891 - footwear_output_acc: 0.9677 - pose_output_acc: 0.9884 - emotion_output_acc: 0.8942 - val_loss: 14.3564 - val_gender_output_loss: 0.8036 - val_image_quality_output_loss: 2.4059 - val_age_output_loss: 2.5518 - val_weight_output_loss: 2.0155 - val_bag_output_loss: 1.5993 - val_footwear_output_loss: 2.1118 - val_pose_output_loss: 1.1057 - val_emotion_output_loss: 1.7628 - val_gender_output_acc: 0.8398 - val_image_quality_output_acc: 0.5367 - val_age_output_acc: 0.3765 - val_weight_output_acc: 0.6066 - val_bag_output_acc: 0.6319 - val_footwear_output_acc: 0.6186 - val_pose_output_acc: 0.8224 - val_emotion_output_acc: 0.6022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00088: loss did not improve from 1.12875\n",
      "Epoch 89/200\n",
      " - 84s - loss: 1.3608 - gender_output_loss: 0.0240 - image_quality_output_loss: 0.1236 - age_output_loss: 0.3437 - weight_output_loss: 0.2740 - bag_output_loss: 0.2207 - footwear_output_loss: 0.0727 - pose_output_loss: 0.0319 - emotion_output_loss: 0.2703 - gender_output_acc: 0.9911 - image_quality_output_acc: 0.9539 - age_output_acc: 0.8687 - weight_output_acc: 0.8870 - bag_output_acc: 0.8914 - footwear_output_acc: 0.9747 - pose_output_acc: 0.9896 - emotion_output_acc: 0.8990 - val_loss: 13.8835 - val_gender_output_loss: 0.7723 - val_image_quality_output_loss: 2.7368 - val_age_output_loss: 2.3675 - val_weight_output_loss: 1.5578 - val_bag_output_loss: 1.6086 - val_footwear_output_loss: 1.9310 - val_pose_output_loss: 1.1493 - val_emotion_output_loss: 1.7602 - val_gender_output_acc: 0.8175 - val_image_quality_output_acc: 0.4965 - val_age_output_acc: 0.3814 - val_weight_output_acc: 0.5208 - val_bag_output_acc: 0.6052 - val_footwear_output_acc: 0.6146 - val_pose_output_acc: 0.7808 - val_emotion_output_acc: 0.5432\n",
      "\n",
      "Epoch 00089: loss did not improve from 1.12875\n",
      "Epoch 90/200\n",
      " - 84s - loss: 1.1308 - gender_output_loss: 0.0139 - image_quality_output_loss: 0.0935 - age_output_loss: 0.2852 - weight_output_loss: 0.2381 - bag_output_loss: 0.1960 - footwear_output_loss: 0.0461 - pose_output_loss: 0.0210 - emotion_output_loss: 0.2369 - gender_output_acc: 0.9957 - image_quality_output_acc: 0.9669 - age_output_acc: 0.8895 - weight_output_acc: 0.8979 - bag_output_acc: 0.8973 - footwear_output_acc: 0.9842 - pose_output_acc: 0.9932 - emotion_output_acc: 0.9083 - val_loss: 15.7467 - val_gender_output_loss: 0.9316 - val_image_quality_output_loss: 3.1594 - val_age_output_loss: 2.7994 - val_weight_output_loss: 2.1540 - val_bag_output_loss: 1.5122 - val_footwear_output_loss: 2.1079 - val_pose_output_loss: 1.0997 - val_emotion_output_loss: 1.9826 - val_gender_output_acc: 0.8328 - val_image_quality_output_acc: 0.5099 - val_age_output_acc: 0.3909 - val_weight_output_acc: 0.6057 - val_bag_output_acc: 0.6334 - val_footwear_output_acc: 0.6310 - val_pose_output_acc: 0.8239 - val_emotion_output_acc: 0.6210\n",
      "\n",
      "Epoch 00090: loss did not improve from 1.12875\n",
      "Epoch 91/200\n",
      " - 84s - loss: 1.3856 - gender_output_loss: 0.0279 - image_quality_output_loss: 0.1242 - age_output_loss: 0.3542 - weight_output_loss: 0.2863 - bag_output_loss: 0.2070 - footwear_output_loss: 0.0938 - pose_output_loss: 0.0337 - emotion_output_loss: 0.2585 - gender_output_acc: 0.9898 - image_quality_output_acc: 0.9540 - age_output_acc: 0.8628 - weight_output_acc: 0.8833 - bag_output_acc: 0.8939 - footwear_output_acc: 0.9669 - pose_output_acc: 0.9889 - emotion_output_acc: 0.9008 - val_loss: 14.8048 - val_gender_output_loss: 0.8134 - val_image_quality_output_loss: 2.6637 - val_age_output_loss: 2.4702 - val_weight_output_loss: 2.0934 - val_bag_output_loss: 1.6476 - val_footwear_output_loss: 2.1130 - val_pose_output_loss: 1.0984 - val_emotion_output_loss: 1.9051 - val_gender_output_acc: 0.8338 - val_image_quality_output_acc: 0.5263 - val_age_output_acc: 0.3790 - val_weight_output_acc: 0.6091 - val_bag_output_acc: 0.6260 - val_footwear_output_acc: 0.6220 - val_pose_output_acc: 0.8105 - val_emotion_output_acc: 0.6215\n",
      "\n",
      "Epoch 00091: loss did not improve from 1.12875\n",
      "Epoch 92/200\n",
      " - 84s - loss: 1.0496 - gender_output_loss: 0.0130 - image_quality_output_loss: 0.0837 - age_output_loss: 0.2545 - weight_output_loss: 0.2214 - bag_output_loss: 0.1932 - footwear_output_loss: 0.0462 - pose_output_loss: 0.0178 - emotion_output_loss: 0.2198 - gender_output_acc: 0.9958 - image_quality_output_acc: 0.9694 - age_output_acc: 0.8997 - weight_output_acc: 0.9040 - bag_output_acc: 0.8971 - footwear_output_acc: 0.9846 - pose_output_acc: 0.9937 - emotion_output_acc: 0.9142 - val_loss: 15.6188 - val_gender_output_loss: 0.8654 - val_image_quality_output_loss: 2.6461 - val_age_output_loss: 2.8693 - val_weight_output_loss: 2.4918 - val_bag_output_loss: 1.5333 - val_footwear_output_loss: 2.1465 - val_pose_output_loss: 1.1297 - val_emotion_output_loss: 1.9368 - val_gender_output_acc: 0.8398 - val_image_quality_output_acc: 0.5139 - val_age_output_acc: 0.3695 - val_weight_output_acc: 0.6215 - val_bag_output_acc: 0.6181 - val_footwear_output_acc: 0.5823 - val_pose_output_acc: 0.8155 - val_emotion_output_acc: 0.5903\n",
      "\n",
      "Epoch 00092: loss improved from 1.12875 to 1.04959, saving model to model.h5\n",
      "Epoch 93/200\n",
      " - 84s - loss: 1.2958 - gender_output_loss: 0.0254 - image_quality_output_loss: 0.1266 - age_output_loss: 0.3212 - weight_output_loss: 0.2641 - bag_output_loss: 0.2071 - footwear_output_loss: 0.0674 - pose_output_loss: 0.0290 - emotion_output_loss: 0.2548 - gender_output_acc: 0.9918 - image_quality_output_acc: 0.9492 - age_output_acc: 0.8757 - weight_output_acc: 0.8924 - bag_output_acc: 0.8924 - footwear_output_acc: 0.9762 - pose_output_acc: 0.9897 - emotion_output_acc: 0.9019 - val_loss: 14.8232 - val_gender_output_loss: 0.8568 - val_image_quality_output_loss: 2.5316 - val_age_output_loss: 2.5924 - val_weight_output_loss: 1.9063 - val_bag_output_loss: 1.7947 - val_footwear_output_loss: 2.2109 - val_pose_output_loss: 1.1215 - val_emotion_output_loss: 1.8090 - val_gender_output_acc: 0.8353 - val_image_quality_output_acc: 0.5298 - val_age_output_acc: 0.3690 - val_weight_output_acc: 0.5938 - val_bag_output_acc: 0.6250 - val_footwear_output_acc: 0.6359 - val_pose_output_acc: 0.8185 - val_emotion_output_acc: 0.6052\n",
      "\n",
      "Epoch 00093: loss did not improve from 1.04959\n",
      "Epoch 94/200\n",
      " - 84s - loss: 1.2460 - gender_output_loss: 0.0227 - image_quality_output_loss: 0.1184 - age_output_loss: 0.3006 - weight_output_loss: 0.2602 - bag_output_loss: 0.2085 - footwear_output_loss: 0.0648 - pose_output_loss: 0.0291 - emotion_output_loss: 0.2415 - gender_output_acc: 0.9935 - image_quality_output_acc: 0.9567 - age_output_acc: 0.8826 - weight_output_acc: 0.8892 - bag_output_acc: 0.8921 - footwear_output_acc: 0.9754 - pose_output_acc: 0.9900 - emotion_output_acc: 0.9069 - val_loss: 15.0285 - val_gender_output_loss: 0.9242 - val_image_quality_output_loss: 2.7195 - val_age_output_loss: 2.5116 - val_weight_output_loss: 1.8534 - val_bag_output_loss: 1.6812 - val_footwear_output_loss: 2.3358 - val_pose_output_loss: 1.0956 - val_emotion_output_loss: 1.9074 - val_gender_output_acc: 0.8075 - val_image_quality_output_acc: 0.5228 - val_age_output_acc: 0.3745 - val_weight_output_acc: 0.5511 - val_bag_output_acc: 0.6186 - val_footwear_output_acc: 0.6305 - val_pose_output_acc: 0.8110 - val_emotion_output_acc: 0.6443\n",
      "\n",
      "Epoch 00094: loss did not improve from 1.04959\n",
      "Epoch 95/200\n",
      " - 84s - loss: 1.0650 - gender_output_loss: 0.0146 - image_quality_output_loss: 0.0819 - age_output_loss: 0.2627 - weight_output_loss: 0.2331 - bag_output_loss: 0.1863 - footwear_output_loss: 0.0486 - pose_output_loss: 0.0218 - emotion_output_loss: 0.2162 - gender_output_acc: 0.9959 - image_quality_output_acc: 0.9704 - age_output_acc: 0.8952 - weight_output_acc: 0.8992 - bag_output_acc: 0.8981 - footwear_output_acc: 0.9844 - pose_output_acc: 0.9931 - emotion_output_acc: 0.9127 - val_loss: 14.9326 - val_gender_output_loss: 0.9057 - val_image_quality_output_loss: 2.5560 - val_age_output_loss: 2.7116 - val_weight_output_loss: 1.8753 - val_bag_output_loss: 1.6932 - val_footwear_output_loss: 2.2003 - val_pose_output_loss: 1.1404 - val_emotion_output_loss: 1.8500 - val_gender_output_acc: 0.8328 - val_image_quality_output_acc: 0.5094 - val_age_output_acc: 0.3760 - val_weight_output_acc: 0.5799 - val_bag_output_acc: 0.6260 - val_footwear_output_acc: 0.6359 - val_pose_output_acc: 0.8041 - val_emotion_output_acc: 0.5779\n",
      "\n",
      "Epoch 00095: loss did not improve from 1.04959\n",
      "Epoch 96/200\n",
      " - 84s - loss: 1.2999 - gender_output_loss: 0.0263 - image_quality_output_loss: 0.1309 - age_output_loss: 0.3170 - weight_output_loss: 0.2653 - bag_output_loss: 0.2089 - footwear_output_loss: 0.0823 - pose_output_loss: 0.0293 - emotion_output_loss: 0.2398 - gender_output_acc: 0.9913 - image_quality_output_acc: 0.9520 - age_output_acc: 0.8768 - weight_output_acc: 0.8890 - bag_output_acc: 0.8918 - footwear_output_acc: 0.9726 - pose_output_acc: 0.9900 - emotion_output_acc: 0.9064 - val_loss: 14.4413 - val_gender_output_loss: 0.8167 - val_image_quality_output_loss: 2.4662 - val_age_output_loss: 2.4618 - val_weight_output_loss: 2.0637 - val_bag_output_loss: 1.5940 - val_footwear_output_loss: 2.1272 - val_pose_output_loss: 1.0559 - val_emotion_output_loss: 1.8557 - val_gender_output_acc: 0.8388 - val_image_quality_output_acc: 0.5134 - val_age_output_acc: 0.3765 - val_weight_output_acc: 0.5992 - val_bag_output_acc: 0.6186 - val_footwear_output_acc: 0.6136 - val_pose_output_acc: 0.8229 - val_emotion_output_acc: 0.5967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00096: loss did not improve from 1.04959\n",
      "Epoch 97/200\n",
      " - 84s - loss: 0.9614 - gender_output_loss: 0.0135 - image_quality_output_loss: 0.0756 - age_output_loss: 0.2374 - weight_output_loss: 0.2133 - bag_output_loss: 0.1763 - footwear_output_loss: 0.0386 - pose_output_loss: 0.0181 - emotion_output_loss: 0.1886 - gender_output_acc: 0.9958 - image_quality_output_acc: 0.9739 - age_output_acc: 0.9053 - weight_output_acc: 0.9044 - bag_output_acc: 0.8979 - footwear_output_acc: 0.9868 - pose_output_acc: 0.9937 - emotion_output_acc: 0.9199 - val_loss: 16.7789 - val_gender_output_loss: 0.9481 - val_image_quality_output_loss: 2.7632 - val_age_output_loss: 2.8570 - val_weight_output_loss: 2.3432 - val_bag_output_loss: 2.0623 - val_footwear_output_loss: 2.5978 - val_pose_output_loss: 1.1185 - val_emotion_output_loss: 2.0887 - val_gender_output_acc: 0.8309 - val_image_quality_output_acc: 0.5387 - val_age_output_acc: 0.3626 - val_weight_output_acc: 0.5828 - val_bag_output_acc: 0.6235 - val_footwear_output_acc: 0.5873 - val_pose_output_acc: 0.8100 - val_emotion_output_acc: 0.6186\n",
      "\n",
      "Epoch 00097: loss improved from 1.04959 to 0.96136, saving model to model.h5\n",
      "Epoch 98/200\n",
      " - 84s - loss: 1.2449 - gender_output_loss: 0.0178 - image_quality_output_loss: 0.1251 - age_output_loss: 0.2981 - weight_output_loss: 0.2510 - bag_output_loss: 0.2009 - footwear_output_loss: 0.0761 - pose_output_loss: 0.0309 - emotion_output_loss: 0.2450 - gender_output_acc: 0.9942 - image_quality_output_acc: 0.9543 - age_output_acc: 0.8837 - weight_output_acc: 0.8938 - bag_output_acc: 0.8929 - footwear_output_acc: 0.9734 - pose_output_acc: 0.9883 - emotion_output_acc: 0.9068 - val_loss: 15.0976 - val_gender_output_loss: 0.8995 - val_image_quality_output_loss: 2.5834 - val_age_output_loss: 2.6332 - val_weight_output_loss: 2.1395 - val_bag_output_loss: 1.6657 - val_footwear_output_loss: 2.1124 - val_pose_output_loss: 1.1308 - val_emotion_output_loss: 1.9332 - val_gender_output_acc: 0.8368 - val_image_quality_output_acc: 0.5248 - val_age_output_acc: 0.3720 - val_weight_output_acc: 0.6062 - val_bag_output_acc: 0.6116 - val_footwear_output_acc: 0.6190 - val_pose_output_acc: 0.8229 - val_emotion_output_acc: 0.6166\n",
      "\n",
      "Epoch 00098: loss did not improve from 0.96136\n",
      "Epoch 99/200\n",
      " - 84s - loss: 1.1372 - gender_output_loss: 0.0185 - image_quality_output_loss: 0.1106 - age_output_loss: 0.2695 - weight_output_loss: 0.2327 - bag_output_loss: 0.1972 - footwear_output_loss: 0.0602 - pose_output_loss: 0.0309 - emotion_output_loss: 0.2176 - gender_output_acc: 0.9944 - image_quality_output_acc: 0.9616 - age_output_acc: 0.8945 - weight_output_acc: 0.8976 - bag_output_acc: 0.8934 - footwear_output_acc: 0.9806 - pose_output_acc: 0.9902 - emotion_output_acc: 0.9116 - val_loss: 15.3511 - val_gender_output_loss: 0.8578 - val_image_quality_output_loss: 2.4877 - val_age_output_loss: 2.9551 - val_weight_output_loss: 2.1885 - val_bag_output_loss: 1.7210 - val_footwear_output_loss: 2.1550 - val_pose_output_loss: 1.1020 - val_emotion_output_loss: 1.8840 - val_gender_output_acc: 0.8338 - val_image_quality_output_acc: 0.5179 - val_age_output_acc: 0.3938 - val_weight_output_acc: 0.5506 - val_bag_output_acc: 0.5809 - val_footwear_output_acc: 0.6314 - val_pose_output_acc: 0.7991 - val_emotion_output_acc: 0.5977\n",
      "\n",
      "Epoch 00099: loss did not improve from 0.96136\n",
      "Epoch 100/200\n",
      " - 84s - loss: 0.9999 - gender_output_loss: 0.0166 - image_quality_output_loss: 0.0848 - age_output_loss: 0.2458 - weight_output_loss: 0.2173 - bag_output_loss: 0.1793 - footwear_output_loss: 0.0408 - pose_output_loss: 0.0180 - emotion_output_loss: 0.1972 - gender_output_acc: 0.9950 - image_quality_output_acc: 0.9678 - age_output_acc: 0.9016 - weight_output_acc: 0.9036 - bag_output_acc: 0.8970 - footwear_output_acc: 0.9858 - pose_output_acc: 0.9938 - emotion_output_acc: 0.9184 - val_loss: 15.7076 - val_gender_output_loss: 0.9143 - val_image_quality_output_loss: 3.0138 - val_age_output_loss: 2.6201 - val_weight_output_loss: 2.0424 - val_bag_output_loss: 1.7740 - val_footwear_output_loss: 2.2122 - val_pose_output_loss: 1.1497 - val_emotion_output_loss: 1.9813 - val_gender_output_acc: 0.8348 - val_image_quality_output_acc: 0.4787 - val_age_output_acc: 0.3725 - val_weight_output_acc: 0.5853 - val_bag_output_acc: 0.6116 - val_footwear_output_acc: 0.6091 - val_pose_output_acc: 0.8090 - val_emotion_output_acc: 0.6146\n",
      "\n",
      "Epoch 00100: loss did not improve from 0.96136\n",
      "Epoch 101/200\n",
      " - 84s - loss: 1.1189 - gender_output_loss: 0.0179 - image_quality_output_loss: 0.1062 - age_output_loss: 0.2706 - weight_output_loss: 0.2378 - bag_output_loss: 0.1865 - footwear_output_loss: 0.0636 - pose_output_loss: 0.0276 - emotion_output_loss: 0.2087 - gender_output_acc: 0.9937 - image_quality_output_acc: 0.9617 - age_output_acc: 0.8936 - weight_output_acc: 0.8980 - bag_output_acc: 0.8962 - footwear_output_acc: 0.9774 - pose_output_acc: 0.9895 - emotion_output_acc: 0.9131 - val_loss: 15.4685 - val_gender_output_loss: 0.9627 - val_image_quality_output_loss: 2.5772 - val_age_output_loss: 2.7880 - val_weight_output_loss: 2.1349 - val_bag_output_loss: 1.7983 - val_footwear_output_loss: 2.1616 - val_pose_output_loss: 1.0668 - val_emotion_output_loss: 1.9790 - val_gender_output_acc: 0.8363 - val_image_quality_output_acc: 0.5169 - val_age_output_acc: 0.3770 - val_weight_output_acc: 0.6071 - val_bag_output_acc: 0.6166 - val_footwear_output_acc: 0.6215 - val_pose_output_acc: 0.8135 - val_emotion_output_acc: 0.6116\n",
      "\n",
      "Epoch 00101: loss did not improve from 0.96136\n",
      "Epoch 102/200\n",
      " - 84s - loss: 0.8820 - gender_output_loss: 0.0083 - image_quality_output_loss: 0.0665 - age_output_loss: 0.2152 - weight_output_loss: 0.1954 - bag_output_loss: 0.1696 - footwear_output_loss: 0.0338 - pose_output_loss: 0.0166 - emotion_output_loss: 0.1766 - gender_output_acc: 0.9976 - image_quality_output_acc: 0.9766 - age_output_acc: 0.9113 - weight_output_acc: 0.9102 - bag_output_acc: 0.8991 - footwear_output_acc: 0.9874 - pose_output_acc: 0.9944 - emotion_output_acc: 0.9258 - val_loss: 15.9156 - val_gender_output_loss: 1.0226 - val_image_quality_output_loss: 2.9308 - val_age_output_loss: 2.5242 - val_weight_output_loss: 2.1805 - val_bag_output_loss: 1.9810 - val_footwear_output_loss: 2.3987 - val_pose_output_loss: 1.2443 - val_emotion_output_loss: 1.6333 - val_gender_output_acc: 0.8259 - val_image_quality_output_acc: 0.5283 - val_age_output_acc: 0.3586 - val_weight_output_acc: 0.6017 - val_bag_output_acc: 0.6002 - val_footwear_output_acc: 0.6210 - val_pose_output_acc: 0.8065 - val_emotion_output_acc: 0.5650\n",
      "\n",
      "Epoch 00102: loss improved from 0.96136 to 0.88201, saving model to model.h5\n",
      "Epoch 103/200\n",
      " - 84s - loss: 1.2235 - gender_output_loss: 0.0283 - image_quality_output_loss: 0.1106 - age_output_loss: 0.2877 - weight_output_loss: 0.2565 - bag_output_loss: 0.1986 - footwear_output_loss: 0.0718 - pose_output_loss: 0.0313 - emotion_output_loss: 0.2386 - gender_output_acc: 0.9905 - image_quality_output_acc: 0.9597 - age_output_acc: 0.8859 - weight_output_acc: 0.8946 - bag_output_acc: 0.8928 - footwear_output_acc: 0.9753 - pose_output_acc: 0.9890 - emotion_output_acc: 0.9071 - val_loss: 15.4533 - val_gender_output_loss: 0.8640 - val_image_quality_output_loss: 2.6034 - val_age_output_loss: 2.8357 - val_weight_output_loss: 2.1398 - val_bag_output_loss: 1.7483 - val_footwear_output_loss: 2.2583 - val_pose_output_loss: 1.0935 - val_emotion_output_loss: 1.9102 - val_gender_output_acc: 0.8328 - val_image_quality_output_acc: 0.5253 - val_age_output_acc: 0.3934 - val_weight_output_acc: 0.6057 - val_bag_output_acc: 0.6359 - val_footwear_output_acc: 0.6270 - val_pose_output_acc: 0.8199 - val_emotion_output_acc: 0.6062\n",
      "\n",
      "Epoch 00103: loss did not improve from 0.88201\n",
      "Epoch 104/200\n",
      " - 84s - loss: 1.1167 - gender_output_loss: 0.0212 - image_quality_output_loss: 0.1064 - age_output_loss: 0.2733 - weight_output_loss: 0.2267 - bag_output_loss: 0.1866 - footwear_output_loss: 0.0645 - pose_output_loss: 0.0270 - emotion_output_loss: 0.2108 - gender_output_acc: 0.9931 - image_quality_output_acc: 0.9616 - age_output_acc: 0.8951 - weight_output_acc: 0.9011 - bag_output_acc: 0.8932 - footwear_output_acc: 0.9774 - pose_output_acc: 0.9909 - emotion_output_acc: 0.9148 - val_loss: 15.9277 - val_gender_output_loss: 0.9720 - val_image_quality_output_loss: 2.8658 - val_age_output_loss: 2.6466 - val_weight_output_loss: 2.0915 - val_bag_output_loss: 1.6403 - val_footwear_output_loss: 2.0932 - val_pose_output_loss: 1.3465 - val_emotion_output_loss: 2.2717 - val_gender_output_acc: 0.8269 - val_image_quality_output_acc: 0.5218 - val_age_output_acc: 0.3854 - val_weight_output_acc: 0.5883 - val_bag_output_acc: 0.6265 - val_footwear_output_acc: 0.5972 - val_pose_output_acc: 0.7723 - val_emotion_output_acc: 0.6473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00104: loss did not improve from 0.88201\n",
      "Epoch 105/200\n",
      " - 84s - loss: 0.9311 - gender_output_loss: 0.0134 - image_quality_output_loss: 0.0780 - age_output_loss: 0.2295 - weight_output_loss: 0.1987 - bag_output_loss: 0.1742 - footwear_output_loss: 0.0410 - pose_output_loss: 0.0145 - emotion_output_loss: 0.1817 - gender_output_acc: 0.9951 - image_quality_output_acc: 0.9717 - age_output_acc: 0.9071 - weight_output_acc: 0.9091 - bag_output_acc: 0.8982 - footwear_output_acc: 0.9865 - pose_output_acc: 0.9953 - emotion_output_acc: 0.9220 - val_loss: 15.4676 - val_gender_output_loss: 0.9614 - val_image_quality_output_loss: 2.4566 - val_age_output_loss: 2.7990 - val_weight_output_loss: 2.1609 - val_bag_output_loss: 1.7961 - val_footwear_output_loss: 2.2747 - val_pose_output_loss: 1.1146 - val_emotion_output_loss: 1.9042 - val_gender_output_acc: 0.8259 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3730 - val_weight_output_acc: 0.5918 - val_bag_output_acc: 0.6156 - val_footwear_output_acc: 0.6062 - val_pose_output_acc: 0.8145 - val_emotion_output_acc: 0.5625\n",
      "\n",
      "Epoch 00105: loss did not improve from 0.88201\n",
      "Epoch 106/200\n",
      " - 84s - loss: 1.0736 - gender_output_loss: 0.0193 - image_quality_output_loss: 0.1012 - age_output_loss: 0.2585 - weight_output_loss: 0.2259 - bag_output_loss: 0.1881 - footwear_output_loss: 0.0575 - pose_output_loss: 0.0177 - emotion_output_loss: 0.2056 - gender_output_acc: 0.9936 - image_quality_output_acc: 0.9635 - age_output_acc: 0.8960 - weight_output_acc: 0.9011 - bag_output_acc: 0.8939 - footwear_output_acc: 0.9803 - pose_output_acc: 0.9941 - emotion_output_acc: 0.9172 - val_loss: 15.7779 - val_gender_output_loss: 0.8852 - val_image_quality_output_loss: 2.6888 - val_age_output_loss: 2.9846 - val_weight_output_loss: 2.0182 - val_bag_output_loss: 1.7965 - val_footwear_output_loss: 2.2600 - val_pose_output_loss: 1.1425 - val_emotion_output_loss: 2.0021 - val_gender_output_acc: 0.8373 - val_image_quality_output_acc: 0.5179 - val_age_output_acc: 0.3854 - val_weight_output_acc: 0.5595 - val_bag_output_acc: 0.6210 - val_footwear_output_acc: 0.6215 - val_pose_output_acc: 0.8170 - val_emotion_output_acc: 0.5962\n",
      "\n",
      "Epoch 00106: loss did not improve from 0.88201\n",
      "Epoch 107/200\n",
      " - 84s - loss: 0.8126 - gender_output_loss: 0.0080 - image_quality_output_loss: 0.0582 - age_output_loss: 0.1992 - weight_output_loss: 0.1794 - bag_output_loss: 0.1634 - footwear_output_loss: 0.0297 - pose_output_loss: 0.0127 - emotion_output_loss: 0.1620 - gender_output_acc: 0.9979 - image_quality_output_acc: 0.9809 - age_output_acc: 0.9153 - weight_output_acc: 0.9141 - bag_output_acc: 0.8987 - footwear_output_acc: 0.9902 - pose_output_acc: 0.9960 - emotion_output_acc: 0.9292 - val_loss: 17.3238 - val_gender_output_loss: 1.0425 - val_image_quality_output_loss: 2.8735 - val_age_output_loss: 2.8408 - val_weight_output_loss: 2.3909 - val_bag_output_loss: 2.0703 - val_footwear_output_loss: 2.5760 - val_pose_output_loss: 1.2497 - val_emotion_output_loss: 2.2801 - val_gender_output_acc: 0.8313 - val_image_quality_output_acc: 0.5397 - val_age_output_acc: 0.3805 - val_weight_output_acc: 0.5580 - val_bag_output_acc: 0.6310 - val_footwear_output_acc: 0.6329 - val_pose_output_acc: 0.8036 - val_emotion_output_acc: 0.5799\n",
      "\n",
      "Epoch 00107: loss improved from 0.88201 to 0.81259, saving model to model.h5\n",
      "Epoch 108/200\n",
      " - 84s - loss: 1.1288 - gender_output_loss: 0.0214 - image_quality_output_loss: 0.1164 - age_output_loss: 0.2700 - weight_output_loss: 0.2256 - bag_output_loss: 0.1896 - footwear_output_loss: 0.0612 - pose_output_loss: 0.0325 - emotion_output_loss: 0.2121 - gender_output_acc: 0.9917 - image_quality_output_acc: 0.9561 - age_output_acc: 0.8929 - weight_output_acc: 0.9006 - bag_output_acc: 0.8935 - footwear_output_acc: 0.9772 - pose_output_acc: 0.9892 - emotion_output_acc: 0.9150 - val_loss: 15.5958 - val_gender_output_loss: 0.8815 - val_image_quality_output_loss: 2.5981 - val_age_output_loss: 2.8516 - val_weight_output_loss: 2.2114 - val_bag_output_loss: 1.7743 - val_footwear_output_loss: 2.1939 - val_pose_output_loss: 1.1130 - val_emotion_output_loss: 1.9720 - val_gender_output_acc: 0.8338 - val_image_quality_output_acc: 0.5367 - val_age_output_acc: 0.3800 - val_weight_output_acc: 0.6012 - val_bag_output_acc: 0.6270 - val_footwear_output_acc: 0.6295 - val_pose_output_acc: 0.8170 - val_emotion_output_acc: 0.6156\n",
      "\n",
      "Epoch 00108: loss did not improve from 0.81259\n",
      "Epoch 109/200\n",
      " - 84s - loss: 0.9825 - gender_output_loss: 0.0155 - image_quality_output_loss: 0.0877 - age_output_loss: 0.2396 - weight_output_loss: 0.2100 - bag_output_loss: 0.1728 - footwear_output_loss: 0.0509 - pose_output_loss: 0.0170 - emotion_output_loss: 0.1890 - gender_output_acc: 0.9944 - image_quality_output_acc: 0.9690 - age_output_acc: 0.9010 - weight_output_acc: 0.9039 - bag_output_acc: 0.8963 - footwear_output_acc: 0.9820 - pose_output_acc: 0.9946 - emotion_output_acc: 0.9185 - val_loss: 15.5811 - val_gender_output_loss: 0.9661 - val_image_quality_output_loss: 3.1254 - val_age_output_loss: 2.7258 - val_weight_output_loss: 1.8203 - val_bag_output_loss: 1.8397 - val_footwear_output_loss: 2.0990 - val_pose_output_loss: 1.1971 - val_emotion_output_loss: 1.8077 - val_gender_output_acc: 0.8333 - val_image_quality_output_acc: 0.5238 - val_age_output_acc: 0.3686 - val_weight_output_acc: 0.5501 - val_bag_output_acc: 0.6066 - val_footwear_output_acc: 0.6329 - val_pose_output_acc: 0.7996 - val_emotion_output_acc: 0.5531\n",
      "\n",
      "Epoch 00109: loss did not improve from 0.81259\n",
      "Epoch 110/200\n",
      " - 84s - loss: 0.8743 - gender_output_loss: 0.0134 - image_quality_output_loss: 0.0706 - age_output_loss: 0.2143 - weight_output_loss: 0.1931 - bag_output_loss: 0.1667 - footwear_output_loss: 0.0364 - pose_output_loss: 0.0161 - emotion_output_loss: 0.1637 - gender_output_acc: 0.9956 - image_quality_output_acc: 0.9769 - age_output_acc: 0.9115 - weight_output_acc: 0.9112 - bag_output_acc: 0.8993 - footwear_output_acc: 0.9882 - pose_output_acc: 0.9944 - emotion_output_acc: 0.9262 - val_loss: 16.3928 - val_gender_output_loss: 0.9933 - val_image_quality_output_loss: 2.6753 - val_age_output_loss: 2.8325 - val_weight_output_loss: 2.3479 - val_bag_output_loss: 1.8403 - val_footwear_output_loss: 2.1984 - val_pose_output_loss: 1.1907 - val_emotion_output_loss: 2.3144 - val_gender_output_acc: 0.8294 - val_image_quality_output_acc: 0.5342 - val_age_output_acc: 0.3829 - val_weight_output_acc: 0.5987 - val_bag_output_acc: 0.6076 - val_footwear_output_acc: 0.6195 - val_pose_output_acc: 0.8194 - val_emotion_output_acc: 0.6424\n",
      "\n",
      "Epoch 00110: loss did not improve from 0.81259\n",
      "Epoch 111/200\n",
      " - 84s - loss: 1.0856 - gender_output_loss: 0.0243 - image_quality_output_loss: 0.1001 - age_output_loss: 0.2685 - weight_output_loss: 0.2135 - bag_output_loss: 0.1843 - footwear_output_loss: 0.0587 - pose_output_loss: 0.0297 - emotion_output_loss: 0.2064 - gender_output_acc: 0.9911 - image_quality_output_acc: 0.9615 - age_output_acc: 0.8951 - weight_output_acc: 0.9064 - bag_output_acc: 0.8919 - footwear_output_acc: 0.9798 - pose_output_acc: 0.9906 - emotion_output_acc: 0.9148 - val_loss: 16.6636 - val_gender_output_loss: 0.8919 - val_image_quality_output_loss: 2.7537 - val_age_output_loss: 2.8913 - val_weight_output_loss: 2.5013 - val_bag_output_loss: 2.1160 - val_footwear_output_loss: 2.2188 - val_pose_output_loss: 1.1578 - val_emotion_output_loss: 2.1329 - val_gender_output_acc: 0.8373 - val_image_quality_output_acc: 0.5060 - val_age_output_acc: 0.3760 - val_weight_output_acc: 0.6062 - val_bag_output_acc: 0.6285 - val_footwear_output_acc: 0.6210 - val_pose_output_acc: 0.8065 - val_emotion_output_acc: 0.5913\n",
      "\n",
      "Epoch 00111: loss did not improve from 0.81259\n",
      "Epoch 112/200\n",
      " - 84s - loss: 0.7741 - gender_output_loss: 0.0068 - image_quality_output_loss: 0.0545 - age_output_loss: 0.1893 - weight_output_loss: 0.1775 - bag_output_loss: 0.1507 - footwear_output_loss: 0.0268 - pose_output_loss: 0.0111 - emotion_output_loss: 0.1573 - gender_output_acc: 0.9980 - image_quality_output_acc: 0.9814 - age_output_acc: 0.9179 - weight_output_acc: 0.9144 - bag_output_acc: 0.9004 - footwear_output_acc: 0.9918 - pose_output_acc: 0.9966 - emotion_output_acc: 0.9296 - val_loss: 16.5065 - val_gender_output_loss: 0.9997 - val_image_quality_output_loss: 2.8385 - val_age_output_loss: 2.9968 - val_weight_output_loss: 2.2355 - val_bag_output_loss: 1.9179 - val_footwear_output_loss: 2.3749 - val_pose_output_loss: 1.3260 - val_emotion_output_loss: 1.8173 - val_gender_output_acc: 0.8264 - val_image_quality_output_acc: 0.5387 - val_age_output_acc: 0.3770 - val_weight_output_acc: 0.5967 - val_bag_output_acc: 0.6200 - val_footwear_output_acc: 0.6329 - val_pose_output_acc: 0.7951 - val_emotion_output_acc: 0.4936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00112: loss improved from 0.81259 to 0.77407, saving model to model.h5\n",
      "Epoch 113/200\n",
      " - 84s - loss: 1.0604 - gender_output_loss: 0.0217 - image_quality_output_loss: 0.0964 - age_output_loss: 0.2599 - weight_output_loss: 0.2162 - bag_output_loss: 0.1766 - footwear_output_loss: 0.0609 - pose_output_loss: 0.0287 - emotion_output_loss: 0.2001 - gender_output_acc: 0.9920 - image_quality_output_acc: 0.9643 - age_output_acc: 0.8980 - weight_output_acc: 0.9069 - bag_output_acc: 0.8961 - footwear_output_acc: 0.9793 - pose_output_acc: 0.9917 - emotion_output_acc: 0.9179 - val_loss: 15.9093 - val_gender_output_loss: 0.8806 - val_image_quality_output_loss: 2.7400 - val_age_output_loss: 2.7997 - val_weight_output_loss: 2.3044 - val_bag_output_loss: 1.8260 - val_footwear_output_loss: 2.2052 - val_pose_output_loss: 1.0917 - val_emotion_output_loss: 2.0616 - val_gender_output_acc: 0.8428 - val_image_quality_output_acc: 0.5377 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.5997 - val_bag_output_acc: 0.6245 - val_footwear_output_acc: 0.6220 - val_pose_output_acc: 0.8229 - val_emotion_output_acc: 0.6235\n",
      "\n",
      "Epoch 00113: loss did not improve from 0.77407\n",
      "Epoch 114/200\n",
      " - 84s - loss: 0.9893 - gender_output_loss: 0.0160 - image_quality_output_loss: 0.0833 - age_output_loss: 0.2503 - weight_output_loss: 0.2062 - bag_output_loss: 0.1731 - footwear_output_loss: 0.0514 - pose_output_loss: 0.0216 - emotion_output_loss: 0.1874 - gender_output_acc: 0.9945 - image_quality_output_acc: 0.9707 - age_output_acc: 0.8982 - weight_output_acc: 0.9080 - bag_output_acc: 0.8948 - footwear_output_acc: 0.9817 - pose_output_acc: 0.9929 - emotion_output_acc: 0.9191 - val_loss: 16.3583 - val_gender_output_loss: 1.1056 - val_image_quality_output_loss: 2.9687 - val_age_output_loss: 2.8961 - val_weight_output_loss: 2.0370 - val_bag_output_loss: 2.1344 - val_footwear_output_loss: 1.9411 - val_pose_output_loss: 1.3159 - val_emotion_output_loss: 1.9595 - val_gender_output_acc: 0.8001 - val_image_quality_output_acc: 0.5357 - val_age_output_acc: 0.3780 - val_weight_output_acc: 0.5308 - val_bag_output_acc: 0.5784 - val_footwear_output_acc: 0.6146 - val_pose_output_acc: 0.7877 - val_emotion_output_acc: 0.5134\n",
      "\n",
      "Epoch 00114: loss did not improve from 0.77407\n",
      "Epoch 115/200\n",
      " - 84s - loss: 0.8459 - gender_output_loss: 0.0131 - image_quality_output_loss: 0.0721 - age_output_loss: 0.2053 - weight_output_loss: 0.1804 - bag_output_loss: 0.1631 - footwear_output_loss: 0.0338 - pose_output_loss: 0.0130 - emotion_output_loss: 0.1651 - gender_output_acc: 0.9950 - image_quality_output_acc: 0.9730 - age_output_acc: 0.9134 - weight_output_acc: 0.9144 - bag_output_acc: 0.8982 - footwear_output_acc: 0.9878 - pose_output_acc: 0.9961 - emotion_output_acc: 0.9266 - val_loss: 17.7867 - val_gender_output_loss: 0.9659 - val_image_quality_output_loss: 3.0039 - val_age_output_loss: 3.1246 - val_weight_output_loss: 2.7664 - val_bag_output_loss: 1.8692 - val_footwear_output_loss: 2.4476 - val_pose_output_loss: 1.1783 - val_emotion_output_loss: 2.4309 - val_gender_output_acc: 0.8363 - val_image_quality_output_acc: 0.5357 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6066 - val_bag_output_acc: 0.6240 - val_footwear_output_acc: 0.6250 - val_pose_output_acc: 0.8204 - val_emotion_output_acc: 0.6488\n",
      "\n",
      "Epoch 00115: loss did not improve from 0.77407\n",
      "Epoch 116/200\n",
      " - 84s - loss: 1.0795 - gender_output_loss: 0.0218 - image_quality_output_loss: 0.1232 - age_output_loss: 0.2544 - weight_output_loss: 0.2184 - bag_output_loss: 0.1818 - footwear_output_loss: 0.0562 - pose_output_loss: 0.0272 - emotion_output_loss: 0.1965 - gender_output_acc: 0.9928 - image_quality_output_acc: 0.9551 - age_output_acc: 0.8958 - weight_output_acc: 0.9009 - bag_output_acc: 0.8937 - footwear_output_acc: 0.9790 - pose_output_acc: 0.9912 - emotion_output_acc: 0.9160 - val_loss: 16.4163 - val_gender_output_loss: 0.9211 - val_image_quality_output_loss: 2.7021 - val_age_output_loss: 2.9224 - val_weight_output_loss: 2.3887 - val_bag_output_loss: 1.8561 - val_footwear_output_loss: 2.4478 - val_pose_output_loss: 1.1048 - val_emotion_output_loss: 2.0733 - val_gender_output_acc: 0.8353 - val_image_quality_output_acc: 0.5218 - val_age_output_acc: 0.3566 - val_weight_output_acc: 0.5823 - val_bag_output_acc: 0.6141 - val_footwear_output_acc: 0.6334 - val_pose_output_acc: 0.8189 - val_emotion_output_acc: 0.6116\n",
      "\n",
      "Epoch 00116: loss did not improve from 0.77407\n",
      "Epoch 117/200\n",
      " - 84s - loss: 0.7204 - gender_output_loss: 0.0107 - image_quality_output_loss: 0.0524 - age_output_loss: 0.1798 - weight_output_loss: 0.1603 - bag_output_loss: 0.1450 - footwear_output_loss: 0.0221 - pose_output_loss: 0.0118 - emotion_output_loss: 0.1382 - gender_output_acc: 0.9970 - image_quality_output_acc: 0.9851 - age_output_acc: 0.9219 - weight_output_acc: 0.9199 - bag_output_acc: 0.9002 - footwear_output_acc: 0.9921 - pose_output_acc: 0.9962 - emotion_output_acc: 0.9332 - val_loss: 17.6336 - val_gender_output_loss: 0.9663 - val_image_quality_output_loss: 3.0929 - val_age_output_loss: 3.1524 - val_weight_output_loss: 2.2118 - val_bag_output_loss: 2.3891 - val_footwear_output_loss: 2.5748 - val_pose_output_loss: 1.3332 - val_emotion_output_loss: 1.9131 - val_gender_output_acc: 0.8373 - val_image_quality_output_acc: 0.5278 - val_age_output_acc: 0.3676 - val_weight_output_acc: 0.5293 - val_bag_output_acc: 0.6235 - val_footwear_output_acc: 0.6364 - val_pose_output_acc: 0.7961 - val_emotion_output_acc: 0.5739\n",
      "\n",
      "Epoch 00117: loss improved from 0.77407 to 0.72038, saving model to model.h5\n",
      "Epoch 118/200\n",
      " - 84s - loss: 0.9853 - gender_output_loss: 0.0215 - image_quality_output_loss: 0.0854 - age_output_loss: 0.2291 - weight_output_loss: 0.2046 - bag_output_loss: 0.1782 - footwear_output_loss: 0.0516 - pose_output_loss: 0.0256 - emotion_output_loss: 0.1893 - gender_output_acc: 0.9932 - image_quality_output_acc: 0.9704 - age_output_acc: 0.9056 - weight_output_acc: 0.9081 - bag_output_acc: 0.8953 - footwear_output_acc: 0.9823 - pose_output_acc: 0.9909 - emotion_output_acc: 0.9172 - val_loss: 16.4963 - val_gender_output_loss: 0.8660 - val_image_quality_output_loss: 2.6947 - val_age_output_loss: 2.9788 - val_weight_output_loss: 2.4664 - val_bag_output_loss: 1.9384 - val_footwear_output_loss: 2.3334 - val_pose_output_loss: 1.1120 - val_emotion_output_loss: 2.1066 - val_gender_output_acc: 0.8393 - val_image_quality_output_acc: 0.5288 - val_age_output_acc: 0.3720 - val_weight_output_acc: 0.5972 - val_bag_output_acc: 0.6344 - val_footwear_output_acc: 0.6265 - val_pose_output_acc: 0.8125 - val_emotion_output_acc: 0.5913\n",
      "\n",
      "Epoch 00118: loss did not improve from 0.72038\n",
      "Epoch 119/200\n",
      " - 84s - loss: 0.9385 - gender_output_loss: 0.0151 - image_quality_output_loss: 0.0944 - age_output_loss: 0.2267 - weight_output_loss: 0.1947 - bag_output_loss: 0.1689 - footwear_output_loss: 0.0474 - pose_output_loss: 0.0206 - emotion_output_loss: 0.1707 - gender_output_acc: 0.9946 - image_quality_output_acc: 0.9657 - age_output_acc: 0.9051 - weight_output_acc: 0.9098 - bag_output_acc: 0.8957 - footwear_output_acc: 0.9840 - pose_output_acc: 0.9937 - emotion_output_acc: 0.9229 - val_loss: 16.2064 - val_gender_output_loss: 1.0494 - val_image_quality_output_loss: 2.7457 - val_age_output_loss: 2.8680 - val_weight_output_loss: 1.8371 - val_bag_output_loss: 2.1498 - val_footwear_output_loss: 2.3626 - val_pose_output_loss: 1.1836 - val_emotion_output_loss: 2.0102 - val_gender_output_acc: 0.8180 - val_image_quality_output_acc: 0.5253 - val_age_output_acc: 0.3562 - val_weight_output_acc: 0.5471 - val_bag_output_acc: 0.5799 - val_footwear_output_acc: 0.6230 - val_pose_output_acc: 0.8061 - val_emotion_output_acc: 0.6200\n",
      "\n",
      "Epoch 00119: loss did not improve from 0.72038\n",
      "Epoch 120/200\n",
      " - 84s - loss: 0.7959 - gender_output_loss: 0.0116 - image_quality_output_loss: 0.0694 - age_output_loss: 0.1936 - weight_output_loss: 0.1696 - bag_output_loss: 0.1534 - footwear_output_loss: 0.0302 - pose_output_loss: 0.0122 - emotion_output_loss: 0.1559 - gender_output_acc: 0.9957 - image_quality_output_acc: 0.9767 - age_output_acc: 0.9192 - weight_output_acc: 0.9176 - bag_output_acc: 0.8989 - footwear_output_acc: 0.9896 - pose_output_acc: 0.9962 - emotion_output_acc: 0.9288 - val_loss: 17.3107 - val_gender_output_loss: 0.9031 - val_image_quality_output_loss: 3.2907 - val_age_output_loss: 2.9862 - val_weight_output_loss: 2.2413 - val_bag_output_loss: 2.2677 - val_footwear_output_loss: 2.4439 - val_pose_output_loss: 1.0860 - val_emotion_output_loss: 2.0919 - val_gender_output_acc: 0.8428 - val_image_quality_output_acc: 0.5184 - val_age_output_acc: 0.3492 - val_weight_output_acc: 0.5898 - val_bag_output_acc: 0.6260 - val_footwear_output_acc: 0.6493 - val_pose_output_acc: 0.8165 - val_emotion_output_acc: 0.6131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00120: loss did not improve from 0.72038\n",
      "Epoch 121/200\n",
      " - 84s - loss: 0.9156 - gender_output_loss: 0.0139 - image_quality_output_loss: 0.0895 - age_output_loss: 0.2205 - weight_output_loss: 0.1821 - bag_output_loss: 0.1652 - footwear_output_loss: 0.0460 - pose_output_loss: 0.0196 - emotion_output_loss: 0.1788 - gender_output_acc: 0.9951 - image_quality_output_acc: 0.9671 - age_output_acc: 0.9084 - weight_output_acc: 0.9107 - bag_output_acc: 0.8962 - footwear_output_acc: 0.9857 - pose_output_acc: 0.9939 - emotion_output_acc: 0.9199 - val_loss: 16.7841 - val_gender_output_loss: 0.9332 - val_image_quality_output_loss: 2.8066 - val_age_output_loss: 2.9571 - val_weight_output_loss: 2.4822 - val_bag_output_loss: 1.9638 - val_footwear_output_loss: 2.3152 - val_pose_output_loss: 1.2172 - val_emotion_output_loss: 2.1088 - val_gender_output_acc: 0.8438 - val_image_quality_output_acc: 0.5283 - val_age_output_acc: 0.3676 - val_weight_output_acc: 0.5893 - val_bag_output_acc: 0.6310 - val_footwear_output_acc: 0.6181 - val_pose_output_acc: 0.8180 - val_emotion_output_acc: 0.5630\n",
      "\n",
      "Epoch 00121: loss did not improve from 0.72038\n",
      "Epoch 122/200\n",
      " - 84s - loss: 0.7322 - gender_output_loss: 0.0105 - image_quality_output_loss: 0.0631 - age_output_loss: 0.1769 - weight_output_loss: 0.1580 - bag_output_loss: 0.1465 - footwear_output_loss: 0.0269 - pose_output_loss: 0.0103 - emotion_output_loss: 0.1400 - gender_output_acc: 0.9964 - image_quality_output_acc: 0.9780 - age_output_acc: 0.9234 - weight_output_acc: 0.9202 - bag_output_acc: 0.8999 - footwear_output_acc: 0.9912 - pose_output_acc: 0.9966 - emotion_output_acc: 0.9313 - val_loss: 18.7748 - val_gender_output_loss: 1.0371 - val_image_quality_output_loss: 3.4626 - val_age_output_loss: 3.0792 - val_weight_output_loss: 2.2732 - val_bag_output_loss: 2.7367 - val_footwear_output_loss: 2.5528 - val_pose_output_loss: 1.3154 - val_emotion_output_loss: 2.3179 - val_gender_output_acc: 0.8368 - val_image_quality_output_acc: 0.4911 - val_age_output_acc: 0.3348 - val_weight_output_acc: 0.5769 - val_bag_output_acc: 0.6121 - val_footwear_output_acc: 0.6200 - val_pose_output_acc: 0.8006 - val_emotion_output_acc: 0.5362\n",
      "\n",
      "Epoch 00122: loss did not improve from 0.72038\n",
      "Epoch 123/200\n",
      " - 84s - loss: 0.9558 - gender_output_loss: 0.0163 - image_quality_output_loss: 0.0932 - age_output_loss: 0.2281 - weight_output_loss: 0.2016 - bag_output_loss: 0.1700 - footwear_output_loss: 0.0475 - pose_output_loss: 0.0192 - emotion_output_loss: 0.1801 - gender_output_acc: 0.9947 - image_quality_output_acc: 0.9661 - age_output_acc: 0.9061 - weight_output_acc: 0.9089 - bag_output_acc: 0.8961 - footwear_output_acc: 0.9835 - pose_output_acc: 0.9931 - emotion_output_acc: 0.9199 - val_loss: 16.7447 - val_gender_output_loss: 0.9348 - val_image_quality_output_loss: 2.6868 - val_age_output_loss: 2.9461 - val_weight_output_loss: 2.4535 - val_bag_output_loss: 1.9991 - val_footwear_output_loss: 2.3261 - val_pose_output_loss: 1.1907 - val_emotion_output_loss: 2.2076 - val_gender_output_acc: 0.8353 - val_image_quality_output_acc: 0.5303 - val_age_output_acc: 0.3864 - val_weight_output_acc: 0.6012 - val_bag_output_acc: 0.6250 - val_footwear_output_acc: 0.6245 - val_pose_output_acc: 0.8165 - val_emotion_output_acc: 0.6096\n",
      "\n",
      "Epoch 00123: loss did not improve from 0.72038\n",
      "Epoch 124/200\n",
      " - 84s - loss: 0.8682 - gender_output_loss: 0.0174 - image_quality_output_loss: 0.0759 - age_output_loss: 0.2064 - weight_output_loss: 0.1842 - bag_output_loss: 0.1664 - footwear_output_loss: 0.0407 - pose_output_loss: 0.0194 - emotion_output_loss: 0.1577 - gender_output_acc: 0.9942 - image_quality_output_acc: 0.9734 - age_output_acc: 0.9135 - weight_output_acc: 0.9117 - bag_output_acc: 0.8949 - footwear_output_acc: 0.9864 - pose_output_acc: 0.9934 - emotion_output_acc: 0.9267 - val_loss: 16.1037 - val_gender_output_loss: 0.8563 - val_image_quality_output_loss: 2.7681 - val_age_output_loss: 2.7612 - val_weight_output_loss: 2.2965 - val_bag_output_loss: 1.6656 - val_footwear_output_loss: 2.1540 - val_pose_output_loss: 1.2469 - val_emotion_output_loss: 2.3551 - val_gender_output_acc: 0.8348 - val_image_quality_output_acc: 0.4906 - val_age_output_acc: 0.3566 - val_weight_output_acc: 0.5947 - val_bag_output_acc: 0.5967 - val_footwear_output_acc: 0.6176 - val_pose_output_acc: 0.8080 - val_emotion_output_acc: 0.6453\n",
      "\n",
      "Epoch 00124: loss did not improve from 0.72038\n",
      "Epoch 125/200\n",
      " - 84s - loss: 0.8101 - gender_output_loss: 0.0143 - image_quality_output_loss: 0.0753 - age_output_loss: 0.2023 - weight_output_loss: 0.1682 - bag_output_loss: 0.1563 - footwear_output_loss: 0.0313 - pose_output_loss: 0.0156 - emotion_output_loss: 0.1469 - gender_output_acc: 0.9959 - image_quality_output_acc: 0.9740 - age_output_acc: 0.9137 - weight_output_acc: 0.9181 - bag_output_acc: 0.8984 - footwear_output_acc: 0.9899 - pose_output_acc: 0.9947 - emotion_output_acc: 0.9286 - val_loss: 16.9391 - val_gender_output_loss: 0.8989 - val_image_quality_output_loss: 2.8192 - val_age_output_loss: 3.0843 - val_weight_output_loss: 2.2488 - val_bag_output_loss: 2.1224 - val_footwear_output_loss: 2.3032 - val_pose_output_loss: 1.2101 - val_emotion_output_loss: 2.2522 - val_gender_output_acc: 0.8373 - val_image_quality_output_acc: 0.5417 - val_age_output_acc: 0.3844 - val_weight_output_acc: 0.5938 - val_bag_output_acc: 0.5893 - val_footwear_output_acc: 0.6270 - val_pose_output_acc: 0.8160 - val_emotion_output_acc: 0.5913\n",
      "\n",
      "Epoch 00125: loss did not improve from 0.72038\n",
      "Epoch 126/200\n",
      " - 84s - loss: 0.8808 - gender_output_loss: 0.0161 - image_quality_output_loss: 0.0809 - age_output_loss: 0.2126 - weight_output_loss: 0.1783 - bag_output_loss: 0.1654 - footwear_output_loss: 0.0457 - pose_output_loss: 0.0220 - emotion_output_loss: 0.1597 - gender_output_acc: 0.9940 - image_quality_output_acc: 0.9720 - age_output_acc: 0.9102 - weight_output_acc: 0.9127 - bag_output_acc: 0.8956 - footwear_output_acc: 0.9837 - pose_output_acc: 0.9912 - emotion_output_acc: 0.9257 - val_loss: 16.5336 - val_gender_output_loss: 0.8486 - val_image_quality_output_loss: 2.7926 - val_age_output_loss: 2.9407 - val_weight_output_loss: 2.2586 - val_bag_output_loss: 1.9965 - val_footwear_output_loss: 2.3104 - val_pose_output_loss: 1.1609 - val_emotion_output_loss: 2.2252 - val_gender_output_acc: 0.8482 - val_image_quality_output_acc: 0.5332 - val_age_output_acc: 0.3705 - val_weight_output_acc: 0.6022 - val_bag_output_acc: 0.6121 - val_footwear_output_acc: 0.6141 - val_pose_output_acc: 0.8110 - val_emotion_output_acc: 0.6161\n",
      "\n",
      "Epoch 00126: loss did not improve from 0.72038\n",
      "Epoch 127/200\n",
      " - 84s - loss: 0.6863 - gender_output_loss: 0.0080 - image_quality_output_loss: 0.0513 - age_output_loss: 0.1688 - weight_output_loss: 0.1504 - bag_output_loss: 0.1393 - footwear_output_loss: 0.0212 - pose_output_loss: 0.0122 - emotion_output_loss: 0.1349 - gender_output_acc: 0.9977 - image_quality_output_acc: 0.9842 - age_output_acc: 0.9245 - weight_output_acc: 0.9207 - bag_output_acc: 0.9008 - footwear_output_acc: 0.9930 - pose_output_acc: 0.9964 - emotion_output_acc: 0.9312 - val_loss: 19.0181 - val_gender_output_loss: 1.0168 - val_image_quality_output_loss: 4.1842 - val_age_output_loss: 3.3085 - val_weight_output_loss: 2.0115 - val_bag_output_loss: 2.4326 - val_footwear_output_loss: 2.4790 - val_pose_output_loss: 1.3683 - val_emotion_output_loss: 2.2172 - val_gender_output_acc: 0.8274 - val_image_quality_output_acc: 0.4524 - val_age_output_acc: 0.3150 - val_weight_output_acc: 0.5704 - val_bag_output_acc: 0.6314 - val_footwear_output_acc: 0.6007 - val_pose_output_acc: 0.8065 - val_emotion_output_acc: 0.6275\n",
      "\n",
      "Epoch 00127: loss improved from 0.72038 to 0.68632, saving model to model.h5\n",
      "Epoch 128/200\n",
      " - 84s - loss: 0.9913 - gender_output_loss: 0.0163 - image_quality_output_loss: 0.1067 - age_output_loss: 0.2412 - weight_output_loss: 0.1961 - bag_output_loss: 0.1755 - footwear_output_loss: 0.0536 - pose_output_loss: 0.0241 - emotion_output_loss: 0.1779 - gender_output_acc: 0.9946 - image_quality_output_acc: 0.9620 - age_output_acc: 0.9036 - weight_output_acc: 0.9090 - bag_output_acc: 0.8950 - footwear_output_acc: 0.9807 - pose_output_acc: 0.9922 - emotion_output_acc: 0.9220 - val_loss: 16.5324 - val_gender_output_loss: 0.9114 - val_image_quality_output_loss: 2.6121 - val_age_output_loss: 2.8864 - val_weight_output_loss: 2.4157 - val_bag_output_loss: 1.9930 - val_footwear_output_loss: 2.2497 - val_pose_output_loss: 1.1682 - val_emotion_output_loss: 2.2959 - val_gender_output_acc: 0.8433 - val_image_quality_output_acc: 0.5283 - val_age_output_acc: 0.3745 - val_weight_output_acc: 0.6042 - val_bag_output_acc: 0.6379 - val_footwear_output_acc: 0.6205 - val_pose_output_acc: 0.8145 - val_emotion_output_acc: 0.6225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00128: loss did not improve from 0.68632\n",
      "Epoch 129/200\n",
      " - 84s - loss: 0.8266 - gender_output_loss: 0.0135 - image_quality_output_loss: 0.0745 - age_output_loss: 0.2041 - weight_output_loss: 0.1688 - bag_output_loss: 0.1535 - footwear_output_loss: 0.0366 - pose_output_loss: 0.0146 - emotion_output_loss: 0.1610 - gender_output_acc: 0.9953 - image_quality_output_acc: 0.9749 - age_output_acc: 0.9164 - weight_output_acc: 0.9149 - bag_output_acc: 0.8984 - footwear_output_acc: 0.9878 - pose_output_acc: 0.9948 - emotion_output_acc: 0.9247 - val_loss: 16.9669 - val_gender_output_loss: 0.7980 - val_image_quality_output_loss: 2.8899 - val_age_output_loss: 2.7281 - val_weight_output_loss: 2.2524 - val_bag_output_loss: 2.3382 - val_footwear_output_loss: 2.4432 - val_pose_output_loss: 1.3459 - val_emotion_output_loss: 2.1712 - val_gender_output_acc: 0.8403 - val_image_quality_output_acc: 0.4826 - val_age_output_acc: 0.3725 - val_weight_output_acc: 0.5392 - val_bag_output_acc: 0.6062 - val_footwear_output_acc: 0.6270 - val_pose_output_acc: 0.7946 - val_emotion_output_acc: 0.5873\n",
      "\n",
      "Epoch 00129: loss did not improve from 0.68632\n",
      "Epoch 130/200\n",
      " - 84s - loss: 0.7289 - gender_output_loss: 0.0111 - image_quality_output_loss: 0.0510 - age_output_loss: 0.1768 - weight_output_loss: 0.1595 - bag_output_loss: 0.1456 - footwear_output_loss: 0.0291 - pose_output_loss: 0.0136 - emotion_output_loss: 0.1421 - gender_output_acc: 0.9964 - image_quality_output_acc: 0.9836 - age_output_acc: 0.9253 - weight_output_acc: 0.9179 - bag_output_acc: 0.8990 - footwear_output_acc: 0.9898 - pose_output_acc: 0.9955 - emotion_output_acc: 0.9297 - val_loss: 18.0855 - val_gender_output_loss: 0.9802 - val_image_quality_output_loss: 2.8159 - val_age_output_loss: 3.2267 - val_weight_output_loss: 2.5930 - val_bag_output_loss: 2.1662 - val_footwear_output_loss: 2.6175 - val_pose_output_loss: 1.3248 - val_emotion_output_loss: 2.3614 - val_gender_output_acc: 0.8393 - val_image_quality_output_acc: 0.5377 - val_age_output_acc: 0.3849 - val_weight_output_acc: 0.5992 - val_bag_output_acc: 0.6300 - val_footwear_output_acc: 0.6419 - val_pose_output_acc: 0.8115 - val_emotion_output_acc: 0.6205\n",
      "\n",
      "Epoch 00130: loss did not improve from 0.68632\n",
      "Epoch 131/200\n",
      " - 84s - loss: 0.8978 - gender_output_loss: 0.0184 - image_quality_output_loss: 0.0900 - age_output_loss: 0.2123 - weight_output_loss: 0.1811 - bag_output_loss: 0.1547 - footwear_output_loss: 0.0445 - pose_output_loss: 0.0217 - emotion_output_loss: 0.1751 - gender_output_acc: 0.9940 - image_quality_output_acc: 0.9701 - age_output_acc: 0.9118 - weight_output_acc: 0.9135 - bag_output_acc: 0.8965 - footwear_output_acc: 0.9843 - pose_output_acc: 0.9929 - emotion_output_acc: 0.9217 - val_loss: 17.3280 - val_gender_output_loss: 0.8713 - val_image_quality_output_loss: 3.2022 - val_age_output_loss: 3.0873 - val_weight_output_loss: 2.5708 - val_bag_output_loss: 2.0028 - val_footwear_output_loss: 2.3689 - val_pose_output_loss: 1.2547 - val_emotion_output_loss: 1.9700 - val_gender_output_acc: 0.8393 - val_image_quality_output_acc: 0.5134 - val_age_output_acc: 0.3720 - val_weight_output_acc: 0.6190 - val_bag_output_acc: 0.6181 - val_footwear_output_acc: 0.6230 - val_pose_output_acc: 0.8100 - val_emotion_output_acc: 0.5809\n",
      "\n",
      "Epoch 00131: loss did not improve from 0.68632\n",
      "Epoch 132/200\n",
      " - 84s - loss: 0.6580 - gender_output_loss: 0.0076 - image_quality_output_loss: 0.0556 - age_output_loss: 0.1593 - weight_output_loss: 0.1413 - bag_output_loss: 0.1384 - footwear_output_loss: 0.0195 - pose_output_loss: 0.0102 - emotion_output_loss: 0.1261 - gender_output_acc: 0.9972 - image_quality_output_acc: 0.9800 - age_output_acc: 0.9266 - weight_output_acc: 0.9234 - bag_output_acc: 0.9000 - footwear_output_acc: 0.9937 - pose_output_acc: 0.9965 - emotion_output_acc: 0.9343 - val_loss: 17.6407 - val_gender_output_loss: 0.9090 - val_image_quality_output_loss: 2.9017 - val_age_output_loss: 3.2651 - val_weight_output_loss: 2.4351 - val_bag_output_loss: 2.1532 - val_footwear_output_loss: 2.6823 - val_pose_output_loss: 1.2189 - val_emotion_output_loss: 2.0754 - val_gender_output_acc: 0.8462 - val_image_quality_output_acc: 0.5357 - val_age_output_acc: 0.3924 - val_weight_output_acc: 0.6121 - val_bag_output_acc: 0.6230 - val_footwear_output_acc: 0.6359 - val_pose_output_acc: 0.8199 - val_emotion_output_acc: 0.6354\n",
      "\n",
      "Epoch 00132: loss improved from 0.68632 to 0.65796, saving model to model.h5\n",
      "Epoch 133/200\n",
      " - 86s - loss: 0.8816 - gender_output_loss: 0.0141 - image_quality_output_loss: 0.0768 - age_output_loss: 0.2095 - weight_output_loss: 0.1846 - bag_output_loss: 0.1616 - footwear_output_loss: 0.0517 - pose_output_loss: 0.0191 - emotion_output_loss: 0.1642 - gender_output_acc: 0.9957 - image_quality_output_acc: 0.9744 - age_output_acc: 0.9122 - weight_output_acc: 0.9117 - bag_output_acc: 0.8962 - footwear_output_acc: 0.9826 - pose_output_acc: 0.9934 - emotion_output_acc: 0.9247 - val_loss: 16.9256 - val_gender_output_loss: 0.9115 - val_image_quality_output_loss: 2.7113 - val_age_output_loss: 2.9616 - val_weight_output_loss: 2.4296 - val_bag_output_loss: 2.0709 - val_footwear_output_loss: 2.2793 - val_pose_output_loss: 1.2662 - val_emotion_output_loss: 2.2953 - val_gender_output_acc: 0.8403 - val_image_quality_output_acc: 0.5407 - val_age_output_acc: 0.3839 - val_weight_output_acc: 0.6052 - val_bag_output_acc: 0.6186 - val_footwear_output_acc: 0.6319 - val_pose_output_acc: 0.8165 - val_emotion_output_acc: 0.6240\n",
      "\n",
      "Epoch 00133: loss did not improve from 0.65796\n",
      "Epoch 134/200\n",
      " - 87s - loss: 0.7887 - gender_output_loss: 0.0149 - image_quality_output_loss: 0.0648 - age_output_loss: 0.1945 - weight_output_loss: 0.1622 - bag_output_loss: 0.1486 - footwear_output_loss: 0.0366 - pose_output_loss: 0.0150 - emotion_output_loss: 0.1521 - gender_output_acc: 0.9950 - image_quality_output_acc: 0.9780 - age_output_acc: 0.9172 - weight_output_acc: 0.9177 - bag_output_acc: 0.8987 - footwear_output_acc: 0.9881 - pose_output_acc: 0.9954 - emotion_output_acc: 0.9253 - val_loss: 17.0216 - val_gender_output_loss: 0.9123 - val_image_quality_output_loss: 3.0649 - val_age_output_loss: 2.8496 - val_weight_output_loss: 2.1159 - val_bag_output_loss: 1.7668 - val_footwear_output_loss: 2.2130 - val_pose_output_loss: 1.3354 - val_emotion_output_loss: 2.7637 - val_gender_output_acc: 0.8363 - val_image_quality_output_acc: 0.4921 - val_age_output_acc: 0.3601 - val_weight_output_acc: 0.5823 - val_bag_output_acc: 0.6166 - val_footwear_output_acc: 0.6062 - val_pose_output_acc: 0.8080 - val_emotion_output_acc: 0.6766\n",
      "\n",
      "Epoch 00134: loss did not improve from 0.65796\n",
      "Epoch 135/200\n",
      " - 86s - loss: 0.7154 - gender_output_loss: 0.0098 - image_quality_output_loss: 0.0520 - age_output_loss: 0.1776 - weight_output_loss: 0.1563 - bag_output_loss: 0.1435 - footwear_output_loss: 0.0284 - pose_output_loss: 0.0117 - emotion_output_loss: 0.1361 - gender_output_acc: 0.9966 - image_quality_output_acc: 0.9825 - age_output_acc: 0.9220 - weight_output_acc: 0.9196 - bag_output_acc: 0.9003 - footwear_output_acc: 0.9903 - pose_output_acc: 0.9951 - emotion_output_acc: 0.9316 - val_loss: 18.0084 - val_gender_output_loss: 0.8919 - val_image_quality_output_loss: 3.1573 - val_age_output_loss: 3.1871 - val_weight_output_loss: 2.4631 - val_bag_output_loss: 2.3045 - val_footwear_output_loss: 2.4165 - val_pose_output_loss: 1.2643 - val_emotion_output_loss: 2.3237 - val_gender_output_acc: 0.8348 - val_image_quality_output_acc: 0.5317 - val_age_output_acc: 0.3616 - val_weight_output_acc: 0.5997 - val_bag_output_acc: 0.6265 - val_footwear_output_acc: 0.6389 - val_pose_output_acc: 0.8229 - val_emotion_output_acc: 0.6488\n",
      "\n",
      "Epoch 00135: loss did not improve from 0.65796\n",
      "Epoch 136/200\n",
      " - 84s - loss: 0.7942 - gender_output_loss: 0.0112 - image_quality_output_loss: 0.0704 - age_output_loss: 0.1929 - weight_output_loss: 0.1640 - bag_output_loss: 0.1504 - footwear_output_loss: 0.0377 - pose_output_loss: 0.0217 - emotion_output_loss: 0.1461 - gender_output_acc: 0.9957 - image_quality_output_acc: 0.9745 - age_output_acc: 0.9148 - weight_output_acc: 0.9171 - bag_output_acc: 0.8974 - footwear_output_acc: 0.9870 - pose_output_acc: 0.9930 - emotion_output_acc: 0.9279 - val_loss: 18.1578 - val_gender_output_loss: 0.9712 - val_image_quality_output_loss: 2.9454 - val_age_output_loss: 3.1906 - val_weight_output_loss: 2.7409 - val_bag_output_loss: 2.0931 - val_footwear_output_loss: 2.5494 - val_pose_output_loss: 1.2985 - val_emotion_output_loss: 2.3686 - val_gender_output_acc: 0.8294 - val_image_quality_output_acc: 0.5293 - val_age_output_acc: 0.3859 - val_weight_output_acc: 0.5848 - val_bag_output_acc: 0.6225 - val_footwear_output_acc: 0.6285 - val_pose_output_acc: 0.8085 - val_emotion_output_acc: 0.6161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00136: loss did not improve from 0.65796\n",
      "Epoch 137/200\n",
      " - 84s - loss: 0.6272 - gender_output_loss: 0.0078 - image_quality_output_loss: 0.0474 - age_output_loss: 0.1544 - weight_output_loss: 0.1402 - bag_output_loss: 0.1315 - footwear_output_loss: 0.0151 - pose_output_loss: 0.0075 - emotion_output_loss: 0.1233 - gender_output_acc: 0.9976 - image_quality_output_acc: 0.9834 - age_output_acc: 0.9299 - weight_output_acc: 0.9228 - bag_output_acc: 0.9015 - footwear_output_acc: 0.9943 - pose_output_acc: 0.9978 - emotion_output_acc: 0.9333 - val_loss: 22.6196 - val_gender_output_loss: 1.2156 - val_image_quality_output_loss: 4.5439 - val_age_output_loss: 4.1664 - val_weight_output_loss: 3.8133 - val_bag_output_loss: 2.4122 - val_footwear_output_loss: 2.7156 - val_pose_output_loss: 1.5382 - val_emotion_output_loss: 2.2145 - val_gender_output_acc: 0.8234 - val_image_quality_output_acc: 0.4846 - val_age_output_acc: 0.3413 - val_weight_output_acc: 0.6359 - val_bag_output_acc: 0.6126 - val_footwear_output_acc: 0.6344 - val_pose_output_acc: 0.8041 - val_emotion_output_acc: 0.5377\n",
      "\n",
      "Epoch 00137: loss improved from 0.65796 to 0.62717, saving model to model.h5\n",
      "Epoch 138/200\n",
      " - 84s - loss: 0.9178 - gender_output_loss: 0.0171 - image_quality_output_loss: 0.1003 - age_output_loss: 0.2151 - weight_output_loss: 0.1874 - bag_output_loss: 0.1636 - footwear_output_loss: 0.0529 - pose_output_loss: 0.0190 - emotion_output_loss: 0.1623 - gender_output_acc: 0.9944 - image_quality_output_acc: 0.9631 - age_output_acc: 0.9099 - weight_output_acc: 0.9115 - bag_output_acc: 0.8965 - footwear_output_acc: 0.9818 - pose_output_acc: 0.9939 - emotion_output_acc: 0.9235 - val_loss: 16.9071 - val_gender_output_loss: 0.9330 - val_image_quality_output_loss: 2.5837 - val_age_output_loss: 2.9410 - val_weight_output_loss: 2.5199 - val_bag_output_loss: 2.1002 - val_footwear_output_loss: 2.3408 - val_pose_output_loss: 1.2603 - val_emotion_output_loss: 2.2284 - val_gender_output_acc: 0.8447 - val_image_quality_output_acc: 0.5352 - val_age_output_acc: 0.3755 - val_weight_output_acc: 0.5957 - val_bag_output_acc: 0.6111 - val_footwear_output_acc: 0.6270 - val_pose_output_acc: 0.8269 - val_emotion_output_acc: 0.6146\n",
      "\n",
      "Epoch 00138: loss did not improve from 0.62717\n",
      "Epoch 139/200\n",
      " - 85s - loss: 0.7899 - gender_output_loss: 0.0140 - image_quality_output_loss: 0.0722 - age_output_loss: 0.1888 - weight_output_loss: 0.1585 - bag_output_loss: 0.1537 - footwear_output_loss: 0.0435 - pose_output_loss: 0.0176 - emotion_output_loss: 0.1415 - gender_output_acc: 0.9955 - image_quality_output_acc: 0.9738 - age_output_acc: 0.9198 - weight_output_acc: 0.9186 - bag_output_acc: 0.8975 - footwear_output_acc: 0.9848 - pose_output_acc: 0.9941 - emotion_output_acc: 0.9286 - val_loss: 17.3501 - val_gender_output_loss: 0.9475 - val_image_quality_output_loss: 3.0082 - val_age_output_loss: 2.7557 - val_weight_output_loss: 2.1176 - val_bag_output_loss: 2.2825 - val_footwear_output_loss: 2.4043 - val_pose_output_loss: 1.5574 - val_emotion_output_loss: 2.2768 - val_gender_output_acc: 0.8388 - val_image_quality_output_acc: 0.5268 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.5218 - val_bag_output_acc: 0.6334 - val_footwear_output_acc: 0.6483 - val_pose_output_acc: 0.7793 - val_emotion_output_acc: 0.6305\n",
      "\n",
      "Epoch 00139: loss did not improve from 0.62717\n",
      "Epoch 140/200\n",
      " - 89s - loss: 0.7273 - gender_output_loss: 0.0118 - image_quality_output_loss: 0.0654 - age_output_loss: 0.1766 - weight_output_loss: 0.1576 - bag_output_loss: 0.1410 - footwear_output_loss: 0.0268 - pose_output_loss: 0.0110 - emotion_output_loss: 0.1370 - gender_output_acc: 0.9959 - image_quality_output_acc: 0.9791 - age_output_acc: 0.9217 - weight_output_acc: 0.9185 - bag_output_acc: 0.9006 - footwear_output_acc: 0.9908 - pose_output_acc: 0.9966 - emotion_output_acc: 0.9304 - val_loss: 17.0541 - val_gender_output_loss: 0.8577 - val_image_quality_output_loss: 2.7649 - val_age_output_loss: 2.8565 - val_weight_output_loss: 2.6374 - val_bag_output_loss: 1.8637 - val_footwear_output_loss: 2.4749 - val_pose_output_loss: 1.2517 - val_emotion_output_loss: 2.3473 - val_gender_output_acc: 0.8428 - val_image_quality_output_acc: 0.5412 - val_age_output_acc: 0.3765 - val_weight_output_acc: 0.6146 - val_bag_output_acc: 0.6136 - val_footwear_output_acc: 0.6220 - val_pose_output_acc: 0.8209 - val_emotion_output_acc: 0.6438\n",
      "\n",
      "Epoch 00140: loss did not improve from 0.62717\n",
      "Epoch 141/200\n",
      " - 88s - loss: 0.7551 - gender_output_loss: 0.0137 - image_quality_output_loss: 0.0587 - age_output_loss: 0.1796 - weight_output_loss: 0.1623 - bag_output_loss: 0.1506 - footwear_output_loss: 0.0306 - pose_output_loss: 0.0172 - emotion_output_loss: 0.1423 - gender_output_acc: 0.9962 - image_quality_output_acc: 0.9796 - age_output_acc: 0.9204 - weight_output_acc: 0.9158 - bag_output_acc: 0.8974 - footwear_output_acc: 0.9896 - pose_output_acc: 0.9948 - emotion_output_acc: 0.9289 - val_loss: 17.7930 - val_gender_output_loss: 0.9456 - val_image_quality_output_loss: 2.9875 - val_age_output_loss: 3.1158 - val_weight_output_loss: 2.5729 - val_bag_output_loss: 2.1205 - val_footwear_output_loss: 2.5962 - val_pose_output_loss: 1.3032 - val_emotion_output_loss: 2.1513 - val_gender_output_acc: 0.8413 - val_image_quality_output_acc: 0.5298 - val_age_output_acc: 0.3497 - val_weight_output_acc: 0.6076 - val_bag_output_acc: 0.6195 - val_footwear_output_acc: 0.6275 - val_pose_output_acc: 0.8199 - val_emotion_output_acc: 0.6081\n",
      "\n",
      "Epoch 00141: loss did not improve from 0.62717\n",
      "Epoch 142/200\n",
      " - 85s - loss: 0.6138 - gender_output_loss: 0.0062 - image_quality_output_loss: 0.0433 - age_output_loss: 0.1515 - weight_output_loss: 0.1382 - bag_output_loss: 0.1328 - footwear_output_loss: 0.0142 - pose_output_loss: 0.0060 - emotion_output_loss: 0.1218 - gender_output_acc: 0.9977 - image_quality_output_acc: 0.9852 - age_output_acc: 0.9282 - weight_output_acc: 0.9234 - bag_output_acc: 0.9007 - footwear_output_acc: 0.9950 - pose_output_acc: 0.9986 - emotion_output_acc: 0.9331 - val_loss: 19.6127 - val_gender_output_loss: 1.3361 - val_image_quality_output_loss: 3.1896 - val_age_output_loss: 3.1270 - val_weight_output_loss: 2.9493 - val_bag_output_loss: 2.5738 - val_footwear_output_loss: 2.6287 - val_pose_output_loss: 1.4030 - val_emotion_output_loss: 2.4052 - val_gender_output_acc: 0.8135 - val_image_quality_output_acc: 0.4970 - val_age_output_acc: 0.3348 - val_weight_output_acc: 0.6151 - val_bag_output_acc: 0.6374 - val_footwear_output_acc: 0.6250 - val_pose_output_acc: 0.8056 - val_emotion_output_acc: 0.5962\n",
      "\n",
      "Epoch 00142: loss improved from 0.62717 to 0.61379, saving model to model.h5\n",
      "Epoch 143/200\n",
      " - 84s - loss: 0.8706 - gender_output_loss: 0.0142 - image_quality_output_loss: 0.0881 - age_output_loss: 0.2137 - weight_output_loss: 0.1743 - bag_output_loss: 0.1561 - footwear_output_loss: 0.0491 - pose_output_loss: 0.0245 - emotion_output_loss: 0.1507 - gender_output_acc: 0.9956 - image_quality_output_acc: 0.9676 - age_output_acc: 0.9103 - weight_output_acc: 0.9141 - bag_output_acc: 0.8977 - footwear_output_acc: 0.9841 - pose_output_acc: 0.9917 - emotion_output_acc: 0.9285 - val_loss: 17.4274 - val_gender_output_loss: 0.9457 - val_image_quality_output_loss: 2.7837 - val_age_output_loss: 3.0728 - val_weight_output_loss: 2.4344 - val_bag_output_loss: 2.3111 - val_footwear_output_loss: 2.4519 - val_pose_output_loss: 1.1860 - val_emotion_output_loss: 2.2418 - val_gender_output_acc: 0.8373 - val_image_quality_output_acc: 0.5303 - val_age_output_acc: 0.3760 - val_weight_output_acc: 0.5630 - val_bag_output_acc: 0.6319 - val_footwear_output_acc: 0.6156 - val_pose_output_acc: 0.8110 - val_emotion_output_acc: 0.6037\n",
      "\n",
      "Epoch 00143: loss did not improve from 0.61379\n",
      "Epoch 144/200\n",
      " - 86s - loss: 0.8121 - gender_output_loss: 0.0143 - image_quality_output_loss: 0.0876 - age_output_loss: 0.1935 - weight_output_loss: 0.1704 - bag_output_loss: 0.1472 - footwear_output_loss: 0.0373 - pose_output_loss: 0.0173 - emotion_output_loss: 0.1445 - gender_output_acc: 0.9948 - image_quality_output_acc: 0.9698 - age_output_acc: 0.9166 - weight_output_acc: 0.9133 - bag_output_acc: 0.8978 - footwear_output_acc: 0.9867 - pose_output_acc: 0.9940 - emotion_output_acc: 0.9264 - val_loss: 18.3159 - val_gender_output_loss: 0.9226 - val_image_quality_output_loss: 2.7742 - val_age_output_loss: 2.9273 - val_weight_output_loss: 2.5281 - val_bag_output_loss: 2.8140 - val_footwear_output_loss: 2.3726 - val_pose_output_loss: 1.3112 - val_emotion_output_loss: 2.6658 - val_gender_output_acc: 0.8378 - val_image_quality_output_acc: 0.4916 - val_age_output_acc: 0.3482 - val_weight_output_acc: 0.5823 - val_bag_output_acc: 0.6136 - val_footwear_output_acc: 0.6285 - val_pose_output_acc: 0.8036 - val_emotion_output_acc: 0.6429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00144: loss did not improve from 0.61379\n",
      "Epoch 145/200\n",
      " - 87s - loss: 0.6860 - gender_output_loss: 0.0073 - image_quality_output_loss: 0.0601 - age_output_loss: 0.1695 - weight_output_loss: 0.1484 - bag_output_loss: 0.1362 - footwear_output_loss: 0.0239 - pose_output_loss: 0.0116 - emotion_output_loss: 0.1290 - gender_output_acc: 0.9976 - image_quality_output_acc: 0.9811 - age_output_acc: 0.9253 - weight_output_acc: 0.9212 - bag_output_acc: 0.9003 - footwear_output_acc: 0.9921 - pose_output_acc: 0.9963 - emotion_output_acc: 0.9320 - val_loss: 17.5304 - val_gender_output_loss: 0.9998 - val_image_quality_output_loss: 2.6334 - val_age_output_loss: 3.1043 - val_weight_output_loss: 2.5411 - val_bag_output_loss: 2.0694 - val_footwear_output_loss: 2.4880 - val_pose_output_loss: 1.2781 - val_emotion_output_loss: 2.4164 - val_gender_output_acc: 0.8328 - val_image_quality_output_acc: 0.5233 - val_age_output_acc: 0.3765 - val_weight_output_acc: 0.6037 - val_bag_output_acc: 0.6111 - val_footwear_output_acc: 0.6076 - val_pose_output_acc: 0.8075 - val_emotion_output_acc: 0.5828\n",
      "\n",
      "Epoch 00145: loss did not improve from 0.61379\n",
      "Epoch 146/200\n",
      " - 84s - loss: 0.7529 - gender_output_loss: 0.0139 - image_quality_output_loss: 0.0702 - age_output_loss: 0.1815 - weight_output_loss: 0.1528 - bag_output_loss: 0.1441 - footwear_output_loss: 0.0351 - pose_output_loss: 0.0149 - emotion_output_loss: 0.1403 - gender_output_acc: 0.9958 - image_quality_output_acc: 0.9752 - age_output_acc: 0.9183 - weight_output_acc: 0.9203 - bag_output_acc: 0.8984 - footwear_output_acc: 0.9878 - pose_output_acc: 0.9953 - emotion_output_acc: 0.9286 - val_loss: 17.8711 - val_gender_output_loss: 0.9597 - val_image_quality_output_loss: 2.7582 - val_age_output_loss: 3.1573 - val_weight_output_loss: 2.2177 - val_bag_output_loss: 2.5668 - val_footwear_output_loss: 2.6497 - val_pose_output_loss: 1.2647 - val_emotion_output_loss: 2.2969 - val_gender_output_acc: 0.8378 - val_image_quality_output_acc: 0.5352 - val_age_output_acc: 0.3363 - val_weight_output_acc: 0.5600 - val_bag_output_acc: 0.6260 - val_footwear_output_acc: 0.6156 - val_pose_output_acc: 0.8175 - val_emotion_output_acc: 0.6027\n",
      "\n",
      "Epoch 00146: loss did not improve from 0.61379\n",
      "Epoch 147/200\n",
      " - 84s - loss: 0.5991 - gender_output_loss: 0.0072 - image_quality_output_loss: 0.0468 - age_output_loss: 0.1415 - weight_output_loss: 0.1305 - bag_output_loss: 0.1301 - footwear_output_loss: 0.0160 - pose_output_loss: 0.0075 - emotion_output_loss: 0.1193 - gender_output_acc: 0.9977 - image_quality_output_acc: 0.9841 - age_output_acc: 0.9323 - weight_output_acc: 0.9258 - bag_output_acc: 0.9004 - footwear_output_acc: 0.9947 - pose_output_acc: 0.9976 - emotion_output_acc: 0.9339 - val_loss: 19.0296 - val_gender_output_loss: 1.0136 - val_image_quality_output_loss: 2.7945 - val_age_output_loss: 3.4078 - val_weight_output_loss: 3.0738 - val_bag_output_loss: 2.3114 - val_footwear_output_loss: 2.5152 - val_pose_output_loss: 1.3635 - val_emotion_output_loss: 2.5499 - val_gender_output_acc: 0.8289 - val_image_quality_output_acc: 0.5149 - val_age_output_acc: 0.3433 - val_weight_output_acc: 0.6275 - val_bag_output_acc: 0.6205 - val_footwear_output_acc: 0.6339 - val_pose_output_acc: 0.8165 - val_emotion_output_acc: 0.6220\n",
      "\n",
      "Epoch 00147: loss improved from 0.61379 to 0.59907, saving model to model.h5\n",
      "Epoch 148/200\n",
      " - 84s - loss: 0.7946 - gender_output_loss: 0.0119 - image_quality_output_loss: 0.0646 - age_output_loss: 0.1923 - weight_output_loss: 0.1589 - bag_output_loss: 0.1602 - footwear_output_loss: 0.0449 - pose_output_loss: 0.0164 - emotion_output_loss: 0.1454 - gender_output_acc: 0.9959 - image_quality_output_acc: 0.9767 - age_output_acc: 0.9173 - weight_output_acc: 0.9166 - bag_output_acc: 0.8956 - footwear_output_acc: 0.9850 - pose_output_acc: 0.9943 - emotion_output_acc: 0.9293 - val_loss: 17.5528 - val_gender_output_loss: 0.9325 - val_image_quality_output_loss: 2.6915 - val_age_output_loss: 3.1647 - val_weight_output_loss: 2.6288 - val_bag_output_loss: 2.1756 - val_footwear_output_loss: 2.4163 - val_pose_output_loss: 1.2517 - val_emotion_output_loss: 2.2916 - val_gender_output_acc: 0.8452 - val_image_quality_output_acc: 0.5407 - val_age_output_acc: 0.3646 - val_weight_output_acc: 0.5972 - val_bag_output_acc: 0.6181 - val_footwear_output_acc: 0.6210 - val_pose_output_acc: 0.8209 - val_emotion_output_acc: 0.6017\n",
      "\n",
      "Epoch 00148: loss did not improve from 0.59907\n",
      "Epoch 149/200\n",
      " - 86s - loss: 0.8335 - gender_output_loss: 0.0140 - image_quality_output_loss: 0.0851 - age_output_loss: 0.1981 - weight_output_loss: 0.1661 - bag_output_loss: 0.1538 - footwear_output_loss: 0.0412 - pose_output_loss: 0.0214 - emotion_output_loss: 0.1537 - gender_output_acc: 0.9950 - image_quality_output_acc: 0.9701 - age_output_acc: 0.9159 - weight_output_acc: 0.9161 - bag_output_acc: 0.8960 - footwear_output_acc: 0.9859 - pose_output_acc: 0.9931 - emotion_output_acc: 0.9237 - val_loss: 17.7813 - val_gender_output_loss: 0.9695 - val_image_quality_output_loss: 2.6778 - val_age_output_loss: 3.1066 - val_weight_output_loss: 2.9024 - val_bag_output_loss: 2.2740 - val_footwear_output_loss: 2.2240 - val_pose_output_loss: 1.1749 - val_emotion_output_loss: 2.4520 - val_gender_output_acc: 0.8343 - val_image_quality_output_acc: 0.5228 - val_age_output_acc: 0.3854 - val_weight_output_acc: 0.6141 - val_bag_output_acc: 0.6091 - val_footwear_output_acc: 0.6131 - val_pose_output_acc: 0.8130 - val_emotion_output_acc: 0.6607\n",
      "\n",
      "Epoch 00149: loss did not improve from 0.59907\n",
      "Epoch 150/200\n",
      " - 84s - loss: 0.7102 - gender_output_loss: 0.0104 - image_quality_output_loss: 0.0671 - age_output_loss: 0.1724 - weight_output_loss: 0.1477 - bag_output_loss: 0.1424 - footwear_output_loss: 0.0243 - pose_output_loss: 0.0129 - emotion_output_loss: 0.1330 - gender_output_acc: 0.9959 - image_quality_output_acc: 0.9767 - age_output_acc: 0.9246 - weight_output_acc: 0.9214 - bag_output_acc: 0.8988 - footwear_output_acc: 0.9920 - pose_output_acc: 0.9956 - emotion_output_acc: 0.9306 - val_loss: 17.5698 - val_gender_output_loss: 0.9525 - val_image_quality_output_loss: 2.6738 - val_age_output_loss: 3.1139 - val_weight_output_loss: 2.6589 - val_bag_output_loss: 2.1958 - val_footwear_output_loss: 2.3798 - val_pose_output_loss: 1.2370 - val_emotion_output_loss: 2.3582 - val_gender_output_acc: 0.8492 - val_image_quality_output_acc: 0.5273 - val_age_output_acc: 0.3735 - val_weight_output_acc: 0.5992 - val_bag_output_acc: 0.6176 - val_footwear_output_acc: 0.6176 - val_pose_output_acc: 0.8185 - val_emotion_output_acc: 0.6126\n",
      "\n",
      "Epoch 00150: loss did not improve from 0.59907\n",
      "Epoch 151/200\n",
      " - 84s - loss: 0.7942 - gender_output_loss: 0.0161 - image_quality_output_loss: 0.0778 - age_output_loss: 0.1837 - weight_output_loss: 0.1699 - bag_output_loss: 0.1520 - footwear_output_loss: 0.0378 - pose_output_loss: 0.0164 - emotion_output_loss: 0.1405 - gender_output_acc: 0.9947 - image_quality_output_acc: 0.9734 - age_output_acc: 0.9188 - weight_output_acc: 0.9141 - bag_output_acc: 0.8974 - footwear_output_acc: 0.9870 - pose_output_acc: 0.9940 - emotion_output_acc: 0.9288 - val_loss: 17.5153 - val_gender_output_loss: 0.9214 - val_image_quality_output_loss: 2.5762 - val_age_output_loss: 3.4193 - val_weight_output_loss: 2.5616 - val_bag_output_loss: 2.0290 - val_footwear_output_loss: 2.4370 - val_pose_output_loss: 1.3898 - val_emotion_output_loss: 2.1810 - val_gender_output_acc: 0.8388 - val_image_quality_output_acc: 0.5233 - val_age_output_acc: 0.3725 - val_weight_output_acc: 0.5933 - val_bag_output_acc: 0.6171 - val_footwear_output_acc: 0.6136 - val_pose_output_acc: 0.7946 - val_emotion_output_acc: 0.6002\n",
      "\n",
      "Epoch 00151: loss did not improve from 0.59907\n",
      "Epoch 152/200\n",
      " - 84s - loss: 0.6136 - gender_output_loss: 0.0076 - image_quality_output_loss: 0.0508 - age_output_loss: 0.1479 - weight_output_loss: 0.1368 - bag_output_loss: 0.1297 - footwear_output_loss: 0.0178 - pose_output_loss: 0.0085 - emotion_output_loss: 0.1145 - gender_output_acc: 0.9975 - image_quality_output_acc: 0.9841 - age_output_acc: 0.9294 - weight_output_acc: 0.9240 - bag_output_acc: 0.9009 - footwear_output_acc: 0.9942 - pose_output_acc: 0.9975 - emotion_output_acc: 0.9348 - val_loss: 19.9397 - val_gender_output_loss: 1.0459 - val_image_quality_output_loss: 2.8896 - val_age_output_loss: 3.9749 - val_weight_output_loss: 3.4028 - val_bag_output_loss: 1.9939 - val_footwear_output_loss: 2.5766 - val_pose_output_loss: 1.3120 - val_emotion_output_loss: 2.7440 - val_gender_output_acc: 0.8363 - val_image_quality_output_acc: 0.5451 - val_age_output_acc: 0.3844 - val_weight_output_acc: 0.6285 - val_bag_output_acc: 0.6101 - val_footwear_output_acc: 0.5928 - val_pose_output_acc: 0.8130 - val_emotion_output_acc: 0.6558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00152: loss did not improve from 0.59907\n",
      "Epoch 153/200\n",
      " - 88s - loss: 0.7604 - gender_output_loss: 0.0146 - image_quality_output_loss: 0.0657 - age_output_loss: 0.1885 - weight_output_loss: 0.1531 - bag_output_loss: 0.1468 - footwear_output_loss: 0.0375 - pose_output_loss: 0.0163 - emotion_output_loss: 0.1380 - gender_output_acc: 0.9950 - image_quality_output_acc: 0.9772 - age_output_acc: 0.9178 - weight_output_acc: 0.9189 - bag_output_acc: 0.8976 - footwear_output_acc: 0.9876 - pose_output_acc: 0.9946 - emotion_output_acc: 0.9309 - val_loss: 17.9341 - val_gender_output_loss: 0.9423 - val_image_quality_output_loss: 2.7240 - val_age_output_loss: 3.2373 - val_weight_output_loss: 2.7137 - val_bag_output_loss: 2.2169 - val_footwear_output_loss: 2.4188 - val_pose_output_loss: 1.2454 - val_emotion_output_loss: 2.4357 - val_gender_output_acc: 0.8363 - val_image_quality_output_acc: 0.5357 - val_age_output_acc: 0.3591 - val_weight_output_acc: 0.6052 - val_bag_output_acc: 0.6424 - val_footwear_output_acc: 0.6329 - val_pose_output_acc: 0.8140 - val_emotion_output_acc: 0.6141\n",
      "\n",
      "Epoch 00153: loss did not improve from 0.59907\n",
      "Epoch 154/200\n",
      " - 92s - loss: 0.7147 - gender_output_loss: 0.0102 - image_quality_output_loss: 0.0679 - age_output_loss: 0.1729 - weight_output_loss: 0.1464 - bag_output_loss: 0.1415 - footwear_output_loss: 0.0271 - pose_output_loss: 0.0168 - emotion_output_loss: 0.1318 - gender_output_acc: 0.9964 - image_quality_output_acc: 0.9756 - age_output_acc: 0.9223 - weight_output_acc: 0.9209 - bag_output_acc: 0.8979 - footwear_output_acc: 0.9903 - pose_output_acc: 0.9955 - emotion_output_acc: 0.9311 - val_loss: 17.2912 - val_gender_output_loss: 1.0016 - val_image_quality_output_loss: 2.6244 - val_age_output_loss: 3.0997 - val_weight_output_loss: 2.1500 - val_bag_output_loss: 2.0608 - val_footwear_output_loss: 2.6914 - val_pose_output_loss: 1.1311 - val_emotion_output_loss: 2.5322 - val_gender_output_acc: 0.8368 - val_image_quality_output_acc: 0.5193 - val_age_output_acc: 0.3676 - val_weight_output_acc: 0.5526 - val_bag_output_acc: 0.6205 - val_footwear_output_acc: 0.5794 - val_pose_output_acc: 0.7966 - val_emotion_output_acc: 0.5283\n",
      "\n",
      "Epoch 00154: loss did not improve from 0.59907\n",
      "Epoch 155/200\n",
      " - 96s - loss: 0.6760 - gender_output_loss: 0.0100 - image_quality_output_loss: 0.0591 - age_output_loss: 0.1641 - weight_output_loss: 0.1400 - bag_output_loss: 0.1411 - footwear_output_loss: 0.0246 - pose_output_loss: 0.0114 - emotion_output_loss: 0.1256 - gender_output_acc: 0.9971 - image_quality_output_acc: 0.9819 - age_output_acc: 0.9240 - weight_output_acc: 0.9239 - bag_output_acc: 0.8990 - footwear_output_acc: 0.9920 - pose_output_acc: 0.9965 - emotion_output_acc: 0.9334 - val_loss: 17.0155 - val_gender_output_loss: 0.9614 - val_image_quality_output_loss: 2.5935 - val_age_output_loss: 2.7866 - val_weight_output_loss: 2.3232 - val_bag_output_loss: 2.3752 - val_footwear_output_loss: 2.5259 - val_pose_output_loss: 1.1792 - val_emotion_output_loss: 2.2707 - val_gender_output_acc: 0.8433 - val_image_quality_output_acc: 0.5387 - val_age_output_acc: 0.3621 - val_weight_output_acc: 0.5744 - val_bag_output_acc: 0.6200 - val_footwear_output_acc: 0.6359 - val_pose_output_acc: 0.8135 - val_emotion_output_acc: 0.6190\n",
      "\n",
      "Epoch 00155: loss did not improve from 0.59907\n",
      "Epoch 156/200\n",
      " - 91s - loss: 0.7197 - gender_output_loss: 0.0107 - image_quality_output_loss: 0.0564 - age_output_loss: 0.1760 - weight_output_loss: 0.1471 - bag_output_loss: 0.1439 - footwear_output_loss: 0.0360 - pose_output_loss: 0.0154 - emotion_output_loss: 0.1344 - gender_output_acc: 0.9959 - image_quality_output_acc: 0.9786 - age_output_acc: 0.9210 - weight_output_acc: 0.9215 - bag_output_acc: 0.8987 - footwear_output_acc: 0.9877 - pose_output_acc: 0.9953 - emotion_output_acc: 0.9296 - val_loss: 19.2354 - val_gender_output_loss: 1.0360 - val_image_quality_output_loss: 3.3167 - val_age_output_loss: 3.4818 - val_weight_output_loss: 2.7647 - val_bag_output_loss: 2.1356 - val_footwear_output_loss: 2.4749 - val_pose_output_loss: 1.2689 - val_emotion_output_loss: 2.7568 - val_gender_output_acc: 0.8363 - val_image_quality_output_acc: 0.5357 - val_age_output_acc: 0.3745 - val_weight_output_acc: 0.6106 - val_bag_output_acc: 0.6161 - val_footwear_output_acc: 0.6190 - val_pose_output_acc: 0.8239 - val_emotion_output_acc: 0.6518\n",
      "\n",
      "Epoch 00156: loss did not improve from 0.59907\n",
      "Epoch 157/200\n",
      " - 100s - loss: 0.5884 - gender_output_loss: 0.0087 - image_quality_output_loss: 0.0463 - age_output_loss: 0.1420 - weight_output_loss: 0.1266 - bag_output_loss: 0.1288 - footwear_output_loss: 0.0163 - pose_output_loss: 0.0080 - emotion_output_loss: 0.1118 - gender_output_acc: 0.9974 - image_quality_output_acc: 0.9861 - age_output_acc: 0.9319 - weight_output_acc: 0.9258 - bag_output_acc: 0.9003 - footwear_output_acc: 0.9946 - pose_output_acc: 0.9976 - emotion_output_acc: 0.9356 - val_loss: 19.5488 - val_gender_output_loss: 1.2194 - val_image_quality_output_loss: 2.6204 - val_age_output_loss: 3.0964 - val_weight_output_loss: 3.7778 - val_bag_output_loss: 2.3266 - val_footwear_output_loss: 2.7356 - val_pose_output_loss: 1.3156 - val_emotion_output_loss: 2.4570 - val_gender_output_acc: 0.8254 - val_image_quality_output_acc: 0.5174 - val_age_output_acc: 0.3418 - val_weight_output_acc: 0.6285 - val_bag_output_acc: 0.6275 - val_footwear_output_acc: 0.6275 - val_pose_output_acc: 0.8269 - val_emotion_output_acc: 0.6751\n",
      "\n",
      "Epoch 00157: loss improved from 0.59907 to 0.58841, saving model to model.h5\n",
      "Epoch 158/200\n",
      " - 91s - loss: 0.7808 - gender_output_loss: 0.0099 - image_quality_output_loss: 0.0741 - age_output_loss: 0.1906 - weight_output_loss: 0.1601 - bag_output_loss: 0.1470 - footwear_output_loss: 0.0375 - pose_output_loss: 0.0184 - emotion_output_loss: 0.1432 - gender_output_acc: 0.9967 - image_quality_output_acc: 0.9740 - age_output_acc: 0.9177 - weight_output_acc: 0.9189 - bag_output_acc: 0.8977 - footwear_output_acc: 0.9877 - pose_output_acc: 0.9939 - emotion_output_acc: 0.9293 - val_loss: 17.9402 - val_gender_output_loss: 0.9901 - val_image_quality_output_loss: 2.7738 - val_age_output_loss: 3.1832 - val_weight_output_loss: 2.5114 - val_bag_output_loss: 2.4213 - val_footwear_output_loss: 2.4259 - val_pose_output_loss: 1.1728 - val_emotion_output_loss: 2.4617 - val_gender_output_acc: 0.8333 - val_image_quality_output_acc: 0.5412 - val_age_output_acc: 0.3735 - val_weight_output_acc: 0.6022 - val_bag_output_acc: 0.6280 - val_footwear_output_acc: 0.6275 - val_pose_output_acc: 0.8323 - val_emotion_output_acc: 0.6126\n",
      "\n",
      "Epoch 00158: loss did not improve from 0.58841\n",
      "Epoch 159/200\n",
      " - 93s - loss: 0.6474 - gender_output_loss: 0.0085 - image_quality_output_loss: 0.0524 - age_output_loss: 0.1489 - weight_output_loss: 0.1393 - bag_output_loss: 0.1352 - footwear_output_loss: 0.0258 - pose_output_loss: 0.0139 - emotion_output_loss: 0.1234 - gender_output_acc: 0.9974 - image_quality_output_acc: 0.9814 - age_output_acc: 0.9295 - weight_output_acc: 0.9226 - bag_output_acc: 0.8990 - footwear_output_acc: 0.9921 - pose_output_acc: 0.9952 - emotion_output_acc: 0.9321 - val_loss: 19.9131 - val_gender_output_loss: 1.1844 - val_image_quality_output_loss: 3.1582 - val_age_output_loss: 3.8510 - val_weight_output_loss: 3.2973 - val_bag_output_loss: 1.9854 - val_footwear_output_loss: 2.5830 - val_pose_output_loss: 1.2761 - val_emotion_output_loss: 2.5777 - val_gender_output_acc: 0.8100 - val_image_quality_output_acc: 0.5283 - val_age_output_acc: 0.3452 - val_weight_output_acc: 0.6255 - val_bag_output_acc: 0.6027 - val_footwear_output_acc: 0.6111 - val_pose_output_acc: 0.8209 - val_emotion_output_acc: 0.6195\n",
      "\n",
      "Epoch 00159: loss did not improve from 0.58841\n",
      "Epoch 160/200\n",
      " - 90s - loss: 0.6223 - gender_output_loss: 0.0076 - image_quality_output_loss: 0.0520 - age_output_loss: 0.1460 - weight_output_loss: 0.1335 - bag_output_loss: 0.1348 - footwear_output_loss: 0.0197 - pose_output_loss: 0.0094 - emotion_output_loss: 0.1193 - gender_output_acc: 0.9971 - image_quality_output_acc: 0.9821 - age_output_acc: 0.9288 - weight_output_acc: 0.9239 - bag_output_acc: 0.9000 - footwear_output_acc: 0.9938 - pose_output_acc: 0.9968 - emotion_output_acc: 0.9351 - val_loss: 19.1802 - val_gender_output_loss: 1.0409 - val_image_quality_output_loss: 3.0264 - val_age_output_loss: 3.4457 - val_weight_output_loss: 2.8132 - val_bag_output_loss: 2.1961 - val_footwear_output_loss: 2.6641 - val_pose_output_loss: 1.2360 - val_emotion_output_loss: 2.7577 - val_gender_output_acc: 0.8378 - val_image_quality_output_acc: 0.4881 - val_age_output_acc: 0.3775 - val_weight_output_acc: 0.6052 - val_bag_output_acc: 0.6166 - val_footwear_output_acc: 0.6151 - val_pose_output_acc: 0.8204 - val_emotion_output_acc: 0.6374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00160: loss did not improve from 0.58841\n",
      "Epoch 161/200\n",
      " - 91s - loss: 0.8364 - gender_output_loss: 0.0181 - image_quality_output_loss: 0.0743 - age_output_loss: 0.1957 - weight_output_loss: 0.1747 - bag_output_loss: 0.1478 - footwear_output_loss: 0.0491 - pose_output_loss: 0.0215 - emotion_output_loss: 0.1552 - gender_output_acc: 0.9941 - image_quality_output_acc: 0.9720 - age_output_acc: 0.9141 - weight_output_acc: 0.9129 - bag_output_acc: 0.8964 - footwear_output_acc: 0.9826 - pose_output_acc: 0.9933 - emotion_output_acc: 0.9253 - val_loss: 17.5465 - val_gender_output_loss: 0.9844 - val_image_quality_output_loss: 2.8106 - val_age_output_loss: 3.2317 - val_weight_output_loss: 2.3477 - val_bag_output_loss: 2.3487 - val_footwear_output_loss: 2.3330 - val_pose_output_loss: 1.2077 - val_emotion_output_loss: 2.2827 - val_gender_output_acc: 0.8323 - val_image_quality_output_acc: 0.5208 - val_age_output_acc: 0.3780 - val_weight_output_acc: 0.5942 - val_bag_output_acc: 0.6280 - val_footwear_output_acc: 0.5957 - val_pose_output_acc: 0.8180 - val_emotion_output_acc: 0.6314\n",
      "\n",
      "Epoch 00161: loss did not improve from 0.58841\n",
      "Epoch 162/200\n",
      " - 91s - loss: 0.5966 - gender_output_loss: 0.0070 - image_quality_output_loss: 0.0433 - age_output_loss: 0.1488 - weight_output_loss: 0.1269 - bag_output_loss: 0.1283 - footwear_output_loss: 0.0167 - pose_output_loss: 0.0098 - emotion_output_loss: 0.1158 - gender_output_acc: 0.9982 - image_quality_output_acc: 0.9865 - age_output_acc: 0.9309 - weight_output_acc: 0.9260 - bag_output_acc: 0.9010 - footwear_output_acc: 0.9944 - pose_output_acc: 0.9972 - emotion_output_acc: 0.9358 - val_loss: 22.5258 - val_gender_output_loss: 0.9876 - val_image_quality_output_loss: 4.9471 - val_age_output_loss: 4.2522 - val_weight_output_loss: 3.0799 - val_bag_output_loss: 2.7407 - val_footwear_output_loss: 2.7275 - val_pose_output_loss: 1.2802 - val_emotion_output_loss: 2.5107 - val_gender_output_acc: 0.8318 - val_image_quality_output_acc: 0.4167 - val_age_output_acc: 0.3948 - val_weight_output_acc: 0.6195 - val_bag_output_acc: 0.5561 - val_footwear_output_acc: 0.6151 - val_pose_output_acc: 0.8145 - val_emotion_output_acc: 0.6607\n",
      "\n",
      "Epoch 00162: loss did not improve from 0.58841\n",
      "Epoch 163/200\n",
      " - 88s - loss: 0.7356 - gender_output_loss: 0.0105 - image_quality_output_loss: 0.0707 - age_output_loss: 0.1734 - weight_output_loss: 0.1504 - bag_output_loss: 0.1451 - footwear_output_loss: 0.0409 - pose_output_loss: 0.0141 - emotion_output_loss: 0.1306 - gender_output_acc: 0.9967 - image_quality_output_acc: 0.9759 - age_output_acc: 0.9208 - weight_output_acc: 0.9190 - bag_output_acc: 0.8979 - footwear_output_acc: 0.9865 - pose_output_acc: 0.9958 - emotion_output_acc: 0.9317 - val_loss: 17.9757 - val_gender_output_loss: 1.0071 - val_image_quality_output_loss: 2.7298 - val_age_output_loss: 3.1975 - val_weight_output_loss: 2.7042 - val_bag_output_loss: 2.2583 - val_footwear_output_loss: 2.4217 - val_pose_output_loss: 1.2455 - val_emotion_output_loss: 2.4115 - val_gender_output_acc: 0.8353 - val_image_quality_output_acc: 0.5327 - val_age_output_acc: 0.3770 - val_weight_output_acc: 0.6076 - val_bag_output_acc: 0.6305 - val_footwear_output_acc: 0.6280 - val_pose_output_acc: 0.8229 - val_emotion_output_acc: 0.6329\n",
      "\n",
      "Epoch 00163: loss did not improve from 0.58841\n",
      "Epoch 164/200\n",
      " - 89s - loss: 0.7114 - gender_output_loss: 0.0101 - image_quality_output_loss: 0.0691 - age_output_loss: 0.1656 - weight_output_loss: 0.1513 - bag_output_loss: 0.1415 - footwear_output_loss: 0.0303 - pose_output_loss: 0.0170 - emotion_output_loss: 0.1266 - gender_output_acc: 0.9969 - image_quality_output_acc: 0.9779 - age_output_acc: 0.9257 - weight_output_acc: 0.9202 - bag_output_acc: 0.8969 - footwear_output_acc: 0.9901 - pose_output_acc: 0.9951 - emotion_output_acc: 0.9319 - val_loss: 17.9640 - val_gender_output_loss: 1.0247 - val_image_quality_output_loss: 2.5190 - val_age_output_loss: 3.6238 - val_weight_output_loss: 2.5234 - val_bag_output_loss: 2.2541 - val_footwear_output_loss: 2.4433 - val_pose_output_loss: 1.2938 - val_emotion_output_loss: 2.2820 - val_gender_output_acc: 0.8294 - val_image_quality_output_acc: 0.5060 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.5962 - val_bag_output_acc: 0.6136 - val_footwear_output_acc: 0.5947 - val_pose_output_acc: 0.8110 - val_emotion_output_acc: 0.6215\n",
      "\n",
      "Epoch 00164: loss did not improve from 0.58841\n",
      "Epoch 165/200\n",
      " - 89s - loss: 0.6349 - gender_output_loss: 0.0085 - image_quality_output_loss: 0.0537 - age_output_loss: 0.1558 - weight_output_loss: 0.1368 - bag_output_loss: 0.1343 - footwear_output_loss: 0.0200 - pose_output_loss: 0.0081 - emotion_output_loss: 0.1178 - gender_output_acc: 0.9975 - image_quality_output_acc: 0.9838 - age_output_acc: 0.9284 - weight_output_acc: 0.9230 - bag_output_acc: 0.8999 - footwear_output_acc: 0.9934 - pose_output_acc: 0.9972 - emotion_output_acc: 0.9327 - val_loss: 18.6928 - val_gender_output_loss: 1.0070 - val_image_quality_output_loss: 2.8937 - val_age_output_loss: 3.5197 - val_weight_output_loss: 2.6125 - val_bag_output_loss: 2.4552 - val_footwear_output_loss: 2.5256 - val_pose_output_loss: 1.1776 - val_emotion_output_loss: 2.5016 - val_gender_output_acc: 0.8353 - val_image_quality_output_acc: 0.5030 - val_age_output_acc: 0.3805 - val_weight_output_acc: 0.5918 - val_bag_output_acc: 0.6280 - val_footwear_output_acc: 0.6151 - val_pose_output_acc: 0.8199 - val_emotion_output_acc: 0.5913\n",
      "\n",
      "Epoch 00165: loss did not improve from 0.58841\n",
      "Epoch 166/200\n",
      " - 87s - loss: 0.6443 - gender_output_loss: 0.0088 - image_quality_output_loss: 0.0503 - age_output_loss: 0.1544 - weight_output_loss: 0.1424 - bag_output_loss: 0.1342 - footwear_output_loss: 0.0233 - pose_output_loss: 0.0119 - emotion_output_loss: 0.1190 - gender_output_acc: 0.9975 - image_quality_output_acc: 0.9841 - age_output_acc: 0.9267 - weight_output_acc: 0.9216 - bag_output_acc: 0.9000 - footwear_output_acc: 0.9928 - pose_output_acc: 0.9963 - emotion_output_acc: 0.9321 - val_loss: 19.4846 - val_gender_output_loss: 1.1084 - val_image_quality_output_loss: 3.3102 - val_age_output_loss: 3.4878 - val_weight_output_loss: 3.0043 - val_bag_output_loss: 2.3813 - val_footwear_output_loss: 2.5459 - val_pose_output_loss: 1.2834 - val_emotion_output_loss: 2.3635 - val_gender_output_acc: 0.8343 - val_image_quality_output_acc: 0.5278 - val_age_output_acc: 0.3566 - val_weight_output_acc: 0.6131 - val_bag_output_acc: 0.6310 - val_footwear_output_acc: 0.6106 - val_pose_output_acc: 0.8175 - val_emotion_output_acc: 0.6057\n",
      "\n",
      "Epoch 00166: loss did not improve from 0.58841\n",
      "Epoch 167/200\n",
      " - 86s - loss: 0.5477 - gender_output_loss: 0.0054 - image_quality_output_loss: 0.0334 - age_output_loss: 0.1372 - weight_output_loss: 0.1235 - bag_output_loss: 0.1216 - footwear_output_loss: 0.0147 - pose_output_loss: 0.0064 - emotion_output_loss: 0.1055 - gender_output_acc: 0.9982 - image_quality_output_acc: 0.9899 - age_output_acc: 0.9329 - weight_output_acc: 0.9264 - bag_output_acc: 0.9026 - footwear_output_acc: 0.9955 - pose_output_acc: 0.9980 - emotion_output_acc: 0.9365 - val_loss: 21.3971 - val_gender_output_loss: 1.2073 - val_image_quality_output_loss: 3.5659 - val_age_output_loss: 3.0161 - val_weight_output_loss: 3.4088 - val_bag_output_loss: 2.3073 - val_footwear_output_loss: 2.8006 - val_pose_output_loss: 1.5239 - val_emotion_output_loss: 3.5672 - val_gender_output_acc: 0.8323 - val_image_quality_output_acc: 0.5030 - val_age_output_acc: 0.3343 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5893 - val_footwear_output_acc: 0.6161 - val_pose_output_acc: 0.8204 - val_emotion_output_acc: 0.7024\n",
      "\n",
      "Epoch 00167: loss improved from 0.58841 to 0.54767, saving model to model.h5\n",
      "Epoch 168/200\n",
      " - 84s - loss: 0.7346 - gender_output_loss: 0.0114 - image_quality_output_loss: 0.0597 - age_output_loss: 0.1773 - weight_output_loss: 0.1565 - bag_output_loss: 0.1461 - footwear_output_loss: 0.0379 - pose_output_loss: 0.0127 - emotion_output_loss: 0.1329 - gender_output_acc: 0.9957 - image_quality_output_acc: 0.9793 - age_output_acc: 0.9196 - weight_output_acc: 0.9169 - bag_output_acc: 0.8980 - footwear_output_acc: 0.9867 - pose_output_acc: 0.9959 - emotion_output_acc: 0.9309 - val_loss: 17.8430 - val_gender_output_loss: 1.0158 - val_image_quality_output_loss: 2.8648 - val_age_output_loss: 3.1849 - val_weight_output_loss: 2.4453 - val_bag_output_loss: 2.2969 - val_footwear_output_loss: 2.3458 - val_pose_output_loss: 1.2523 - val_emotion_output_loss: 2.4371 - val_gender_output_acc: 0.8373 - val_image_quality_output_acc: 0.5258 - val_age_output_acc: 0.3844 - val_weight_output_acc: 0.5764 - val_bag_output_acc: 0.6280 - val_footwear_output_acc: 0.6176 - val_pose_output_acc: 0.8170 - val_emotion_output_acc: 0.6002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00168: loss did not improve from 0.54767\n",
      "Epoch 169/200\n",
      " - 84s - loss: 0.6485 - gender_output_loss: 0.0125 - image_quality_output_loss: 0.0441 - age_output_loss: 0.1565 - weight_output_loss: 0.1407 - bag_output_loss: 0.1362 - footwear_output_loss: 0.0259 - pose_output_loss: 0.0125 - emotion_output_loss: 0.1200 - gender_output_acc: 0.9955 - image_quality_output_acc: 0.9863 - age_output_acc: 0.9276 - weight_output_acc: 0.9231 - bag_output_acc: 0.8990 - footwear_output_acc: 0.9910 - pose_output_acc: 0.9956 - emotion_output_acc: 0.9329 - val_loss: 19.0325 - val_gender_output_loss: 1.0000 - val_image_quality_output_loss: 2.9628 - val_age_output_loss: 3.3067 - val_weight_output_loss: 2.7799 - val_bag_output_loss: 2.4474 - val_footwear_output_loss: 2.5509 - val_pose_output_loss: 1.3243 - val_emotion_output_loss: 2.6607 - val_gender_output_acc: 0.8279 - val_image_quality_output_acc: 0.5387 - val_age_output_acc: 0.3428 - val_weight_output_acc: 0.6171 - val_bag_output_acc: 0.6280 - val_footwear_output_acc: 0.6002 - val_pose_output_acc: 0.8016 - val_emotion_output_acc: 0.5020\n",
      "\n",
      "Epoch 00169: loss did not improve from 0.54767\n",
      "Epoch 170/200\n",
      " - 84s - loss: 0.6231 - gender_output_loss: 0.0087 - image_quality_output_loss: 0.0556 - age_output_loss: 0.1553 - weight_output_loss: 0.1267 - bag_output_loss: 0.1296 - footwear_output_loss: 0.0198 - pose_output_loss: 0.0092 - emotion_output_loss: 0.1182 - gender_output_acc: 0.9970 - image_quality_output_acc: 0.9820 - age_output_acc: 0.9266 - weight_output_acc: 0.9269 - bag_output_acc: 0.9007 - footwear_output_acc: 0.9931 - pose_output_acc: 0.9972 - emotion_output_acc: 0.9333 - val_loss: 18.5759 - val_gender_output_loss: 0.9515 - val_image_quality_output_loss: 3.0153 - val_age_output_loss: 3.1135 - val_weight_output_loss: 2.5235 - val_bag_output_loss: 2.4478 - val_footwear_output_loss: 2.6651 - val_pose_output_loss: 1.4268 - val_emotion_output_loss: 2.4325 - val_gender_output_acc: 0.8418 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.3517 - val_weight_output_acc: 0.5972 - val_bag_output_acc: 0.6300 - val_footwear_output_acc: 0.6419 - val_pose_output_acc: 0.8021 - val_emotion_output_acc: 0.5422\n",
      "\n",
      "Epoch 00170: loss did not improve from 0.54767\n",
      "Epoch 171/200\n",
      " - 84s - loss: 0.7089 - gender_output_loss: 0.0109 - image_quality_output_loss: 0.0664 - age_output_loss: 0.1645 - weight_output_loss: 0.1470 - bag_output_loss: 0.1418 - footwear_output_loss: 0.0311 - pose_output_loss: 0.0152 - emotion_output_loss: 0.1319 - gender_output_acc: 0.9970 - image_quality_output_acc: 0.9765 - age_output_acc: 0.9239 - weight_output_acc: 0.9195 - bag_output_acc: 0.8976 - footwear_output_acc: 0.9898 - pose_output_acc: 0.9949 - emotion_output_acc: 0.9300 - val_loss: 18.5607 - val_gender_output_loss: 0.9866 - val_image_quality_output_loss: 2.8696 - val_age_output_loss: 3.4655 - val_weight_output_loss: 2.7437 - val_bag_output_loss: 2.2459 - val_footwear_output_loss: 2.4590 - val_pose_output_loss: 1.3124 - val_emotion_output_loss: 2.4780 - val_gender_output_acc: 0.8423 - val_image_quality_output_acc: 0.5397 - val_age_output_acc: 0.3725 - val_weight_output_acc: 0.6071 - val_bag_output_acc: 0.6210 - val_footwear_output_acc: 0.6324 - val_pose_output_acc: 0.8214 - val_emotion_output_acc: 0.6086\n",
      "\n",
      "Epoch 00171: loss did not improve from 0.54767\n",
      "Epoch 172/200\n",
      " - 84s - loss: 0.5636 - gender_output_loss: 0.0059 - image_quality_output_loss: 0.0455 - age_output_loss: 0.1332 - weight_output_loss: 0.1245 - bag_output_loss: 0.1233 - footwear_output_loss: 0.0134 - pose_output_loss: 0.0073 - emotion_output_loss: 0.1104 - gender_output_acc: 0.9984 - image_quality_output_acc: 0.9875 - age_output_acc: 0.9355 - weight_output_acc: 0.9251 - bag_output_acc: 0.9014 - footwear_output_acc: 0.9957 - pose_output_acc: 0.9974 - emotion_output_acc: 0.9354 - val_loss: 21.3981 - val_gender_output_loss: 0.9006 - val_image_quality_output_loss: 4.7080 - val_age_output_loss: 3.6511 - val_weight_output_loss: 2.6956 - val_bag_output_loss: 2.6695 - val_footwear_output_loss: 2.7349 - val_pose_output_loss: 1.3497 - val_emotion_output_loss: 2.6886 - val_gender_output_acc: 0.8353 - val_image_quality_output_acc: 0.4097 - val_age_output_acc: 0.3919 - val_weight_output_acc: 0.5694 - val_bag_output_acc: 0.6275 - val_footwear_output_acc: 0.6106 - val_pose_output_acc: 0.8011 - val_emotion_output_acc: 0.6379\n",
      "\n",
      "Epoch 00172: loss did not improve from 0.54767\n",
      "Epoch 173/200\n",
      " - 84s - loss: 0.7236 - gender_output_loss: 0.0119 - image_quality_output_loss: 0.0624 - age_output_loss: 0.1695 - weight_output_loss: 0.1550 - bag_output_loss: 0.1413 - footwear_output_loss: 0.0344 - pose_output_loss: 0.0129 - emotion_output_loss: 0.1362 - gender_output_acc: 0.9962 - image_quality_output_acc: 0.9808 - age_output_acc: 0.9212 - weight_output_acc: 0.9174 - bag_output_acc: 0.8982 - footwear_output_acc: 0.9879 - pose_output_acc: 0.9957 - emotion_output_acc: 0.9293 - val_loss: 18.1961 - val_gender_output_loss: 1.0129 - val_image_quality_output_loss: 2.8739 - val_age_output_loss: 3.1715 - val_weight_output_loss: 2.6554 - val_bag_output_loss: 2.3682 - val_footwear_output_loss: 2.4718 - val_pose_output_loss: 1.2779 - val_emotion_output_loss: 2.3646 - val_gender_output_acc: 0.8442 - val_image_quality_output_acc: 0.5293 - val_age_output_acc: 0.3869 - val_weight_output_acc: 0.6057 - val_bag_output_acc: 0.6181 - val_footwear_output_acc: 0.6161 - val_pose_output_acc: 0.8209 - val_emotion_output_acc: 0.6260\n",
      "\n",
      "Epoch 00173: loss did not improve from 0.54767\n",
      "Epoch 174/200\n",
      " - 84s - loss: 0.6119 - gender_output_loss: 0.0064 - image_quality_output_loss: 0.0451 - age_output_loss: 0.1492 - weight_output_loss: 0.1298 - bag_output_loss: 0.1339 - footwear_output_loss: 0.0204 - pose_output_loss: 0.0103 - emotion_output_loss: 0.1168 - gender_output_acc: 0.9981 - image_quality_output_acc: 0.9851 - age_output_acc: 0.9303 - weight_output_acc: 0.9253 - bag_output_acc: 0.8991 - footwear_output_acc: 0.9931 - pose_output_acc: 0.9966 - emotion_output_acc: 0.9339 - val_loss: 18.8036 - val_gender_output_loss: 1.0181 - val_image_quality_output_loss: 3.0622 - val_age_output_loss: 2.6238 - val_weight_output_loss: 2.6925 - val_bag_output_loss: 3.0610 - val_footwear_output_loss: 3.0020 - val_pose_output_loss: 1.5946 - val_emotion_output_loss: 1.7494 - val_gender_output_acc: 0.8318 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.3447 - val_weight_output_acc: 0.4306 - val_bag_output_acc: 0.6111 - val_footwear_output_acc: 0.6190 - val_pose_output_acc: 0.7753 - val_emotion_output_acc: 0.5714\n",
      "\n",
      "Epoch 00174: loss did not improve from 0.54767\n",
      "Epoch 175/200\n",
      " - 85s - loss: 0.5974 - gender_output_loss: 0.0067 - image_quality_output_loss: 0.0453 - age_output_loss: 0.1440 - weight_output_loss: 0.1273 - bag_output_loss: 0.1309 - footwear_output_loss: 0.0184 - pose_output_loss: 0.0106 - emotion_output_loss: 0.1142 - gender_output_acc: 0.9971 - image_quality_output_acc: 0.9853 - age_output_acc: 0.9313 - weight_output_acc: 0.9262 - bag_output_acc: 0.9001 - footwear_output_acc: 0.9929 - pose_output_acc: 0.9962 - emotion_output_acc: 0.9339 - val_loss: 18.6885 - val_gender_output_loss: 0.9877 - val_image_quality_output_loss: 2.8454 - val_age_output_loss: 3.3636 - val_weight_output_loss: 2.5940 - val_bag_output_loss: 2.3600 - val_footwear_output_loss: 2.6792 - val_pose_output_loss: 1.2660 - val_emotion_output_loss: 2.5925 - val_gender_output_acc: 0.8398 - val_image_quality_output_acc: 0.4980 - val_age_output_acc: 0.3661 - val_weight_output_acc: 0.5600 - val_bag_output_acc: 0.6290 - val_footwear_output_acc: 0.6176 - val_pose_output_acc: 0.8224 - val_emotion_output_acc: 0.6007\n",
      "\n",
      "Epoch 00175: loss did not improve from 0.54767\n",
      "Epoch 176/200\n",
      " - 84s - loss: 0.6128 - gender_output_loss: 0.0096 - image_quality_output_loss: 0.0424 - age_output_loss: 0.1441 - weight_output_loss: 0.1319 - bag_output_loss: 0.1302 - footwear_output_loss: 0.0259 - pose_output_loss: 0.0158 - emotion_output_loss: 0.1128 - gender_output_acc: 0.9965 - image_quality_output_acc: 0.9852 - age_output_acc: 0.9301 - weight_output_acc: 0.9240 - bag_output_acc: 0.8990 - footwear_output_acc: 0.9917 - pose_output_acc: 0.9957 - emotion_output_acc: 0.9339 - val_loss: 19.4643 - val_gender_output_loss: 1.0823 - val_image_quality_output_loss: 2.8785 - val_age_output_loss: 3.5794 - val_weight_output_loss: 3.0963 - val_bag_output_loss: 2.2911 - val_footwear_output_loss: 2.6032 - val_pose_output_loss: 1.3068 - val_emotion_output_loss: 2.6265 - val_gender_output_acc: 0.8358 - val_image_quality_output_acc: 0.5283 - val_age_output_acc: 0.3775 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.6225 - val_footwear_output_acc: 0.6116 - val_pose_output_acc: 0.8135 - val_emotion_output_acc: 0.5853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00176: loss did not improve from 0.54767\n",
      "Epoch 177/200\n",
      " - 84s - loss: 0.5271 - gender_output_loss: 0.0048 - image_quality_output_loss: 0.0307 - age_output_loss: 0.1303 - weight_output_loss: 0.1193 - bag_output_loss: 0.1217 - footwear_output_loss: 0.0119 - pose_output_loss: 0.0063 - emotion_output_loss: 0.1021 - gender_output_acc: 0.9985 - image_quality_output_acc: 0.9903 - age_output_acc: 0.9354 - weight_output_acc: 0.9274 - bag_output_acc: 0.9016 - footwear_output_acc: 0.9966 - pose_output_acc: 0.9983 - emotion_output_acc: 0.9366 - val_loss: 20.1202 - val_gender_output_loss: 1.0524 - val_image_quality_output_loss: 3.2516 - val_age_output_loss: 3.4825 - val_weight_output_loss: 2.5314 - val_bag_output_loss: 2.8305 - val_footwear_output_loss: 2.7217 - val_pose_output_loss: 1.4955 - val_emotion_output_loss: 2.7545 - val_gender_output_acc: 0.8442 - val_image_quality_output_acc: 0.5432 - val_age_output_acc: 0.3725 - val_weight_output_acc: 0.5367 - val_bag_output_acc: 0.6106 - val_footwear_output_acc: 0.6131 - val_pose_output_acc: 0.8125 - val_emotion_output_acc: 0.6190\n",
      "\n",
      "Epoch 00177: loss improved from 0.54767 to 0.52709, saving model to model.h5\n",
      "Epoch 178/200\n",
      " - 84s - loss: 0.7391 - gender_output_loss: 0.0141 - image_quality_output_loss: 0.0695 - age_output_loss: 0.1721 - weight_output_loss: 0.1503 - bag_output_loss: 0.1388 - footwear_output_loss: 0.0382 - pose_output_loss: 0.0216 - emotion_output_loss: 0.1347 - gender_output_acc: 0.9959 - image_quality_output_acc: 0.9763 - age_output_acc: 0.9213 - weight_output_acc: 0.9192 - bag_output_acc: 0.8987 - footwear_output_acc: 0.9865 - pose_output_acc: 0.9935 - emotion_output_acc: 0.9291 - val_loss: 18.1337 - val_gender_output_loss: 0.9279 - val_image_quality_output_loss: 2.8145 - val_age_output_loss: 3.2177 - val_weight_output_loss: 2.6734 - val_bag_output_loss: 2.2818 - val_footwear_output_loss: 2.5217 - val_pose_output_loss: 1.1639 - val_emotion_output_loss: 2.5328 - val_gender_output_acc: 0.8447 - val_image_quality_output_acc: 0.5342 - val_age_output_acc: 0.3705 - val_weight_output_acc: 0.5997 - val_bag_output_acc: 0.6334 - val_footwear_output_acc: 0.6176 - val_pose_output_acc: 0.8105 - val_emotion_output_acc: 0.6121\n",
      "\n",
      "Epoch 00178: loss did not improve from 0.52709\n",
      "Epoch 179/200\n",
      " - 84s - loss: 0.7681 - gender_output_loss: 0.0154 - image_quality_output_loss: 0.0710 - age_output_loss: 0.1838 - weight_output_loss: 0.1577 - bag_output_loss: 0.1463 - footwear_output_loss: 0.0313 - pose_output_loss: 0.0177 - emotion_output_loss: 0.1450 - gender_output_acc: 0.9957 - image_quality_output_acc: 0.9753 - age_output_acc: 0.9197 - weight_output_acc: 0.9174 - bag_output_acc: 0.8975 - footwear_output_acc: 0.9891 - pose_output_acc: 0.9944 - emotion_output_acc: 0.9261 - val_loss: 17.1644 - val_gender_output_loss: 0.9565 - val_image_quality_output_loss: 2.5915 - val_age_output_loss: 3.1226 - val_weight_output_loss: 2.5225 - val_bag_output_loss: 2.1958 - val_footwear_output_loss: 2.2708 - val_pose_output_loss: 1.2580 - val_emotion_output_loss: 2.2467 - val_gender_output_acc: 0.8299 - val_image_quality_output_acc: 0.5362 - val_age_output_acc: 0.3795 - val_weight_output_acc: 0.6032 - val_bag_output_acc: 0.6032 - val_footwear_output_acc: 0.6066 - val_pose_output_acc: 0.8061 - val_emotion_output_acc: 0.6022\n",
      "\n",
      "Epoch 00179: loss did not improve from 0.52709\n",
      "Epoch 180/200\n",
      " - 84s - loss: 0.6171 - gender_output_loss: 0.0080 - image_quality_output_loss: 0.0516 - age_output_loss: 0.1446 - weight_output_loss: 0.1343 - bag_output_loss: 0.1288 - footwear_output_loss: 0.0200 - pose_output_loss: 0.0100 - emotion_output_loss: 0.1197 - gender_output_acc: 0.9977 - image_quality_output_acc: 0.9838 - age_output_acc: 0.9312 - weight_output_acc: 0.9229 - bag_output_acc: 0.9001 - footwear_output_acc: 0.9933 - pose_output_acc: 0.9970 - emotion_output_acc: 0.9334 - val_loss: 18.4546 - val_gender_output_loss: 0.9763 - val_image_quality_output_loss: 2.8798 - val_age_output_loss: 3.1381 - val_weight_output_loss: 2.8029 - val_bag_output_loss: 2.3196 - val_footwear_output_loss: 2.5797 - val_pose_output_loss: 1.2434 - val_emotion_output_loss: 2.5148 - val_gender_output_acc: 0.8423 - val_image_quality_output_acc: 0.5377 - val_age_output_acc: 0.3700 - val_weight_output_acc: 0.6176 - val_bag_output_acc: 0.6205 - val_footwear_output_acc: 0.6181 - val_pose_output_acc: 0.8185 - val_emotion_output_acc: 0.6310\n",
      "\n",
      "Epoch 00180: loss did not improve from 0.52709\n",
      "Epoch 181/200\n",
      " - 84s - loss: 0.6523 - gender_output_loss: 0.0103 - image_quality_output_loss: 0.0525 - age_output_loss: 0.1600 - weight_output_loss: 0.1349 - bag_output_loss: 0.1345 - footwear_output_loss: 0.0225 - pose_output_loss: 0.0108 - emotion_output_loss: 0.1270 - gender_output_acc: 0.9964 - image_quality_output_acc: 0.9828 - age_output_acc: 0.9262 - weight_output_acc: 0.9235 - bag_output_acc: 0.8980 - footwear_output_acc: 0.9923 - pose_output_acc: 0.9964 - emotion_output_acc: 0.9300 - val_loss: 19.5753 - val_gender_output_loss: 1.1048 - val_image_quality_output_loss: 3.2435 - val_age_output_loss: 3.2997 - val_weight_output_loss: 2.9444 - val_bag_output_loss: 2.1745 - val_footwear_output_loss: 2.6097 - val_pose_output_loss: 1.4768 - val_emotion_output_loss: 2.7219 - val_gender_output_acc: 0.8353 - val_image_quality_output_acc: 0.5273 - val_age_output_acc: 0.3859 - val_weight_output_acc: 0.6091 - val_bag_output_acc: 0.6156 - val_footwear_output_acc: 0.6096 - val_pose_output_acc: 0.8075 - val_emotion_output_acc: 0.6369\n",
      "\n",
      "Epoch 00181: loss did not improve from 0.52709\n",
      "Epoch 182/200\n",
      " - 84s - loss: 0.5265 - gender_output_loss: 0.0038 - image_quality_output_loss: 0.0313 - age_output_loss: 0.1288 - weight_output_loss: 0.1170 - bag_output_loss: 0.1216 - footwear_output_loss: 0.0173 - pose_output_loss: 0.0062 - emotion_output_loss: 0.1005 - gender_output_acc: 0.9990 - image_quality_output_acc: 0.9904 - age_output_acc: 0.9348 - weight_output_acc: 0.9279 - bag_output_acc: 0.9019 - footwear_output_acc: 0.9937 - pose_output_acc: 0.9979 - emotion_output_acc: 0.9381 - val_loss: 20.4557 - val_gender_output_loss: 1.1298 - val_image_quality_output_loss: 3.2848 - val_age_output_loss: 3.2097 - val_weight_output_loss: 2.9638 - val_bag_output_loss: 2.8927 - val_footwear_output_loss: 3.1166 - val_pose_output_loss: 1.4032 - val_emotion_output_loss: 2.4552 - val_gender_output_acc: 0.8338 - val_image_quality_output_acc: 0.5412 - val_age_output_acc: 0.3735 - val_weight_output_acc: 0.6096 - val_bag_output_acc: 0.6131 - val_footwear_output_acc: 0.6240 - val_pose_output_acc: 0.8026 - val_emotion_output_acc: 0.6190\n",
      "\n",
      "Epoch 00182: loss improved from 0.52709 to 0.52654, saving model to model.h5\n",
      "Epoch 183/200\n",
      " - 84s - loss: 0.6626 - gender_output_loss: 0.0116 - image_quality_output_loss: 0.0503 - age_output_loss: 0.1578 - weight_output_loss: 0.1411 - bag_output_loss: 0.1371 - footwear_output_loss: 0.0366 - pose_output_loss: 0.0089 - emotion_output_loss: 0.1190 - gender_output_acc: 0.9964 - image_quality_output_acc: 0.9832 - age_output_acc: 0.9258 - weight_output_acc: 0.9211 - bag_output_acc: 0.8984 - footwear_output_acc: 0.9875 - pose_output_acc: 0.9963 - emotion_output_acc: 0.9344 - val_loss: 18.8867 - val_gender_output_loss: 1.0975 - val_image_quality_output_loss: 2.8551 - val_age_output_loss: 3.2668 - val_weight_output_loss: 2.6101 - val_bag_output_loss: 2.4647 - val_footwear_output_loss: 2.6544 - val_pose_output_loss: 1.3912 - val_emotion_output_loss: 2.5468 - val_gender_output_acc: 0.8368 - val_image_quality_output_acc: 0.5342 - val_age_output_acc: 0.3631 - val_weight_output_acc: 0.5992 - val_bag_output_acc: 0.6344 - val_footwear_output_acc: 0.6215 - val_pose_output_acc: 0.8170 - val_emotion_output_acc: 0.6305\n",
      "\n",
      "Epoch 00183: loss did not improve from 0.52654\n",
      "Epoch 184/200\n",
      " - 84s - loss: 0.6350 - gender_output_loss: 0.0110 - image_quality_output_loss: 0.0480 - age_output_loss: 0.1500 - weight_output_loss: 0.1309 - bag_output_loss: 0.1348 - footwear_output_loss: 0.0272 - pose_output_loss: 0.0156 - emotion_output_loss: 0.1177 - gender_output_acc: 0.9964 - image_quality_output_acc: 0.9848 - age_output_acc: 0.9288 - weight_output_acc: 0.9249 - bag_output_acc: 0.8992 - footwear_output_acc: 0.9912 - pose_output_acc: 0.9944 - emotion_output_acc: 0.9333 - val_loss: 20.6333 - val_gender_output_loss: 1.1015 - val_image_quality_output_loss: 5.0998 - val_age_output_loss: 3.2249 - val_weight_output_loss: 2.6417 - val_bag_output_loss: 2.1399 - val_footwear_output_loss: 2.5143 - val_pose_output_loss: 1.3087 - val_emotion_output_loss: 2.6026 - val_gender_output_acc: 0.8313 - val_image_quality_output_acc: 0.4177 - val_age_output_acc: 0.3388 - val_weight_output_acc: 0.5809 - val_bag_output_acc: 0.6086 - val_footwear_output_acc: 0.6255 - val_pose_output_acc: 0.8085 - val_emotion_output_acc: 0.6374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00184: loss did not improve from 0.52654\n",
      "Epoch 185/200\n",
      " - 89s - loss: 0.5998 - gender_output_loss: 0.0085 - image_quality_output_loss: 0.0418 - age_output_loss: 0.1460 - weight_output_loss: 0.1302 - bag_output_loss: 0.1294 - footwear_output_loss: 0.0159 - pose_output_loss: 0.0092 - emotion_output_loss: 0.1188 - gender_output_acc: 0.9974 - image_quality_output_acc: 0.9857 - age_output_acc: 0.9319 - weight_output_acc: 0.9248 - bag_output_acc: 0.8998 - footwear_output_acc: 0.9944 - pose_output_acc: 0.9969 - emotion_output_acc: 0.9342 - val_loss: 18.6260 - val_gender_output_loss: 1.0634 - val_image_quality_output_loss: 2.9016 - val_age_output_loss: 3.0937 - val_weight_output_loss: 2.5267 - val_bag_output_loss: 2.5496 - val_footwear_output_loss: 2.6721 - val_pose_output_loss: 1.3197 - val_emotion_output_loss: 2.4993 - val_gender_output_acc: 0.8368 - val_image_quality_output_acc: 0.5298 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.5813 - val_bag_output_acc: 0.6121 - val_footwear_output_acc: 0.6166 - val_pose_output_acc: 0.8135 - val_emotion_output_acc: 0.6181\n",
      "\n",
      "Epoch 00185: loss did not improve from 0.52654\n",
      "Epoch 186/200\n",
      " - 90s - loss: 0.6557 - gender_output_loss: 0.0079 - image_quality_output_loss: 0.0549 - age_output_loss: 0.1621 - weight_output_loss: 0.1310 - bag_output_loss: 0.1380 - footwear_output_loss: 0.0220 - pose_output_loss: 0.0170 - emotion_output_loss: 0.1227 - gender_output_acc: 0.9971 - image_quality_output_acc: 0.9805 - age_output_acc: 0.9238 - weight_output_acc: 0.9240 - bag_output_acc: 0.8977 - footwear_output_acc: 0.9930 - pose_output_acc: 0.9946 - emotion_output_acc: 0.9322 - val_loss: 19.4534 - val_gender_output_loss: 1.0817 - val_image_quality_output_loss: 3.0733 - val_age_output_loss: 3.4132 - val_weight_output_loss: 2.7151 - val_bag_output_loss: 2.6213 - val_footwear_output_loss: 2.7449 - val_pose_output_loss: 1.2999 - val_emotion_output_loss: 2.5040 - val_gender_output_acc: 0.8318 - val_image_quality_output_acc: 0.5312 - val_age_output_acc: 0.3562 - val_weight_output_acc: 0.6042 - val_bag_output_acc: 0.6245 - val_footwear_output_acc: 0.6305 - val_pose_output_acc: 0.8165 - val_emotion_output_acc: 0.6453\n",
      "\n",
      "Epoch 00186: loss did not improve from 0.52654\n",
      "Epoch 187/200\n",
      " - 93s - loss: 0.5110 - gender_output_loss: 0.0043 - image_quality_output_loss: 0.0343 - age_output_loss: 0.1224 - weight_output_loss: 0.1154 - bag_output_loss: 0.1176 - footwear_output_loss: 0.0105 - pose_output_loss: 0.0061 - emotion_output_loss: 0.1004 - gender_output_acc: 0.9987 - image_quality_output_acc: 0.9899 - age_output_acc: 0.9375 - weight_output_acc: 0.9283 - bag_output_acc: 0.9021 - footwear_output_acc: 0.9966 - pose_output_acc: 0.9977 - emotion_output_acc: 0.9377 - val_loss: 21.7420 - val_gender_output_loss: 1.2089 - val_image_quality_output_loss: 3.1827 - val_age_output_loss: 4.2017 - val_weight_output_loss: 3.6607 - val_bag_output_loss: 2.6656 - val_footwear_output_loss: 2.8676 - val_pose_output_loss: 1.3957 - val_emotion_output_loss: 2.5590 - val_gender_output_acc: 0.8323 - val_image_quality_output_acc: 0.5456 - val_age_output_acc: 0.3765 - val_weight_output_acc: 0.6166 - val_bag_output_acc: 0.6086 - val_footwear_output_acc: 0.6171 - val_pose_output_acc: 0.8065 - val_emotion_output_acc: 0.6235\n",
      "\n",
      "Epoch 00187: loss improved from 0.52654 to 0.51098, saving model to model.h5\n",
      "Epoch 188/200\n",
      " - 92s - loss: 0.6963 - gender_output_loss: 0.0091 - image_quality_output_loss: 0.0572 - age_output_loss: 0.1646 - weight_output_loss: 0.1419 - bag_output_loss: 0.1436 - footwear_output_loss: 0.0313 - pose_output_loss: 0.0189 - emotion_output_loss: 0.1296 - gender_output_acc: 0.9960 - image_quality_output_acc: 0.9806 - age_output_acc: 0.9246 - weight_output_acc: 0.9210 - bag_output_acc: 0.8969 - footwear_output_acc: 0.9899 - pose_output_acc: 0.9942 - emotion_output_acc: 0.9303 - val_loss: 18.7020 - val_gender_output_loss: 1.0325 - val_image_quality_output_loss: 2.8764 - val_age_output_loss: 3.3810 - val_weight_output_loss: 2.7565 - val_bag_output_loss: 2.5132 - val_footwear_output_loss: 2.4164 - val_pose_output_loss: 1.1720 - val_emotion_output_loss: 2.5539 - val_gender_output_acc: 0.8363 - val_image_quality_output_acc: 0.5303 - val_age_output_acc: 0.3661 - val_weight_output_acc: 0.6131 - val_bag_output_acc: 0.6265 - val_footwear_output_acc: 0.6240 - val_pose_output_acc: 0.8194 - val_emotion_output_acc: 0.6121\n",
      "\n",
      "Epoch 00188: loss did not improve from 0.51098\n",
      "Epoch 189/200\n",
      " - 91s - loss: 0.5905 - gender_output_loss: 0.0094 - image_quality_output_loss: 0.0465 - age_output_loss: 0.1387 - weight_output_loss: 0.1271 - bag_output_loss: 0.1258 - footwear_output_loss: 0.0180 - pose_output_loss: 0.0111 - emotion_output_loss: 0.1139 - gender_output_acc: 0.9970 - image_quality_output_acc: 0.9846 - age_output_acc: 0.9325 - weight_output_acc: 0.9241 - bag_output_acc: 0.9009 - footwear_output_acc: 0.9939 - pose_output_acc: 0.9960 - emotion_output_acc: 0.9349 - val_loss: 20.3097 - val_gender_output_loss: 0.9805 - val_image_quality_output_loss: 3.7321 - val_age_output_loss: 3.7986 - val_weight_output_loss: 3.4715 - val_bag_output_loss: 2.1210 - val_footwear_output_loss: 2.6398 - val_pose_output_loss: 1.2052 - val_emotion_output_loss: 2.3610 - val_gender_output_acc: 0.8408 - val_image_quality_output_acc: 0.4995 - val_age_output_acc: 0.3849 - val_weight_output_acc: 0.6295 - val_bag_output_acc: 0.6136 - val_footwear_output_acc: 0.5873 - val_pose_output_acc: 0.8160 - val_emotion_output_acc: 0.6319\n",
      "\n",
      "Epoch 00189: loss did not improve from 0.51098\n",
      "Epoch 190/200\n",
      " - 89s - loss: 0.5671 - gender_output_loss: 0.0061 - image_quality_output_loss: 0.0422 - age_output_loss: 0.1411 - weight_output_loss: 0.1238 - bag_output_loss: 0.1246 - footwear_output_loss: 0.0134 - pose_output_loss: 0.0071 - emotion_output_loss: 0.1088 - gender_output_acc: 0.9976 - image_quality_output_acc: 0.9875 - age_output_acc: 0.9316 - weight_output_acc: 0.9262 - bag_output_acc: 0.9010 - footwear_output_acc: 0.9958 - pose_output_acc: 0.9977 - emotion_output_acc: 0.9348 - val_loss: 18.9166 - val_gender_output_loss: 1.0333 - val_image_quality_output_loss: 2.7868 - val_age_output_loss: 3.1529 - val_weight_output_loss: 2.8600 - val_bag_output_loss: 2.4676 - val_footwear_output_loss: 2.6706 - val_pose_output_loss: 1.2357 - val_emotion_output_loss: 2.7095 - val_gender_output_acc: 0.8492 - val_image_quality_output_acc: 0.5124 - val_age_output_acc: 0.3740 - val_weight_output_acc: 0.6096 - val_bag_output_acc: 0.6399 - val_footwear_output_acc: 0.6111 - val_pose_output_acc: 0.8274 - val_emotion_output_acc: 0.6344\n",
      "\n",
      "Epoch 00190: loss did not improve from 0.51098\n",
      "Epoch 191/200\n",
      " - 91s - loss: 0.6403 - gender_output_loss: 0.0099 - image_quality_output_loss: 0.0489 - age_output_loss: 0.1558 - weight_output_loss: 0.1323 - bag_output_loss: 0.1341 - footwear_output_loss: 0.0254 - pose_output_loss: 0.0131 - emotion_output_loss: 0.1209 - gender_output_acc: 0.9967 - image_quality_output_acc: 0.9827 - age_output_acc: 0.9275 - weight_output_acc: 0.9234 - bag_output_acc: 0.8979 - footwear_output_acc: 0.9915 - pose_output_acc: 0.9957 - emotion_output_acc: 0.9310 - val_loss: 19.0623 - val_gender_output_loss: 0.9895 - val_image_quality_output_loss: 3.4550 - val_age_output_loss: 3.4922 - val_weight_output_loss: 2.4651 - val_bag_output_loss: 2.2012 - val_footwear_output_loss: 2.6604 - val_pose_output_loss: 1.3037 - val_emotion_output_loss: 2.4952 - val_gender_output_acc: 0.8413 - val_image_quality_output_acc: 0.4663 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6042 - val_bag_output_acc: 0.6022 - val_footwear_output_acc: 0.6324 - val_pose_output_acc: 0.8155 - val_emotion_output_acc: 0.6121\n",
      "\n",
      "Epoch 00191: loss did not improve from 0.51098\n",
      "Epoch 192/200\n",
      " - 89s - loss: 0.5316 - gender_output_loss: 0.0057 - image_quality_output_loss: 0.0310 - age_output_loss: 0.1295 - weight_output_loss: 0.1176 - bag_output_loss: 0.1249 - footwear_output_loss: 0.0146 - pose_output_loss: 0.0056 - emotion_output_loss: 0.1027 - gender_output_acc: 0.9982 - image_quality_output_acc: 0.9900 - age_output_acc: 0.9352 - weight_output_acc: 0.9280 - bag_output_acc: 0.9004 - footwear_output_acc: 0.9960 - pose_output_acc: 0.9984 - emotion_output_acc: 0.9367 - val_loss: 20.4165 - val_gender_output_loss: 1.1150 - val_image_quality_output_loss: 2.8855 - val_age_output_loss: 3.0831 - val_weight_output_loss: 3.3558 - val_bag_output_loss: 2.7692 - val_footwear_output_loss: 2.8258 - val_pose_output_loss: 1.2696 - val_emotion_output_loss: 3.1125 - val_gender_output_acc: 0.8110 - val_image_quality_output_acc: 0.5104 - val_age_output_acc: 0.3408 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5977 - val_footwear_output_acc: 0.5938 - val_pose_output_acc: 0.8189 - val_emotion_output_acc: 0.6528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00192: loss did not improve from 0.51098\n",
      "Epoch 193/200\n",
      " - 91s - loss: 0.7075 - gender_output_loss: 0.0128 - image_quality_output_loss: 0.0574 - age_output_loss: 0.1718 - weight_output_loss: 0.1433 - bag_output_loss: 0.1388 - footwear_output_loss: 0.0413 - pose_output_loss: 0.0165 - emotion_output_loss: 0.1255 - gender_output_acc: 0.9955 - image_quality_output_acc: 0.9797 - age_output_acc: 0.9235 - weight_output_acc: 0.9201 - bag_output_acc: 0.8986 - footwear_output_acc: 0.9851 - pose_output_acc: 0.9945 - emotion_output_acc: 0.9315 - val_loss: 18.2722 - val_gender_output_loss: 0.9686 - val_image_quality_output_loss: 2.7258 - val_age_output_loss: 3.3142 - val_weight_output_loss: 2.6120 - val_bag_output_loss: 2.4173 - val_footwear_output_loss: 2.4654 - val_pose_output_loss: 1.1448 - val_emotion_output_loss: 2.6240 - val_gender_output_acc: 0.8452 - val_image_quality_output_acc: 0.5407 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.5962 - val_bag_output_acc: 0.6314 - val_footwear_output_acc: 0.6111 - val_pose_output_acc: 0.8180 - val_emotion_output_acc: 0.6200\n",
      "\n",
      "Epoch 00193: loss did not improve from 0.51098\n",
      "Epoch 194/200\n",
      " - 88s - loss: 0.5934 - gender_output_loss: 0.0092 - image_quality_output_loss: 0.0455 - age_output_loss: 0.1417 - weight_output_loss: 0.1328 - bag_output_loss: 0.1251 - footwear_output_loss: 0.0204 - pose_output_loss: 0.0112 - emotion_output_loss: 0.1076 - gender_output_acc: 0.9975 - image_quality_output_acc: 0.9850 - age_output_acc: 0.9312 - weight_output_acc: 0.9243 - bag_output_acc: 0.9010 - footwear_output_acc: 0.9926 - pose_output_acc: 0.9962 - emotion_output_acc: 0.9359 - val_loss: 18.7035 - val_gender_output_loss: 1.0023 - val_image_quality_output_loss: 2.6774 - val_age_output_loss: 3.3891 - val_weight_output_loss: 2.7703 - val_bag_output_loss: 2.3543 - val_footwear_output_loss: 2.5470 - val_pose_output_loss: 1.1615 - val_emotion_output_loss: 2.8014 - val_gender_output_acc: 0.8363 - val_image_quality_output_acc: 0.5293 - val_age_output_acc: 0.3596 - val_weight_output_acc: 0.5868 - val_bag_output_acc: 0.5947 - val_footwear_output_acc: 0.5868 - val_pose_output_acc: 0.8180 - val_emotion_output_acc: 0.5878\n",
      "\n",
      "Epoch 00194: loss did not improve from 0.51098\n",
      "Epoch 195/200\n",
      " - 90s - loss: 0.5486 - gender_output_loss: 0.0070 - image_quality_output_loss: 0.0340 - age_output_loss: 0.1337 - weight_output_loss: 0.1219 - bag_output_loss: 0.1238 - footwear_output_loss: 0.0149 - pose_output_loss: 0.0074 - emotion_output_loss: 0.1059 - gender_output_acc: 0.9977 - image_quality_output_acc: 0.9898 - age_output_acc: 0.9327 - weight_output_acc: 0.9265 - bag_output_acc: 0.9009 - footwear_output_acc: 0.9945 - pose_output_acc: 0.9977 - emotion_output_acc: 0.9363 - val_loss: 19.1019 - val_gender_output_loss: 0.9691 - val_image_quality_output_loss: 2.9018 - val_age_output_loss: 3.4287 - val_weight_output_loss: 2.8547 - val_bag_output_loss: 2.5438 - val_footwear_output_loss: 2.7370 - val_pose_output_loss: 1.2025 - val_emotion_output_loss: 2.4642 - val_gender_output_acc: 0.8457 - val_image_quality_output_acc: 0.5332 - val_age_output_acc: 0.3715 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.6260 - val_footwear_output_acc: 0.6310 - val_pose_output_acc: 0.8224 - val_emotion_output_acc: 0.6255\n",
      "\n",
      "Epoch 00195: loss did not improve from 0.51098\n",
      "Epoch 196/200\n",
      " - 88s - loss: 0.5895 - gender_output_loss: 0.0072 - image_quality_output_loss: 0.0439 - age_output_loss: 0.1395 - weight_output_loss: 0.1244 - bag_output_loss: 0.1307 - footwear_output_loss: 0.0208 - pose_output_loss: 0.0096 - emotion_output_loss: 0.1136 - gender_output_acc: 0.9973 - image_quality_output_acc: 0.9854 - age_output_acc: 0.9306 - weight_output_acc: 0.9260 - bag_output_acc: 0.9003 - footwear_output_acc: 0.9924 - pose_output_acc: 0.9961 - emotion_output_acc: 0.9344 - val_loss: 19.9425 - val_gender_output_loss: 1.1104 - val_image_quality_output_loss: 2.9598 - val_age_output_loss: 3.4982 - val_weight_output_loss: 3.2763 - val_bag_output_loss: 2.4499 - val_footwear_output_loss: 2.6130 - val_pose_output_loss: 1.3647 - val_emotion_output_loss: 2.6702 - val_gender_output_acc: 0.8313 - val_image_quality_output_acc: 0.5293 - val_age_output_acc: 0.3646 - val_weight_output_acc: 0.6260 - val_bag_output_acc: 0.6404 - val_footwear_output_acc: 0.6156 - val_pose_output_acc: 0.8170 - val_emotion_output_acc: 0.6334\n",
      "\n",
      "Epoch 00196: loss did not improve from 0.51098\n",
      "Epoch 197/200\n",
      " - 90s - loss: 0.5134 - gender_output_loss: 0.0040 - image_quality_output_loss: 0.0353 - age_output_loss: 0.1206 - weight_output_loss: 0.1130 - bag_output_loss: 0.1199 - footwear_output_loss: 0.0122 - pose_output_loss: 0.0074 - emotion_output_loss: 0.1012 - gender_output_acc: 0.9989 - image_quality_output_acc: 0.9898 - age_output_acc: 0.9389 - weight_output_acc: 0.9294 - bag_output_acc: 0.9020 - footwear_output_acc: 0.9960 - pose_output_acc: 0.9982 - emotion_output_acc: 0.9367 - val_loss: 19.4932 - val_gender_output_loss: 1.1037 - val_image_quality_output_loss: 3.2539 - val_age_output_loss: 3.5346 - val_weight_output_loss: 2.5271 - val_bag_output_loss: 2.2193 - val_footwear_output_loss: 2.6301 - val_pose_output_loss: 1.2807 - val_emotion_output_loss: 2.9438 - val_gender_output_acc: 0.8383 - val_image_quality_output_acc: 0.5253 - val_age_output_acc: 0.3482 - val_weight_output_acc: 0.5957 - val_bag_output_acc: 0.6181 - val_footwear_output_acc: 0.6215 - val_pose_output_acc: 0.8194 - val_emotion_output_acc: 0.6379\n",
      "\n",
      "Epoch 00197: loss did not improve from 0.51098\n",
      "Epoch 198/200\n",
      " - 88s - loss: 0.7077 - gender_output_loss: 0.0121 - image_quality_output_loss: 0.0565 - age_output_loss: 0.1776 - weight_output_loss: 0.1445 - bag_output_loss: 0.1438 - footwear_output_loss: 0.0304 - pose_output_loss: 0.0169 - emotion_output_loss: 0.1260 - gender_output_acc: 0.9959 - image_quality_output_acc: 0.9810 - age_output_acc: 0.9205 - weight_output_acc: 0.9207 - bag_output_acc: 0.8976 - footwear_output_acc: 0.9902 - pose_output_acc: 0.9942 - emotion_output_acc: 0.9307 - val_loss: 18.5764 - val_gender_output_loss: 1.0467 - val_image_quality_output_loss: 2.8114 - val_age_output_loss: 3.4299 - val_weight_output_loss: 2.6239 - val_bag_output_loss: 2.3824 - val_footwear_output_loss: 2.5078 - val_pose_output_loss: 1.2433 - val_emotion_output_loss: 2.5310 - val_gender_output_acc: 0.8433 - val_image_quality_output_acc: 0.5243 - val_age_output_acc: 0.3571 - val_weight_output_acc: 0.5987 - val_bag_output_acc: 0.6354 - val_footwear_output_acc: 0.6195 - val_pose_output_acc: 0.8130 - val_emotion_output_acc: 0.5992\n",
      "\n",
      "Epoch 00198: loss did not improve from 0.51098\n",
      "Epoch 199/200\n",
      " - 86s - loss: 0.5950 - gender_output_loss: 0.0097 - image_quality_output_loss: 0.0386 - age_output_loss: 0.1424 - weight_output_loss: 0.1288 - bag_output_loss: 0.1279 - footwear_output_loss: 0.0264 - pose_output_loss: 0.0118 - emotion_output_loss: 0.1093 - gender_output_acc: 0.9970 - image_quality_output_acc: 0.9872 - age_output_acc: 0.9317 - weight_output_acc: 0.9247 - bag_output_acc: 0.8999 - footwear_output_acc: 0.9906 - pose_output_acc: 0.9964 - emotion_output_acc: 0.9359 - val_loss: 19.1962 - val_gender_output_loss: 1.1272 - val_image_quality_output_loss: 3.3539 - val_age_output_loss: 3.0120 - val_weight_output_loss: 2.6020 - val_bag_output_loss: 2.3717 - val_footwear_output_loss: 2.5021 - val_pose_output_loss: 1.3743 - val_emotion_output_loss: 2.8530 - val_gender_output_acc: 0.8274 - val_image_quality_output_acc: 0.4812 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.5908 - val_bag_output_acc: 0.6022 - val_footwear_output_acc: 0.6007 - val_pose_output_acc: 0.8070 - val_emotion_output_acc: 0.6369\n",
      "\n",
      "Epoch 00199: loss did not improve from 0.51098\n",
      "Epoch 200/200\n",
      " - 86s - loss: 0.5407 - gender_output_loss: 0.0060 - image_quality_output_loss: 0.0303 - age_output_loss: 0.1333 - weight_output_loss: 0.1163 - bag_output_loss: 0.1242 - footwear_output_loss: 0.0152 - pose_output_loss: 0.0089 - emotion_output_loss: 0.1064 - gender_output_acc: 0.9977 - image_quality_output_acc: 0.9916 - age_output_acc: 0.9338 - weight_output_acc: 0.9283 - bag_output_acc: 0.9009 - footwear_output_acc: 0.9947 - pose_output_acc: 0.9975 - emotion_output_acc: 0.9359 - val_loss: 19.6559 - val_gender_output_loss: 1.0732 - val_image_quality_output_loss: 2.9998 - val_age_output_loss: 3.6325 - val_weight_output_loss: 2.9263 - val_bag_output_loss: 2.2454 - val_footwear_output_loss: 2.6473 - val_pose_output_loss: 1.3451 - val_emotion_output_loss: 2.7863 - val_gender_output_acc: 0.8343 - val_image_quality_output_acc: 0.5223 - val_age_output_acc: 0.3611 - val_weight_output_acc: 0.6141 - val_bag_output_acc: 0.6305 - val_footwear_output_acc: 0.6086 - val_pose_output_acc: 0.8080 - val_emotion_output_acc: 0.6096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00200: loss did not improve from 0.51098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fe0a4e9e80>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########1st Iter for 200 epoch\n",
    "\n",
    "\n",
    "\n",
    "model2.fit_generator(\n",
    "    generator=train_gen,\n",
    "    validation_data=valid_gen,\n",
    "    epochs=200,\n",
    "    verbose=2,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
